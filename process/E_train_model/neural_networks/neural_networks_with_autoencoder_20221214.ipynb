{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Decide which algorithm and version of the data we are going to use for model training\n",
    "(it'll be neural network in this file)\n",
    "\n",
    "Additionally, choose:\n",
    "* if we'll skip scaling the data\n",
    "* if we'll use full categories instead of dummies\n",
    "* what fraction of the data we'll use for testing (0.1)\n",
    "* if the data split will be randomised (it won't!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:06.592418Z",
     "iopub.status.busy": "2022-12-07T14:05:06.591775Z",
     "iopub.status.idle": "2022-12-07T14:05:06.602072Z",
     "shell.execute_reply": "2022-12-07T14:05:06.601361Z",
     "shell.execute_reply.started": "2022-12-07T14:05:06.592327Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#ALGORITHM = 'Neural Network'\n",
    "ALGORITHM = 'Neural Network [TYPE]'\n",
    "ALGORITHM_DETAIL = ''\n",
    "ALGORITHM_DETAIL_ORIG = ALGORITHM_DETAIL\n",
    "#ALGORITHM_DETAIL += ' tbc'\n",
    "DATA_DETAIL = []\n",
    "#DATA_DETAIL = ['no scale','no dummies']\n",
    "VERSION = '09'\n",
    "\n",
    "RANDOM_STATE = 101\n",
    "TRAINING_SIZE = 0.9\n",
    "\n",
    "CROSS_VALIDATION_SCORING = 'r2'\n",
    "\n",
    "price_divisor = 1\n",
    "\n",
    "\n",
    "#selected_neural_network='simplest'\n",
    "#selected_neural_network='quite simple'\n",
    "#selected_neural_network='recommended simple v2'\n",
    "#selected_neural_network='adapted v3'\n",
    "\n",
    "\n",
    "# ---- FIRST NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = 'recommended simple v1'\n",
    "#selected_nn_code = 'm01 simple'\n",
    "\n",
    "# ---- 2nd NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m02 two layers\"\n",
    "\n",
    "\n",
    "# ---- 3rd NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m03 2 layers+wider\"\n",
    "\n",
    "\n",
    "# ---- 4th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m04 3 layers+wider\"\n",
    "\n",
    "# ---- 5th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m05 rec deep\"\n",
    "\n",
    "# ---- 6th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m05 my deep\"\n",
    "\n",
    "#selected_neural_network = selected_nn_code = \"\"\n",
    "\n",
    "# ---- 7th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m11 mega\"\n",
    "\n",
    "# ---- 8th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m12 mega\"\n",
    "\n",
    "# ---- 9th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m13 mega\"\n",
    "\n",
    "# ---- 10th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "#selected_neural_network = selected_nn_code = \"m14 mega\"\n",
    "\n",
    "# ---- 10th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "selected_neural_network = selected_nn_code = \"m15 mega + dropout\"\n",
    "\n",
    "\n",
    "\n",
    "ALGORITHM = ALGORITHM.replace(\"[TYPE]\", selected_nn_code)\n",
    "\n",
    "create_python_script = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: loading all dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:18.573801Z",
     "iopub.status.busy": "2022-12-07T14:52:18.573463Z",
     "iopub.status.idle": "2022-12-07T14:52:21.551851Z",
     "shell.execute_reply": "2022-12-07T14:52:21.550879Z",
     "shell.execute_reply.started": "2022-12-07T14:52:18.573774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Installing collected packages: tabulate\r\n",
      "Successfully installed tabulate-0.9.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"JPY_PARENT_PID\" in os.environ:\n",
    "    is_jupyter = True\n",
    "else:\n",
    "    is_jupyter = False\n",
    "\n",
    "\n",
    "if is_jupyter:\n",
    "    #! pip install scikeras\n",
    "    !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:06.607583Z",
     "iopub.status.busy": "2022-12-07T14:05:06.607393Z",
     "iopub.status.idle": "2022-12-07T14:05:08.350118Z",
     "shell.execute_reply": "2022-12-07T14:05:08.349107Z",
     "shell.execute_reply.started": "2022-12-07T14:05:06.607565Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notebook_environment': 'gradient', 'use_gpu': True, 'debug_mode': False, 'quick_mode': False, 'quick_override_cv_splits': 2, 'quick_override_n_iter': 10, 'quick_override_n_jobs': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from termcolor import colored\n",
    "from time import time\n",
    "import sklearn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "with open('../../z_envs/_envs.json') as f:\n",
    "    env_vars = json.loads(f.read())\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    run_env = 'colab'\n",
    "except:\n",
    "    try:\n",
    "        run_env = env_vars['notebook_environment']\n",
    "    except:\n",
    "        run_env = 'unknown'\n",
    "\n",
    "if \"JPY_PARENT_PID\" in os.environ:\n",
    "    is_jupyter = True\n",
    "else:\n",
    "    is_jupyter = False\n",
    "\n",
    "use_gpu = env_vars.get('use_gpu', False)\n",
    "debug_mode = env_vars.get('debug_mode', False)\n",
    "quick_mode = env_vars.get('quick_mode', False)\n",
    "OVERRIDE_CV = env_vars.get('quick_override_cv_splits', None) if quick_mode else None\n",
    "OVERRIDE_N_ITER = env_vars.get('quick_override_n_iter', None) if quick_mode else None\n",
    "OVERRIDE_JOBS = env_vars.get('quick_override_n_jobs', None) if quick_mode else None\n",
    "OVERRIDE_VERBOSE = 1\n",
    "#if quick_mode:OVERRIDE_CV, OVERRIDE_N_ITER = 2, 10\n",
    "\n",
    "already_timed = False\n",
    "no_dummies = 'no dummies' in DATA_DETAIL\n",
    "no_scaling = 'no scaling' in DATA_DETAIL\n",
    "#not_catboost = 'catboost' not in ALGORITHM.lower() or not no_dummies\n",
    "using_catboost = 'catboost' in ALGORITHM.lower()\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..', '..'))\n",
    "if module_path not in sys.path:\n",
    "    #sys.path.append(module_path+\"\\\\zfunctions\")\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "if run_env not in ['colab', 'gradient', 'cloud']:\n",
    "    cloud_run = False\n",
    "    from functions_b__get_the_data_20221116 import set_csv_directory\n",
    "    set_csv_directory('final_split')\n",
    "else:\n",
    "    cloud_run = True\n",
    "\n",
    "from functions_0__common_20221116 import get_columns\n",
    "from functions_b__get_the_data_20221116 import get_combined_dataset, get_source_dataframe\n",
    "from functions_d1__prepare_cleanse_data_20221116 import tidy_dataset\n",
    "from functions_d2__transform_enrich_data_20221116 import preprocess, feature_engineer\n",
    "from functions_d3__prepare_store_data_20221116 import create_train_test_data\n",
    "from functions_e__train_model_20221116 import get_chosen_model, make_modelling_pipeline, get_cv_params, fit_model_with_cross_validation, get_hyperparameters\n",
    "from functions_f_evaluate_model_20221116 import get_best_estimator_average_time, get_results, update_results\n",
    "\n",
    "print(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include any overrides specific to the algorthm / python environment being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:08.351758Z",
     "iopub.status.busy": "2022-12-07T14:05:08.351382Z",
     "iopub.status.idle": "2022-12-07T14:05:08.355841Z",
     "shell.execute_reply": "2022-12-07T14:05:08.354812Z",
     "shell.execute_reply.started": "2022-12-07T14:05:08.351732Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#running_locally = True\n",
    "running_locally = run_env == 'local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: creating the ANN model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:08.358371Z",
     "iopub.status.busy": "2022-12-07T14:05:08.358157Z",
     "iopub.status.idle": "2022-12-07T14:05:10.449885Z",
     "shell.execute_reply": "2022-12-07T14:05:10.449187Z",
     "shell.execute_reply.started": "2022-12-07T14:05:08.358351Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "loss_dict = {\n",
    "    \"mean_squared_error\":'mse',\n",
    "    \"mean_absolute_error\":'mae'\n",
    "            }\n",
    "\n",
    "def make_simple_ann(key, inputs=-1):\n",
    "    if False:\n",
    "        pass\n",
    "    elif key == 'quite simple':\n",
    "\n",
    "        new_algorithm_detail = ALGORITHM_DETAIL_ORIG + 'quite simple model + normalise, mse'\n",
    "\n",
    "        learn_rate = 0.1\n",
    "        epochs, chosen_loss = 100, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'recommended simple v1':\n",
    "\n",
    "        learn_rate = 0.003 #0.3\n",
    "        epochs, chosen_loss = 50, 'mean_squared_error'\n",
    "\n",
    "        new_algorithm_detail = ALGORITHM_DETAIL_ORIG + 'recommended simple model/mse'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm02 two layers':\n",
    "\n",
    "        learn_rate = 0.003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(X_train.shape[1], activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "\n",
    "    elif key == 'm03 2 layers+wider':\n",
    "\n",
    "        learn_rate = 0.0003 # 0.003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm04 3 layers+wider':\n",
    "\n",
    "        learn_rate = 0.003\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            layers.Dense(40, activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm0x four layers,wider,batchnorm':\n",
    "\n",
    "        learn_rate = 0.0003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        #from layers.normalization import BatchNormalization\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        batchnorm = layers.BatchNormalization()\n",
    "        activation = layers.Activation('relu')\n",
    "\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        #new_algorithm_detail += ' +norm'\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            #normalizer,\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            batchnorm,\n",
    "            activation,\n",
    "            layers.Dense(40, activation='relu'),\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm05 rec deep':\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        # Compile the network :\n",
    "        #chosen_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        learn_rate = 0.0003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "    elif key == 'm11 mega':\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(2148, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(2148, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        # Compile the network :\n",
    "        #chosen_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs, chosen_loss = 400, 'mean_squared_error'\n",
    "\n",
    "    elif key == 'm12 mega':\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        # Compile the network :\n",
    "        #chosen_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs, chosen_loss = 400, 'mean_squared_error'\n",
    "    elif key == 'm13 mega':\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        #normalizer.adapt(np.array(128))\n",
    "\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(normalizer),\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs = 400\n",
    "        chosen_loss = 'mean_absolute_error' # 'mean_squared_error'\n",
    "\n",
    "    elif key == 'm14 mega':\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        batchnorm = layers.BatchNormalization()\n",
    "        activation = layers.Activation('relu')\n",
    "\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(normalizer)\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs = 400\n",
    "        chosen_loss = 'mean_absolute_error' # 'mean_squared_error'\n",
    "\n",
    "    elif key == \"m15 mega + dropout\":\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        batchnorm = layers.BatchNormalization()\n",
    "        activation = layers.Activation('relu')\n",
    "\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(normalizer)\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "\n",
    "        chosen_model.add(keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "\n",
    "        chosen_model.add(keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs = 400\n",
    "        chosen_loss = 'mean_absolute_error' # 'mean_squared_error'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"make_simple_ann: no entry for key:\", key)\n",
    "\n",
    "    if running_locally:\n",
    "        epochs = 8\n",
    "\n",
    "    # Compile the network :\n",
    "    chosen_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate),\n",
    "        loss=chosen_loss)\n",
    "\n",
    "\n",
    "    new_algorithm_detail = ALGORITHM_DETAIL_ORIG + loss_dict[chosen_loss]\n",
    "    new_algorithm_detail += f' +epochs={epochs}'\n",
    "    new_algorithm_detail += f' +learn={learn_rate}'\n",
    "\n",
    "    return chosen_model, new_algorithm_detail, epochs, {'learning_rate':learn_rate}\n",
    "\n",
    "#make_simple_ann('m04 four layers,wider,batchnorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: get the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:10.451452Z",
     "iopub.status.busy": "2022-12-07T14:05:10.450990Z",
     "iopub.status.idle": "2022-12-07T14:05:10.455099Z",
     "shell.execute_reply": "2022-12-07T14:05:10.454474Z",
     "shell.execute_reply.started": "2022-12-07T14:05:10.451427Z"
    }
   },
   "outputs": [],
   "source": [
    "columns, booleans, floats, categories, custom, wildcard = get_columns(version=VERSION)\n",
    "LABEL = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:10.456195Z",
     "iopub.status.busy": "2022-12-07T14:05:10.455983Z",
     "iopub.status.idle": "2022-12-07T14:05:11.107767Z",
     "shell.execute_reply": "2022-12-07T14:05:11.107128Z",
     "shell.execute_reply.started": "2022-12-07T14:05:10.456175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data from ../../../https://raw.githubusercontent.com/jayportfolio/capstone_streamlit/main/data/final/df_listings_v09.csv\n"
     ]
    }
   ],
   "source": [
    "df, retrieval_type = get_source_dataframe(cloud_run, VERSION, folder_prefix='../../../', row_limit=None)\n",
    "df_orig = df.copy()\n",
    "\n",
    "if retrieval_type != 'tidy':\n",
    "    df = tidy_dataset(df, version=int(VERSION))\n",
    "    df = feature_engineer(df, version=int(VERSION))\n",
    "\n",
    "\n",
    "    df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.108943Z",
     "iopub.status.busy": "2022-12-07T14:05:11.108690Z",
     "iopub.status.idle": "2022-12-07T14:05:11.113380Z",
     "shell.execute_reply": "2022-12-07T14:05:11.112760Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.108920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mfeatures\u001B[0m ->  ['bedrooms', 'bathrooms', 'nearestStation', 'location.latitude', 'location.longitude', 'latitude_deviation', 'longitude_deviation', 'tenure.tenureType']\n",
      "\u001B[1m\u001B[32mlabel\u001B[0m ->  Price\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"features\", \"blue\"), \"-> \", columns)\n",
    "columns.insert(0, LABEL)\n",
    "print(colored(f\"label\", \"green\", None, ['bold']), \"-> \", LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.114368Z",
     "iopub.status.busy": "2022-12-07T14:05:11.114163Z",
     "iopub.status.idle": "2022-12-07T14:05:11.147575Z",
     "shell.execute_reply": "2022-12-07T14:05:11.146653Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.114349Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = preprocess(df, version=VERSION)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.148796Z",
     "iopub.status.busy": "2022-12-07T14:05:11.148563Z",
     "iopub.status.idle": "2022-12-07T14:05:11.173891Z",
     "shell.execute_reply": "2022-12-07T14:05:11.173199Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.148775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             Price  bedrooms  bathrooms  nearestStation  location.latitude  \\\n14520525  550000.0       3.0        1.0        0.274316          51.529950   \n27953107  400000.0       2.0        2.0        0.305845          51.549390   \n33593487  579950.0       2.0        1.0        0.438045          51.447180   \n35271294  370000.0       2.0        1.0        0.399307          51.449568   \n44749111  475000.0       2.0        1.0        0.410550          51.370050   \n46204665  435000.0       3.0        2.0        0.314779          51.539070   \n49020666  200000.0       1.0        1.0        0.875911          51.539959   \n49036279  275000.0       2.0        1.0        0.474368          51.541780   \n49303873  450000.0       3.0        2.0        0.577040          51.524880   \n52064391  349950.0       2.0        2.0        0.212734          51.470800   \n52187854  450000.0       1.0        1.0        0.446802          51.527199   \n52845963  200000.0       2.0        1.0        0.650562          51.398040   \n52913496  220000.0       1.0        1.0        0.945991          51.539383   \n53609433  489995.0       1.0        1.0        0.087081          51.532620   \n53938989  450000.0       2.0        1.0        0.775203          51.658287   \n54713232  332000.0       2.0        1.0        0.319226          51.612300   \n54904122  365000.0       2.0        1.0        0.260722          51.593595   \n54991934  430000.0       3.0        1.0        0.497268          51.528720   \n55043230  260000.0       1.0        1.0        0.384607          51.544430   \n55187658  430000.0       2.0        2.0        0.289033          51.507570   \n55805965  280000.0       2.0        1.0        0.742859          51.520910   \n55839051  599950.0       2.0        1.0        0.259168          51.579186   \n55940994  385000.0       2.0        2.0        0.403987          51.376930   \n56449305  380000.0       2.0        1.0        0.310271          51.600483   \n57221413  475000.0       3.0        2.0        0.409784          51.497260   \n57878227  490000.0       2.0        1.0        0.052498          51.580270   \n59258796  475000.0       3.0        1.0        0.424573          51.536335   \n59658138  499995.0       1.0        1.0        0.396544          51.462211   \n60741240  435000.0       2.0        1.0        0.162014          51.612150   \n61387062  375000.0       2.0        1.0        0.493102          51.448697   \n\n          location.longitude  latitude_deviation  longitude_deviation  \\\n14520525           -0.207020            0.030230             0.102600   \n27953107           -0.482600            0.049670             0.378180   \n33593487           -0.338770            0.052540             0.234350   \n35271294           -0.140154            0.050152             0.035734   \n44749111           -0.212410            0.129670             0.107990   \n46204665           -0.198935            0.039350             0.094515   \n49020666           -0.380863            0.040239             0.276443   \n49036279            0.037890            0.042060             0.142310   \n49303873            0.187200            0.025160             0.291620   \n52064391           -0.361820            0.028920             0.257400   \n52187854           -0.202898            0.027479             0.098478   \n52845963           -0.076812            0.101680             0.027608   \n52913496           -0.382239            0.039663             0.277819   \n53609433           -0.107860            0.032900             0.003440   \n53938989           -0.207902            0.158567             0.103482   \n54713232           -0.119860            0.112580             0.015440   \n54904122            0.022046            0.093875             0.126466   \n54991934            0.039180            0.029000             0.143600   \n55043230            0.014500            0.044710             0.118920   \n55187658            0.078030            0.007850             0.182450   \n55805965            0.022680            0.021190             0.127100   \n55839051           -0.209020            0.079466             0.104600   \n55940994           -0.238870            0.122790             0.134450   \n56449305           -0.062096            0.100763             0.042324   \n57221413           -0.422530            0.002460             0.318110   \n57878227            0.022290            0.080550             0.126710   \n59258796           -0.068537            0.036615             0.035883   \n59658138           -0.196876            0.037509             0.092456   \n60741240           -0.277430            0.112430             0.173010   \n61387062           -0.174068            0.051023             0.069648   \n\n          tenure.tenureType  feature__balcony  feature__chain free  \\\n14520525          LEASEHOLD                 0                    0   \n27953107          LEASEHOLD                 1                    0   \n33593487           FREEHOLD                 0                    0   \n35271294          LEASEHOLD                 1                    0   \n44749111           FREEHOLD                 0                    0   \n46204665          LEASEHOLD                 0                    0   \n49020666          LEASEHOLD                 0                    0   \n49036279          LEASEHOLD                 0                    0   \n49303873           FREEHOLD                 0                    0   \n52064391          LEASEHOLD                 0                    0   \n52187854          LEASEHOLD                 0                    0   \n52845963          LEASEHOLD                 0                    0   \n52913496          LEASEHOLD                 0                    1   \n53609433          LEASEHOLD                 0                    0   \n53938989           FREEHOLD                 0                    0   \n54713232  SHARE_OF_FREEHOLD                 0                    0   \n54904122  SHARE_OF_FREEHOLD                 0                    0   \n54991934           FREEHOLD                 0                    0   \n55043230          LEASEHOLD                 0                    0   \n55187658          LEASEHOLD                 0                    0   \n55805965          LEASEHOLD                 0                    0   \n55839051          LEASEHOLD                 0                    0   \n55940994          LEASEHOLD                 0                    0   \n56449305           FREEHOLD                 0                    0   \n57221413           FREEHOLD                 0                    0   \n57878227  SHARE_OF_FREEHOLD                 0                    0   \n59258796          LEASEHOLD                 0                    0   \n59658138          LEASEHOLD                 0                    0   \n60741240          LEASEHOLD                 1                    0   \n61387062          LEASEHOLD                 0                    0   \n\n          feature__no onward chain  feature__off street parking  \\\n14520525                         0                            0   \n27953107                         0                            0   \n33593487                         1                            0   \n35271294                         0                            0   \n44749111                         0                            0   \n46204665                         0                            0   \n49020666                         0                            0   \n49036279                         0                            0   \n49303873                         0                            0   \n52064391                         0                            0   \n52187854                         0                            0   \n52845963                         0                            0   \n52913496                         0                            0   \n53609433                         0                            0   \n53938989                         0                            0   \n54713232                         0                            0   \n54904122                         0                            0   \n54991934                         0                            0   \n55043230                         0                            0   \n55187658                         0                            0   \n55805965                         0                            0   \n55839051                         0                            0   \n55940994                         0                            0   \n56449305                         0                            0   \n57221413                         0                            0   \n57878227                         0                            0   \n59258796                         0                            0   \n59658138                         0                            0   \n60741240                         0                            0   \n61387062                         0                            0   \n\n          feature__one bedroom  feature__private balcony  \\\n14520525                     0                         1   \n27953107                     0                         0   \n33593487                     0                         0   \n35271294                     0                         0   \n44749111                     0                         0   \n46204665                     0                         0   \n49020666                     1                         0   \n49036279                     0                         0   \n49303873                     0                         0   \n52064391                     0                         1   \n52187854                     0                         0   \n52845963                     0                         0   \n52913496                     0                         0   \n53609433                     0                         0   \n53938989                     0                         0   \n54713232                     0                         0   \n54904122                     0                         0   \n54991934                     0                         0   \n55043230                     0                         0   \n55187658                     0                         0   \n55805965                     0                         0   \n55839051                     0                         0   \n55940994                     0                         0   \n56449305                     0                         0   \n57221413                     0                         0   \n57878227                     0                         0   \n59258796                     0                         0   \n59658138                     0                         0   \n60741240                     0                         0   \n61387062                     0                         0   \n\n          feature__share of freehold  feature__three bedrooms  \\\n14520525                           0                        0   \n27953107                           0                        0   \n33593487                           0                        0   \n35271294                           0                        0   \n44749111                           0                        0   \n46204665                           0                        1   \n49020666                           0                        0   \n49036279                           0                        0   \n49303873                           0                        0   \n52064391                           0                        0   \n52187854                           0                        0   \n52845963                           0                        0   \n52913496                           0                        0   \n53609433                           0                        0   \n53938989                           0                        0   \n54713232                           0                        0   \n54904122                           1                        0   \n54991934                           0                        0   \n55043230                           0                        0   \n55187658                           0                        0   \n55805965                           0                        0   \n55839051                           0                        0   \n55940994                           0                        0   \n56449305                           0                        0   \n57221413                           0                        1   \n57878227                           0                        0   \n59258796                           0                        1   \n59658138                           0                        0   \n60741240                           0                        0   \n61387062                           0                        0   \n\n          feature__two bedrooms  feature__two double bedrooms  \n14520525                      0                             0  \n27953107                      0                             1  \n33593487                      0                             0  \n35271294                      0                             0  \n44749111                      0                             0  \n46204665                      0                             0  \n49020666                      0                             0  \n49036279                      0                             0  \n49303873                      0                             0  \n52064391                      1                             0  \n52187854                      0                             0  \n52845963                      0                             0  \n52913496                      0                             0  \n53609433                      0                             0  \n53938989                      0                             0  \n54713232                      0                             0  \n54904122                      0                             0  \n54991934                      0                             0  \n55043230                      0                             0  \n55187658                      0                             0  \n55805965                      0                             0  \n55839051                      0                             0  \n55940994                      1                             0  \n56449305                      1                             0  \n57221413                      0                             0  \n57878227                      0                             0  \n59258796                      0                             0  \n59658138                      0                             0  \n60741240                      0                             1  \n61387062                      0                             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>nearestStation</th>\n      <th>location.latitude</th>\n      <th>location.longitude</th>\n      <th>latitude_deviation</th>\n      <th>longitude_deviation</th>\n      <th>tenure.tenureType</th>\n      <th>feature__balcony</th>\n      <th>feature__chain free</th>\n      <th>feature__no onward chain</th>\n      <th>feature__off street parking</th>\n      <th>feature__one bedroom</th>\n      <th>feature__private balcony</th>\n      <th>feature__share of freehold</th>\n      <th>feature__three bedrooms</th>\n      <th>feature__two bedrooms</th>\n      <th>feature__two double bedrooms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14520525</th>\n      <td>550000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.274316</td>\n      <td>51.529950</td>\n      <td>-0.207020</td>\n      <td>0.030230</td>\n      <td>0.102600</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27953107</th>\n      <td>400000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.305845</td>\n      <td>51.549390</td>\n      <td>-0.482600</td>\n      <td>0.049670</td>\n      <td>0.378180</td>\n      <td>LEASEHOLD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33593487</th>\n      <td>579950.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.438045</td>\n      <td>51.447180</td>\n      <td>-0.338770</td>\n      <td>0.052540</td>\n      <td>0.234350</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35271294</th>\n      <td>370000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.399307</td>\n      <td>51.449568</td>\n      <td>-0.140154</td>\n      <td>0.050152</td>\n      <td>0.035734</td>\n      <td>LEASEHOLD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44749111</th>\n      <td>475000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.410550</td>\n      <td>51.370050</td>\n      <td>-0.212410</td>\n      <td>0.129670</td>\n      <td>0.107990</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46204665</th>\n      <td>435000.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.314779</td>\n      <td>51.539070</td>\n      <td>-0.198935</td>\n      <td>0.039350</td>\n      <td>0.094515</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49020666</th>\n      <td>200000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.875911</td>\n      <td>51.539959</td>\n      <td>-0.380863</td>\n      <td>0.040239</td>\n      <td>0.276443</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49036279</th>\n      <td>275000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.474368</td>\n      <td>51.541780</td>\n      <td>0.037890</td>\n      <td>0.042060</td>\n      <td>0.142310</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49303873</th>\n      <td>450000.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.577040</td>\n      <td>51.524880</td>\n      <td>0.187200</td>\n      <td>0.025160</td>\n      <td>0.291620</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52064391</th>\n      <td>349950.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.212734</td>\n      <td>51.470800</td>\n      <td>-0.361820</td>\n      <td>0.028920</td>\n      <td>0.257400</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52187854</th>\n      <td>450000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.446802</td>\n      <td>51.527199</td>\n      <td>-0.202898</td>\n      <td>0.027479</td>\n      <td>0.098478</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52845963</th>\n      <td>200000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.650562</td>\n      <td>51.398040</td>\n      <td>-0.076812</td>\n      <td>0.101680</td>\n      <td>0.027608</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52913496</th>\n      <td>220000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.945991</td>\n      <td>51.539383</td>\n      <td>-0.382239</td>\n      <td>0.039663</td>\n      <td>0.277819</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53609433</th>\n      <td>489995.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.087081</td>\n      <td>51.532620</td>\n      <td>-0.107860</td>\n      <td>0.032900</td>\n      <td>0.003440</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53938989</th>\n      <td>450000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.775203</td>\n      <td>51.658287</td>\n      <td>-0.207902</td>\n      <td>0.158567</td>\n      <td>0.103482</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54713232</th>\n      <td>332000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.319226</td>\n      <td>51.612300</td>\n      <td>-0.119860</td>\n      <td>0.112580</td>\n      <td>0.015440</td>\n      <td>SHARE_OF_FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54904122</th>\n      <td>365000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.260722</td>\n      <td>51.593595</td>\n      <td>0.022046</td>\n      <td>0.093875</td>\n      <td>0.126466</td>\n      <td>SHARE_OF_FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54991934</th>\n      <td>430000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.497268</td>\n      <td>51.528720</td>\n      <td>0.039180</td>\n      <td>0.029000</td>\n      <td>0.143600</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55043230</th>\n      <td>260000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.384607</td>\n      <td>51.544430</td>\n      <td>0.014500</td>\n      <td>0.044710</td>\n      <td>0.118920</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55187658</th>\n      <td>430000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.289033</td>\n      <td>51.507570</td>\n      <td>0.078030</td>\n      <td>0.007850</td>\n      <td>0.182450</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55805965</th>\n      <td>280000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.742859</td>\n      <td>51.520910</td>\n      <td>0.022680</td>\n      <td>0.021190</td>\n      <td>0.127100</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55839051</th>\n      <td>599950.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.259168</td>\n      <td>51.579186</td>\n      <td>-0.209020</td>\n      <td>0.079466</td>\n      <td>0.104600</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55940994</th>\n      <td>385000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.403987</td>\n      <td>51.376930</td>\n      <td>-0.238870</td>\n      <td>0.122790</td>\n      <td>0.134450</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56449305</th>\n      <td>380000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.310271</td>\n      <td>51.600483</td>\n      <td>-0.062096</td>\n      <td>0.100763</td>\n      <td>0.042324</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57221413</th>\n      <td>475000.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.409784</td>\n      <td>51.497260</td>\n      <td>-0.422530</td>\n      <td>0.002460</td>\n      <td>0.318110</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57878227</th>\n      <td>490000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.052498</td>\n      <td>51.580270</td>\n      <td>0.022290</td>\n      <td>0.080550</td>\n      <td>0.126710</td>\n      <td>SHARE_OF_FREEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59258796</th>\n      <td>475000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.424573</td>\n      <td>51.536335</td>\n      <td>-0.068537</td>\n      <td>0.036615</td>\n      <td>0.035883</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59658138</th>\n      <td>499995.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.396544</td>\n      <td>51.462211</td>\n      <td>-0.196876</td>\n      <td>0.037509</td>\n      <td>0.092456</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>60741240</th>\n      <td>435000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.162014</td>\n      <td>51.612150</td>\n      <td>-0.277430</td>\n      <td>0.112430</td>\n      <td>0.173010</td>\n      <td>LEASEHOLD</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>61387062</th>\n      <td>375000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.493102</td>\n      <td>51.448697</td>\n      <td>-0.174068</td>\n      <td>0.051023</td>\n      <td>0.069648</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Price'] = df['Price'] / price_divisor # potentially making the price smaller to make the ANN perform better\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.175019Z",
     "iopub.status.busy": "2022-12-07T14:05:11.174797Z",
     "iopub.status.idle": "2022-12-07T14:05:11.211907Z",
     "shell.execute_reply": "2022-12-07T14:05:11.211335Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.174998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44127, 19)\n",
      "(39714, 21) (4413, 21) (39714, 1) (4413, 1) (39714, 1) (4413, 1) (39714, 1) (4413, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_index, X_test_index, y_train_index, y_test_index, df_features, df_labels = create_train_test_data(\n",
    "    df,\n",
    "    categories=categories,\n",
    "    RANDOM_STATE=RANDOM_STATE, return_index=True,\n",
    "    drop_nulls=True,\n",
    "    no_dummies=no_dummies\n",
    ")\n",
    "\n",
    "#print(X_train[0])\n",
    "print(df.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train_index.shape, X_test_index.shape,\n",
    "      y_train_index.shape, y_test_index.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\r\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\r\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot) (3.0.9)\r\n",
      "Installing collected packages: pydot\r\n",
      "Successfully installed pydot-1.4.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/400\n",
      "2483/2483 - 9s - loss: 0.0282 - val_loss: 0.0046 - 9s/epoch - 3ms/step\n",
      "Epoch 2/400\n",
      "2483/2483 - 8s - loss: 0.0072 - val_loss: 0.0027 - 8s/epoch - 3ms/step\n",
      "Epoch 3/400\n",
      "2483/2483 - 8s - loss: 0.0052 - val_loss: 0.0019 - 8s/epoch - 3ms/step\n",
      "Epoch 4/400\n",
      "2483/2483 - 8s - loss: 0.0044 - val_loss: 0.0017 - 8s/epoch - 3ms/step\n",
      "Epoch 5/400\n",
      "2483/2483 - 8s - loss: 0.0039 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 6/400\n",
      "2483/2483 - 8s - loss: 0.0036 - val_loss: 9.8320e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 7/400\n",
      "2483/2483 - 8s - loss: 0.0032 - val_loss: 0.0012 - 8s/epoch - 3ms/step\n",
      "Epoch 8/400\n",
      "2483/2483 - 8s - loss: 0.0031 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 9/400\n",
      "2483/2483 - 8s - loss: 0.0029 - val_loss: 9.7219e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 10/400\n",
      "2483/2483 - 8s - loss: 0.0029 - val_loss: 9.0200e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 11/400\n",
      "2483/2483 - 8s - loss: 0.0027 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 12/400\n",
      "2483/2483 - 8s - loss: 0.0027 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 13/400\n",
      "2483/2483 - 8s - loss: 0.0026 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 14/400\n",
      "2483/2483 - 8s - loss: 0.0025 - val_loss: 9.9082e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 15/400\n",
      "2483/2483 - 8s - loss: 0.0024 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 16/400\n",
      "2483/2483 - 8s - loss: 0.0023 - val_loss: 7.9326e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 17/400\n",
      "2483/2483 - 8s - loss: 0.0023 - val_loss: 6.7560e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 18/400\n",
      "2483/2483 - 8s - loss: 0.0022 - val_loss: 7.4313e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 19/400\n",
      "2483/2483 - 8s - loss: 0.0022 - val_loss: 9.9536e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 20/400\n",
      "2483/2483 - 8s - loss: 0.0022 - val_loss: 6.0048e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 21/400\n",
      "2483/2483 - 8s - loss: 0.0021 - val_loss: 7.5689e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 22/400\n",
      "2483/2483 - 8s - loss: 0.0021 - val_loss: 8.7469e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 23/400\n",
      "2483/2483 - 8s - loss: 0.0020 - val_loss: 8.3346e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 24/400\n",
      "2483/2483 - 8s - loss: 0.0021 - val_loss: 8.0712e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 25/400\n",
      "2483/2483 - 8s - loss: 0.0021 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 26/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 5.2491e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 27/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 7.9613e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 28/400\n",
      "2483/2483 - 8s - loss: 0.0020 - val_loss: 5.0713e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 29/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 9.7526e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 30/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 6.9407e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 31/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 6.8299e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 32/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 4.6822e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 33/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 5.0799e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 34/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 6.0196e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 35/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 8.7043e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 36/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 6.4433e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 37/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 5.4640e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 38/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 9.5361e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 39/400\n",
      "2483/2483 - 8s - loss: 0.0019 - val_loss: 5.3230e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 40/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 8.6304e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 41/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 5.4331e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 42/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 8.1073e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 43/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 7.7230e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 44/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 6.2096e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 45/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 7.8360e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 46/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 5.3094e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 47/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 5.1637e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 48/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 4.2687e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 49/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 4.9876e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 50/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 5.6585e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 51/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 7.7469e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 52/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 4.9688e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 53/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 6.0632e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 54/400\n",
      "2483/2483 - 8s - loss: 0.0018 - val_loss: 6.1476e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 55/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.2216e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 56/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.1721e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 57/400\n",
      "2483/2483 - 8s - loss: 0.0017 - val_loss: 4.7868e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 58/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.4069e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 59/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 5.5579e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 60/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 5.9687e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 61/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 5.4015e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 62/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 5.3416e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 63/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 6.0987e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 64/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.3833e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 65/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 4.3505e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 66/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 6.0213e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 67/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.9563e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 68/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.0554e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 69/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 2.8012e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 70/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 4.9041e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 71/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 4.3047e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 72/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 5.0690e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 73/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 3.0226e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 74/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 3.1513e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 75/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 4.1416e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 76/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 4.3828e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 77/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.6792e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 78/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 4.3833e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 79/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 3.7896e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 80/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.5916e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 81/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 4.9569e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 82/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 6.9326e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 83/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 3.7601e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 84/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 6.4710e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 85/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 5.6274e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 86/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 5.2973e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 87/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.8548e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 88/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 5.0112e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 89/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 6.5953e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 90/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.3289e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 91/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 5.5294e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 92/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 5.9663e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 93/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.4621e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 94/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.1110e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 95/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.3482e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 96/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.5372e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 97/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.8821e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 98/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.2621e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 99/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.6086e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 100/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.5987e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 101/400\n",
      "2483/2483 - 8s - loss: 0.0016 - val_loss: 6.6179e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 102/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.6726e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 103/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.9206e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 104/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 105/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.8479e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 106/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.7695e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 107/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 9.6106e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 108/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.3077e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 109/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.0146e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 110/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 111/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 9.4090e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 112/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.3704e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 113/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.0519e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 114/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.7494e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 115/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 5.7101e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 116/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.4550e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 117/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 118/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 8.1443e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 119/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.0217e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 120/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.0051e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 121/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.7372e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 122/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.4322e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 123/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.8156e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 124/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.5249e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 125/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.8041e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 126/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 127/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.7700e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 128/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.7514e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 129/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.7796e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 130/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.3472e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 131/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 5.9641e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 132/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.1225e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 133/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 134/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.0968e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 135/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.0659e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 136/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.6144e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 137/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.4962e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 138/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.7708e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 139/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.9144e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 140/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.7822e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 141/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.0469e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 142/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.2254e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 143/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 9.0384e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 144/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.2807e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 145/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.9375e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 146/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 9.6475e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 147/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.8331e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 148/400\n",
      "2483/2483 - 8s - loss: 0.0015 - val_loss: 7.5030e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 149/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.3033e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 150/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.2922e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 151/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.1853e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 152/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 0.0013 - 8s/epoch - 3ms/step\n",
      "Epoch 153/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.2317e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 154/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.3958e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 155/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.8234e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 156/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.0997e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 157/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.7927e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 158/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.2431e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 159/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.8152e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 160/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.4827e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 161/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.1412e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 162/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.1949e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 163/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.1324e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 164/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.2477e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 165/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 5.6025e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 166/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 9.8573e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 167/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.3371e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 168/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.1864e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 169/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.5357e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 170/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.2466e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 171/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 5.9243e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 172/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.6457e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 173/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.4489e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 174/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.8751e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 175/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.3840e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 176/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 7.1846e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 177/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.4362e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 178/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.4389e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 179/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 5.7598e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 180/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.8033e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 181/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.1799e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 182/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.4067e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 183/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.9143e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 184/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.1107e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 185/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.0453e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 186/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.3437e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 187/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.7486e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 188/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.7047e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 189/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.2485e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 190/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 9.5109e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 191/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 5.5665e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 192/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.3061e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 193/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.4504e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 194/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.3855e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 195/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.2257e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 196/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 6.6118e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 197/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.0706e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 198/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.9310e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 199/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.0923e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 200/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.3270e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 201/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.6605e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 202/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.5638e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 203/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.6167e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 204/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.6291e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 205/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.1239e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 206/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.6456e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 207/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.7556e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 208/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.6008e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 209/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.6131e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 210/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 211/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.6120e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 212/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.2920e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 213/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.8446e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 214/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.5925e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 215/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.2608e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 216/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 5.7009e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 217/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.6928e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 218/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 8.4639e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 219/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.4031e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 220/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.2948e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 221/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 222/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.0647e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 223/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.2831e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 224/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.7273e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 225/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.2224e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 226/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.4108e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 227/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.8477e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 228/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.1294e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 229/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.8994e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 230/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.2459e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 231/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.6007e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 232/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.4796e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 233/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.4078e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 234/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6078e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 235/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 5.9535e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 236/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 237/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.6585e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 238/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6881e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 239/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.4979e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 240/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.5618e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 241/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.0559e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 242/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.8217e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 243/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.0483e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 244/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.7736e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 245/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.1945e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 246/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.5778e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 247/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.9758e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 248/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.4761e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 249/400\n",
      "2483/2483 - 8s - loss: 0.0014 - val_loss: 9.2149e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 250/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.3357e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 251/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.7933e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 252/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.3949e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 253/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.5730e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 254/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 7.3281e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 255/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 9.2011e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 256/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.4597e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 257/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.1629e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 258/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6582e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 259/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.8996e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 260/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.1151e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 261/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.0474e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 262/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.4380e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 263/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.7183e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 264/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.4319e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 265/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.2503e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 266/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.7175e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 267/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.2597e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 268/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.4859e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 269/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 8.0473e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 270/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.3388e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 271/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.9036e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 272/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.9362e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 273/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6022e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 274/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 275/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.4762e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 276/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 5.9718e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 277/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.6099e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 278/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.1937e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 279/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6662e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 280/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.6051e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 281/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.3308e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 282/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.2021e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 283/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.6092e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 284/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.6564e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 285/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.8352e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 286/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.1336e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 287/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.6450e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 288/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.2182e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 289/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 0.0012 - 8s/epoch - 3ms/step\n",
      "Epoch 290/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.6537e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 291/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.4207e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 292/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.4806e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 293/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.2267e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 294/400\n",
      "2483/2483 - 8s - loss: 0.0013 - val_loss: 6.9151e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 295/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.7178e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 296/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.1519e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 297/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.8109e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 298/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.4200e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 299/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.7772e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 300/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.5573e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 301/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.1229e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 302/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.0617e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 303/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.7328e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 304/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.4605e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 305/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.2584e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 306/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.3710e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 307/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.9947e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 308/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.9770e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 309/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.3998e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 310/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.1890e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 311/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.6085e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 312/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.5007e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 313/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.4437e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 314/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6035e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 315/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.0702e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 316/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.3543e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 317/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.6521e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 318/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.2503e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 319/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.2156e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 320/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.9697e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 321/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.5441e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 322/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.4043e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 323/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.2263e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 324/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.0293e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 325/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.1239e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 326/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0013 - 8s/epoch - 3ms/step\n",
      "Epoch 327/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.9312e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 328/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.2898e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 329/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 330/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.0638e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 331/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.1068e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 332/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.7574e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 333/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.6112e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 334/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.0931e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 335/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.0012e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 336/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.2804e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 337/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.4334e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 338/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.7115e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 339/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0012 - 8s/epoch - 3ms/step\n",
      "Epoch 340/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.5021e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 341/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 5.9189e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 342/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.4427e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 343/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.6155e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 344/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.7864e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 345/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.2933e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 346/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.9866e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 347/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.2110e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 348/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 349/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.5966e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 350/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.7690e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 351/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 352/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.6789e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 353/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.5444e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 354/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.5432e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 355/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.9359e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 356/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.0283e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 357/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.0837e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 358/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.5507e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 359/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.3203e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 360/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.9546e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 361/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 8.5227e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 362/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.5834e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 363/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.0530e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 364/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.6340e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 365/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0012 - 8s/epoch - 3ms/step\n",
      "Epoch 366/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.2965e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 367/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.0154e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 368/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.4906e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 369/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.1042e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 370/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0011 - 8s/epoch - 3ms/step\n",
      "Epoch 371/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0010 - 8s/epoch - 3ms/step\n",
      "Epoch 372/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 6.4326e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 373/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.2269e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 374/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.9805e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 375/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.9399e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 376/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.6379e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 377/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 7.2268e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 378/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.0652e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 379/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.3614e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 380/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.2789e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 381/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.5405e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 382/400\n",
      "2483/2483 - 8s - loss: 0.0012 - val_loss: 9.4477e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 383/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.9586e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 384/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.3876e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 385/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.6215e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 386/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.8068e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 387/400\n",
      "2483/2483 - 8s - loss: 0.0010 - val_loss: 6.7136e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 388/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.9297e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 389/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 5.4635e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 390/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.5478e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 391/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.9378e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 392/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.1022e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 393/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 6.1202e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 394/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.5646e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 395/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 7.3461e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 396/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.5061e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 397/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.2153e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 398/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 9.9588e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 399/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 8.0286e-04 - 8s/epoch - 3ms/step\n",
      "Epoch 400/400\n",
      "2483/2483 - 8s - loss: 0.0011 - val_loss: 0.0016 - 8s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# train autoencoder for regression with no compression in the bottleneck layer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "#X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "# number of input columns\n",
    "n_inputs = X_train.shape[1]\n",
    "# split into train test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# scale data\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# define encoder\n",
    "visible = Input(shape=(n_inputs,))\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = ReLU()(e)\n",
    "# define bottleneck\n",
    "n_bottleneck = n_inputs\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "# define decoder\n",
    "d = Dense(n_inputs*2)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = ReLU()(d)\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(d)\n",
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# plot the autoencoder\n",
    "plot_model(model, 'autoencoder.png', show_shapes=True)\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train, X_train, epochs=400, batch_size=16, verbose=2, validation_data=(X_test,X_test))\n",
    "# plot loss\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "# define an encoder model (without the decoder)\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "plot_model(encoder, 'encoder.png', show_shapes=True)\n",
    "# save the encoder to file\n",
    "encoder.save('encoder.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "[525000. 225000. 475000. 475000. 425000.]\n",
      "[400000. 369950. 350000. 549000. 495000.]\n",
      "Scaled:\n",
      "[[0.85]\n",
      " [0.25]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [0.65]]\n",
      "[[0.6   ]\n",
      " [0.5399]\n",
      " [0.5   ]\n",
      " [0.898 ]\n",
      " [0.79  ]]\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1242/1242 [==============================] - 1s 1ms/step\n",
      "138/138 [==============================] - 0s 953us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55959.007878356526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "\n",
    "# support vector regression performance with encoded input\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import load_model\n",
    "# define dataset\n",
    "#???X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1, random_state=1)\n",
    "# split into train test sets\n",
    "#???X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(\"Before:\")\n",
    "print(y_train[0:5])\n",
    "print(y_test[0:5])\n",
    "# reshape target variables so that we can transform them\n",
    "y_train = y_train.reshape((len(y_train), 1))\n",
    "y_test = y_test.reshape((len(y_test), 1))\n",
    "# scale input data\n",
    "trans_in = MinMaxScaler()\n",
    "trans_in.fit(X_train)\n",
    "X_train = trans_in.transform(X_train)\n",
    "X_test = trans_in.transform(X_test)\n",
    "# scale output data\n",
    "trans_out = MinMaxScaler()\n",
    "trans_out.fit(y_train)\n",
    "y_train = trans_out.transform(y_train)\n",
    "y_test = trans_out.transform(y_test)\n",
    "print(\"Scaled:\")\n",
    "print(y_train[0:5])\n",
    "print(y_test[0:5])\n",
    "# load the model from file\n",
    "encoder = load_model('encoder.h5')\n",
    "# encode the train data\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "# encode the test data\n",
    "X_test_encode = encoder.predict(X_test)\n",
    "# define model\n",
    "model = SVR()\n",
    "# fit model on the training dataset\n",
    "model.fit(X_train_encode, y_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(X_test_encode)\n",
    "# invert transforms so we can calculate errors\n",
    "yhat = yhat.reshape((len(yhat), 1))\n",
    "yhat = trans_out.inverse_transform(yhat)\n",
    "y_test = trans_out.inverse_transform(y_test)\n",
    "# calculate error\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[400000.]\n",
      " [369950.]\n",
      " [350000.]\n",
      " ...\n",
      " [400000.]\n",
      " [450000.]\n",
      " [275000.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1242/1242 [==============================] - 1s 993us/step\n",
      "138/138 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212534162863.56067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from keras.saving.save import load_model\n",
    "\n",
    "# load the model from file\n",
    "encoder = load_model('encoder.h5')\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "# encode the test data\n",
    "X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "# define model\n",
    "model = SVR()\n",
    "# fit model on the training dataset\n",
    "model.fit(X_train_encode, y_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(X_test_encode)\n",
    "\n",
    "yhat = model.predict(X_test_encode)\n",
    "# invert transforms so we can calculate errors\n",
    "yhat = yhat.reshape((len(yhat), 1))\n",
    "yhat = trans_out.inverse_transform(yhat)\n",
    "y_test = trans_out.inverse_transform(y_test)\n",
    "# calculate error\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400000.] [405828.26935591]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0], yhat[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage:\n",
    "* #### retrieve the hyperparameters for this model, and\n",
    "* #### train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.213435Z",
     "iopub.status.busy": "2022-12-07T14:05:11.212918Z",
     "iopub.status.idle": "2022-12-07T14:05:12.601138Z",
     "shell.execute_reply": "2022-12-07T14:05:12.600488Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.213410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'mae +epochs=400 +learn=0.0003'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_model, ALGORITHM_DETAIL, chosen_epochs, chosen_params = make_simple_ann(selected_neural_network)\n",
    "\n",
    "ALGORITHM_DETAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:12.602626Z",
     "iopub.status.busy": "2022-12-07T14:05:12.602064Z",
     "iopub.status.idle": "2022-12-07T14:05:12.617161Z",
     "shell.execute_reply": "2022-12-07T14:05:12.616542Z",
     "shell.execute_reply.started": "2022-12-07T14:05:12.602602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_neural_network m15 mega + dropout\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, 21)               43        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               2816      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     multiple                  0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,413,100\n",
      "Trainable params: 2,405,889\n",
      "Non-trainable params: 7,211\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"selected_neural_network\",selected_neural_network)\n",
    "trainable_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:12.621501Z",
     "iopub.status.busy": "2022-12-07T14:05:12.621019Z",
     "iopub.status.idle": "2022-12-07T14:16:30.291615Z",
     "shell.execute_reply": "2022-12-07T14:16:30.290697Z",
     "shell.execute_reply.started": "2022-12-07T14:05:12.621480Z"
    },
    "pycharm": {
     "name": "#%%time\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1117/1117 [==============================] - 10s 7ms/step - loss: 0.1628 - val_loss: 0.1212\n",
      "Epoch 2/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1283 - val_loss: 0.1262\n",
      "Epoch 3/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1230 - val_loss: 0.1168\n",
      "Epoch 4/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1206 - val_loss: 0.1142\n",
      "Epoch 5/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1178 - val_loss: 0.1182\n",
      "Epoch 6/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1165 - val_loss: 0.1103\n",
      "Epoch 7/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1150 - val_loss: 0.1155\n",
      "Epoch 8/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1141 - val_loss: 0.1073\n",
      "Epoch 9/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1138 - val_loss: 0.1083\n",
      "Epoch 10/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1128 - val_loss: 0.1084\n",
      "Epoch 11/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1114 - val_loss: 0.1069\n",
      "Epoch 12/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1115 - val_loss: 0.1146\n",
      "Epoch 13/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1102 - val_loss: 0.1057\n",
      "Epoch 14/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1101 - val_loss: 0.1090\n",
      "Epoch 15/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1087 - val_loss: 0.1029\n",
      "Epoch 16/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1080 - val_loss: 0.1037\n",
      "Epoch 17/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1072 - val_loss: 0.1057\n",
      "Epoch 18/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1066 - val_loss: 0.1035\n",
      "Epoch 19/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1062 - val_loss: 0.1020\n",
      "Epoch 20/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1058 - val_loss: 0.1044\n",
      "Epoch 21/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1051 - val_loss: 0.1030\n",
      "Epoch 22/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1049 - val_loss: 0.1033\n",
      "Epoch 23/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1046 - val_loss: 0.1046\n",
      "Epoch 24/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1040 - val_loss: 0.1030\n",
      "Epoch 25/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1033 - val_loss: 0.1034\n",
      "Epoch 26/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1028 - val_loss: 0.1021\n",
      "Epoch 27/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1026 - val_loss: 0.1005\n",
      "Epoch 28/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1024 - val_loss: 0.1048\n",
      "Epoch 29/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1020 - val_loss: 0.1036\n",
      "Epoch 30/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1013 - val_loss: 0.1074\n",
      "Epoch 31/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1013 - val_loss: 0.1008\n",
      "Epoch 32/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1008 - val_loss: 0.1018\n",
      "Epoch 33/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1006 - val_loss: 0.1047\n",
      "Epoch 34/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.1001 - val_loss: 0.1003\n",
      "Epoch 35/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0996 - val_loss: 0.1005\n",
      "Epoch 36/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0996 - val_loss: 0.1000\n",
      "Epoch 37/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0991 - val_loss: 0.1036\n",
      "Epoch 38/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0992 - val_loss: 0.1014\n",
      "Epoch 39/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0983 - val_loss: 0.1007\n",
      "Epoch 40/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0982 - val_loss: 0.1023\n",
      "Epoch 41/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0982 - val_loss: 0.1021\n",
      "Epoch 42/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0981 - val_loss: 0.0990\n",
      "Epoch 43/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0978 - val_loss: 0.1002\n",
      "Epoch 44/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0974 - val_loss: 0.1031\n",
      "Epoch 45/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0975 - val_loss: 0.1039\n",
      "Epoch 46/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0970 - val_loss: 0.1003\n",
      "Epoch 47/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0968 - val_loss: 0.0988\n",
      "Epoch 48/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0964 - val_loss: 0.0989\n",
      "Epoch 49/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0962 - val_loss: 0.1005\n",
      "Epoch 50/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0959 - val_loss: 0.1007\n",
      "Epoch 51/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0965 - val_loss: 0.0999\n",
      "Epoch 52/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0953 - val_loss: 0.0990\n",
      "Epoch 53/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0953 - val_loss: 0.1002\n",
      "Epoch 54/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0955 - val_loss: 0.1016\n",
      "Epoch 55/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0949 - val_loss: 0.0992\n",
      "Epoch 56/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0949 - val_loss: 0.1002\n",
      "Epoch 57/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0949 - val_loss: 0.0991\n",
      "Epoch 58/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0943 - val_loss: 0.0997\n",
      "Epoch 59/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0940 - val_loss: 0.0989\n",
      "Epoch 60/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0943 - val_loss: 0.0987\n",
      "Epoch 61/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0937 - val_loss: 0.1000\n",
      "Epoch 62/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0939 - val_loss: 0.0998\n",
      "Epoch 63/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0937 - val_loss: 0.0999\n",
      "Epoch 64/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0934 - val_loss: 0.0988\n",
      "Epoch 65/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0932 - val_loss: 0.0980\n",
      "Epoch 66/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0933 - val_loss: 0.0988\n",
      "Epoch 67/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0930 - val_loss: 0.0994\n",
      "Epoch 68/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0928 - val_loss: 0.0995\n",
      "Epoch 69/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0921 - val_loss: 0.0978\n",
      "Epoch 70/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0923 - val_loss: 0.0988\n",
      "Epoch 71/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0923 - val_loss: 0.0985\n",
      "Epoch 72/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0923 - val_loss: 0.0979\n",
      "Epoch 73/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0917 - val_loss: 0.0986\n",
      "Epoch 74/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0921 - val_loss: 0.0986\n",
      "Epoch 75/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0918 - val_loss: 0.0985\n",
      "Epoch 76/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0916 - val_loss: 0.0988\n",
      "Epoch 77/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0913 - val_loss: 0.0976\n",
      "Epoch 78/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0914 - val_loss: 0.1001\n",
      "Epoch 79/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0913 - val_loss: 0.0974\n",
      "Epoch 80/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0907 - val_loss: 0.0996\n",
      "Epoch 81/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0905 - val_loss: 0.0977\n",
      "Epoch 82/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0908 - val_loss: 0.0975\n",
      "Epoch 83/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0908 - val_loss: 0.0982\n",
      "Epoch 84/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0903 - val_loss: 0.1005\n",
      "Epoch 85/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0904 - val_loss: 0.0990\n",
      "Epoch 86/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0900 - val_loss: 0.1010\n",
      "Epoch 87/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0899 - val_loss: 0.1015\n",
      "Epoch 88/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0899 - val_loss: 0.0984\n",
      "Epoch 89/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0900 - val_loss: 0.1010\n",
      "Epoch 90/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0896 - val_loss: 0.0985\n",
      "Epoch 91/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0894 - val_loss: 0.0988\n",
      "Epoch 92/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0894 - val_loss: 0.0976\n",
      "Epoch 93/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0891 - val_loss: 0.0973\n",
      "Epoch 94/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0890 - val_loss: 0.0980\n",
      "Epoch 95/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0890 - val_loss: 0.0985\n",
      "Epoch 96/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0890 - val_loss: 0.0993\n",
      "Epoch 97/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0884 - val_loss: 0.0986\n",
      "Epoch 98/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0886 - val_loss: 0.0998\n",
      "Epoch 99/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0881 - val_loss: 0.0981\n",
      "Epoch 100/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0881 - val_loss: 0.0985\n",
      "Epoch 101/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0880 - val_loss: 0.0972\n",
      "Epoch 102/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0881 - val_loss: 0.0980\n",
      "Epoch 103/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0876 - val_loss: 0.0978\n",
      "Epoch 104/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0881 - val_loss: 0.0975\n",
      "Epoch 105/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0878 - val_loss: 0.0983\n",
      "Epoch 106/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0874 - val_loss: 0.0982\n",
      "Epoch 107/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0875 - val_loss: 0.0998\n",
      "Epoch 108/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0872 - val_loss: 0.0998\n",
      "Epoch 109/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0869 - val_loss: 0.0996\n",
      "Epoch 110/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0868 - val_loss: 0.0975\n",
      "Epoch 111/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0868 - val_loss: 0.0985\n",
      "Epoch 112/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0865 - val_loss: 0.0984\n",
      "Epoch 113/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0866 - val_loss: 0.0980\n",
      "Epoch 114/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0867 - val_loss: 0.0985\n",
      "Epoch 115/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0866 - val_loss: 0.0982\n",
      "Epoch 116/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0864 - val_loss: 0.0976\n",
      "Epoch 117/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0864 - val_loss: 0.0980\n",
      "Epoch 118/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0860 - val_loss: 0.0984\n",
      "Epoch 119/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0860 - val_loss: 0.0984\n",
      "Epoch 120/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0858 - val_loss: 0.0996\n",
      "Epoch 121/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0859 - val_loss: 0.0976\n",
      "Epoch 122/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0856 - val_loss: 0.1025\n",
      "Epoch 123/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0858 - val_loss: 0.0976\n",
      "Epoch 124/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0855 - val_loss: 0.0986\n",
      "Epoch 125/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0853 - val_loss: 0.1001\n",
      "Epoch 126/400\n",
      "1112/1117 [============================>.] - ETA: 0s - loss: 0.0853Restoring model weights from the end of the best epoch: 101.\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 0.0853 - val_loss: 0.0985\n",
      "Epoch 126: early stopping\n"
     ]
    }
   ],
   "source": [
    "val_split = 0.1\n",
    "min_delta=0 #10, #50, #10, #50,\n",
    "val_delta_patience = 25 # 10\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", #\"loss\", #\"val_loss\",\n",
    "    min_delta=min_delta, \n",
    "    patience=val_delta_patience,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True # False,\n",
    ")\n",
    "\n",
    "pipe_start = time()\n",
    "\n",
    "history = trainable_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=chosen_epochs,\n",
    "    # verbose=0 to suppress logging.\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split=val_split,  #0.2,\n",
    "    callbacks=[callback],\n",
    ")\n",
    "pipe_end = time()\n",
    "estimated_time = round((pipe_end - pipe_start), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:16:30.293337Z",
     "iopub.status.busy": "2022-12-07T14:16:30.292573Z",
     "iopub.status.idle": "2022-12-07T14:16:30.296473Z",
     "shell.execute_reply": "2022-12-07T14:16:30.295872Z",
     "shell.execute_reply.started": "2022-12-07T14:16:30.293311Z"
    }
   },
   "outputs": [],
   "source": [
    "#ALGORITHM_DETAIL.replace(\"epochs=\", f\"epochs={len(hist)}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Get the results and print some graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:36.528561Z",
     "iopub.status.busy": "2022-12-07T14:51:36.527933Z",
     "iopub.status.idle": "2022-12-07T14:51:36.543328Z",
     "shell.execute_reply": "2022-12-07T14:51:36.542380Z",
     "shell.execute_reply.started": "2022-12-07T14:51:36.528531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped at 126, loss=0.09 valloss=0.1\n",
      "loss=8.53e-02 valloss=9.72e-02 +valsplit=0.1 +patn=25 stop=126/400 \n",
      "mae +epochs=400 +learn=0.0003\n"
     ]
    },
    {
     "data": {
      "text/plain": "         loss  val_loss  epoch\n121  0.085584  0.102539    121\n122  0.085786  0.097605    122\n123  0.085499  0.098600    123\n124  0.085337  0.100067    124\n125  0.085297  0.098536    125",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n      <th>epoch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>121</th>\n      <td>0.085584</td>\n      <td>0.102539</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>0.085786</td>\n      <td>0.097605</td>\n      <td>122</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>0.085499</td>\n      <td>0.098600</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>0.085337</td>\n      <td>0.100067</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>0.085297</td>\n      <td>0.098536</td>\n      <td>125</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "early_end_lossX = hist.iloc[-1]['loss']\n",
    "early_end_loss = hist['loss'].min()\n",
    "early_end_valloss = hist['val_loss'].min()\n",
    "\n",
    "#more_detail = f\"loss={round(early_end_loss,2)} valloss={round(early_end_valloss,2)}\"\n",
    "more_detail = f\"loss={early_end_loss:.2e} valloss={early_end_valloss:.2e}\"\n",
    "more_detail += f' +valsplit={val_split} +patn={val_delta_patience}'\n",
    "\n",
    "# f\"{x:.2e}\"\n",
    "\n",
    "if len(hist) != chosen_epochs:\n",
    "    print(f'stopped at {len(hist)}, loss={round(early_end_loss,2)} valloss={round(early_end_valloss,2)}')\n",
    "    #ALGORITHM_DETAIL += f\" +stop={len(hist)}\"\n",
    "    more_detail += f\" stop={len(hist)}/{chosen_epochs} \"\n",
    "    #more_detail += ALGORITHM_DETAIL.replace(\"epochs=\", f\"epochs={len(hist)}/\")\n",
    "\n",
    "\n",
    "if price_divisor!=1:\n",
    "    print('in preprocessing, divided all Prices by ', price_divisor)\n",
    "    more_detail += f' div={price_divisor}'\n",
    "\n",
    "\n",
    "print(more_detail)\n",
    "print(ALGORITHM_DETAIL)\n",
    "    \n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:36.545715Z",
     "iopub.status.busy": "2022-12-07T14:51:36.544947Z",
     "iopub.status.idle": "2022-12-07T14:51:36.688602Z",
     "shell.execute_reply": "2022-12-07T14:51:36.687744Z",
     "shell.execute_reply.started": "2022-12-07T14:51:36.545690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.04094029963017\n",
      "10.004094029963017\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAApDUlEQVR4nO3deZxU1Zn/8U+zNSoukSVGmwg68qSJCw7oz8nELArGQZEorlGJApohZoJkRCXEiYk/o1FigppRE9SYn464QVAHghqj6LiFDjiIxaMmaGxQUJDgQjdL1++PcxqKoqq6uruqqwq+79er7XvPPafqOV3ST997zz2nKplMIiIiUkidSh2AiIjseJRcRESk4JRcRESk4JRcRESk4JRcRESk4LqUOoBysGjRomR1dXWr223euIGq916lExpxJyKV6aOuPenR+7NtavvJJ5+8P3jw4N6Zjim5ANXV1dTW1rap7caPakgsXsS+n+3H5mQSmppIJpsASDY1QVUVW0Z7J5OQbAKqoKpqS1kyuSmUpdg6QjwZ63faup/cDFSRjG2qSIb3SlVVtTXlbTPcPLlNW4D6t9+ipuazW9ptG0ja6255jS1vtE27MLS9Kn4lgSRVacPdk1Upx5t/Htu9ftW2saS8RqbR8/X19dTU1OSMbdvGyfjfDPUzyvYHRPrn1vY/NJbX17NfTU3uSlt+vi1Jr9eauNL6lGfb5cvr2W+/LPGnfwbp/581l6fXTyYzft7FkrMPZSKZ63/ZJCxfsTylD8ntP/q0NoM+N4iuXbu1KZa6urq3sh1Tcmmnrj160nWvfem134GlDqXNPmmqZv/PtS25louGql3p38Y/EMrFhk49OLCC+7Cx8+78QwXHD7Cpy+4cVOF92Nw1wYAy6IPuuYiISMEpuYiISMHpspiI7LQ2btxIfX09DQ0NW/YTiUSJo2qfYvShe/fu1NTU0LVr17zbKLmIyE6rvr6e3XffnX79+lFVVcX69evZZZddSh1WuxS6D8lkktWrV1NfX0///v3zbqfLYiKy02poaKBnz55UZRq9JgBUVVXRs2fPLWd3+VJyEZGdmhJLy9ryM1JyERGRglNyEREpocMPP7zUIRSFkouIiBScRouJiJSBZDLJddddxzPPPENVVRXjx49n+PDhrFq1iokTJ/LRRx+xefNmrrzySg4//HCmTJnCK6+8QlVVFaNGjeK8884rdRe2oeQiIgI8VFfPjJfeolOnwl3QOX1IX0YNzm+usscee4ylS5cye/ZsPvjgA0499VSGDBnCo48+yhe/+EXGjx/P5s2bWb9+PYlEgpUrV/Loo48CsG7duoLFXCi6LCYiUgbq6uo44YQT6Ny5M7169eKII45g8eLFHHLIIcycOZObbrqJ1157jR49etC3b1/efvttrrrqKubPn0+PHj1KHf52dOYiIgKMGlzD8IE9y+4hyiOOOIK7776bp59+mssvv5zzzz+fr3/968yePZtnn32WGTNmMHfuXK655ppSh7oNnbmIiJSBIUOGMHfuXDZv3syaNWtYsGABhx56KMuXL6dXr16cfvrpnHbaaSxZsoQ1a9aQTCb52te+xsUXX8yrr75a6vC3ozMXEZEyMGzYMBYuXMjIkSOpqqpi0qRJ9O7dm1mzZnH77bfTpUsXdt11V37605+yatUqJk+eTFNcx+l73/teiaPfnpKLiEgJLVy4EAhPwV922WVcdtll2xw/+eSTOfnkk7drN2vWrA6Jr63KLrmY2WHArUAP4E3gbHffbiiEmU0ExhHWWVsMnO/uDWZ2LHA94ZLfR8B57v5GB4UvIiKU5z2X6cDl7n4IMAuYlF7BzPYDvgsMcfeDgc7AmfHwLYSENAj4L+AHHRG0iIhsVY7JZQAwP24/DozKUq8LsIuZdQF2BVbE8iSwR9zeM6VcREQ6SNldFgOWACOB3wGnAX3TK7j7cjObCvwNWA885u6PxcPjgDlmth5YBxzV0hs2Nja2a3GdhoaGil5gqNLjB/WhHFRi/Bs3bmT9+vVb9pPJ5Db7lahYfWjtImQlSS5m9gSwT4ZDU4AxwI1mdgXwMLAhQ/tPERJQf2At8ICZnePudwMTgeHu/qKZTQJuICScrKqrq6mtrW1zfxKJRLval1qlxw/qQzmoxPgTicQ2z7VosbDsunbtut3nW1dXl7V+SZKLuw9tocpxAGY2ADghw/GhwDJ3fy/Wmwl8wczmAYe5+4ux3n3A7wsTtYiI5Kvs7rmYWZ/4vRPhZvytGar9DTjKzHY1syrgWCABfADsGZMSwLBYLiIiHajskgtwlpm9Biwl3Iy/E8DM9jWzOQDxzORB4M+EYcidgF+5+ybgAuAhM3sZOJcMo81ERCpRrrVf6uvrOfHEEzswmtzK7oa+u08DpmUoXwEMT9n/IfDDDPVmEYYwi4hIiZRdchERKYlF99Kt7i7o1Llwr3n4OTDorKyHp06dymc+8xnOPvtsAG666SY6d+7Miy++yLp169i0aRMTJkxg6NCWblNvq7GxkSuvvJJXXnmFzp07c/nll3PUUUfx+uuvM3nyZDZu3EhTUxM33XQTffr04eKLL+bdd9+lqamJb3/72wwfPrzlN2mBkouISIkMHz6cn/zkJ1uSy9y5c7n99tsZPXo0PXr0YM2aNZxxxhkce+yxVFVV5f2699xzDwCPPPIIf/nLXxg7dizz5s1jxowZjB49mpNOOokNGzbQ1NTE008/TZ8+ffjVr34FwIcffliQvim5iIgADDqLDfb1Dh2KPHDgQFavXs3KlSv54IMP2GOPPejVqxfXXHMNf/rTn+jUqRMrV67k/fffp3fv3nm/bl1dHeeccw4ABx54IPvuuy/Lli1j0KBB3Hrrrbz77rscd9xx9OvXjwEDBvDTn/6U66+/nq9+9asMGTKkIH0rxxv6IiI7jeOPP5558+YxZ84chg8fziOPPMKaNWuYOXMms2fPplevXjQ2NhbkvUaMGMEtt9xC9+7dufDCC3n++efp378/M2fOZMCAAfziF7/g5ptvLsh7KbmIiJTQ8OHDmTNnDvPmzeP444/nww8/pGfPnnTt2pUXXniB5cuXt/o1hwwZwiOPPALAsmXLeOeddzjggAN4++236du3L6NHj+bYY4/F3Vm5ciW77LILI0eOZOzYsQVbG0aXxURESuiggw7i448/pk+fPvTp04cRI0Ywfvx4RowYwcEHH8wBBxzQ6tf8xje+wZVXXsmIESPo3Lkz11xzDd26dWPu3LnMnj2bLl260KtXL771rW+xePFirrvuOjp16kSXLl248sorC9IvJRcRkRJrPssA2Hvvvbnvvvsy1mte+yWTmpoaHn30UdavX091dXXGZY8vvPBCLrzwwm3Kjj76aI4++ug2Rp6dLouJiEjB6cxFRKSCuDuXXnrpNmXdunXjgQceKFFEmSm5iMhOLZlMtuoZklIzM2bPnt2h75lMJlvdRpfFRGSn1b17d1avXt2mX547i2QyyerVq+nevXur2unMRUR2WjU1NdTX1/Pee+8BYUGsrl27ljiq9ilGH7p3705NTU2r2ii5iMhOq2vXrvTv33/LfiUueJauXPpQsuRiZqcBVwK1wJHuviCWdwNuA4YATcAEd38qQ/tBhLVeugObgG+7+0tmtidwN/BZQv+muvudxe6PiIhsVcp7Lq8ApwDz08ovAHD3QwiLff0sLhyW7jrgR+4+CPiPuA9wEfCqux8GfCW271bw6EVEJKuSJRd3T7i7Zzg0EHgy1lkFrCWcxaRLAnvE7T0JC4s1l+8eV6jsAawhnNmIiEgHKcd7Li8DJ5nZvUBfYHD8/lJavYuBeWY2lZAkvxDLbwYeJiSb3YEz3L0p1xs2NjaSSLR9NeSGhoZ2tS+1So8f1IdyUOnxg/pQSEVNLmb2BLBPhkNT3D3bQO07CPdhFgBvAc8BmzPUGw9MdPeHzOx04HZgKPA1YBFwDHAg8LiZPePu67LFWV1d3a4bYOVyA62tKj1+UB/KQaXHD+pDa9XV1WU9VtTk4u6tWz4ttNkETGzeN7PngNcyVP0mMCFuPwBMj9vnA9e6exJ4w8yWAZ9j+zMfEREpkrJ7iNLMdjWz3eL2MGCTu2eaA3oF8OW4fQzwetz+G3BsbP9pwIC/FjVoERHZRimHIp8M3AT0Bv7bzBa5+9eAPoR7KU3AcuDclDbTgVvjsOULgGlm1gVoAJqn+rwK+I2ZLQaqgMvc/f2O6peIiJQwubj7LGBWhvI3CWcbmdqMS9l+lnCzP73OCuC4ggUqIiKtVnaXxUREpPIpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMFlfYjSzPbOo32Tu68tXDgiIrIjyPWE/or4VZWjTmfCio8iIiJb5EouCXc/PFdjM1tY4HhERGQHkOueyz/l0T6fOiIispPJeubi7g3N22b2ReAgd7/TzHoDPdx9WWqd1jCz64ERwAbgL8D57r42rnV/G2FZ4yZggrs/laH9IOBWoDthCeNvu/tLZvYpwmJjBxJmSh7j7q+0JUYREWm7FkeLmdkPgcuAybGoK3B3O9/3ceBgdz+UsBBY82tfAODuhwDDgJ+ZWaYYrwN+5O6DgP+I+wDfBxbF1x0NTGtnnCIi0gb5DEU+GTgJ+Bi2TGm/e3ve1N0fiytOArwA1MTtgcCTsc4qYC3hLCZdEtgjbu9JGHiQ3n4p0C8uGCYiIh0on+SyIS4ZnARoXiWygMYAc+P2y8BJZtbFzPoT1mvpm6HNxcD1ZvY2MJWtZz4vA6fEOI8E9mdr4hIRkQ6Sz2Jh95vZbcBeZnYBIRn8uqVGZvYEsE+GQ1PcfXasM4Vwz+SeeOwOoBZYALwFPAdszvAa44GJ7v6QmZ0O3A4MBa4lrE65CFgMLMzSfhuNjY0kEomWqmXV0NDQrvalVunxg/pQDio9flAfCqkqmUy2WCmuZX8c4ZmXee7+eHvf2MzOA74FHOvun2Sp8xwwzt1fTSv/O7CXuyfNrAr4u7vvkVanClgGHOru63LFkkgkkrW1tW3uSyKRoD3tS63S4wf1oRxUevygPrRWXV1d3eDBgzPdumj5zCVennqmOaGY2S5m1i8uR9wmZnY8cCnw5dTEYma7AlXu/nFMaJvSE0u0Avgy8BRwDPB6bL8X8Im7bwDGAfNbSiwiIlJ4+VwWewD4Qsr+5lh2RDve92agGnjczABecPd/BfoA88ysCVgOnNvcwMymA7e6+wLCqLJpZtaFMOT4wlitFrjLzJLAEmBsO2IUEZE2yie5dIlnAgC4+4b4PEqbufs/ZCl/E7Asx8albD9LuNmfXud5YEB7YhMRkfbLZ7TYe2Z2UvOOmY0E3i9eSCIiUunyOXP5V+AeM7uZcEP/bcIDiiIiIhm1mFzc/S/AUWbWI+5/VPSoRESkouVaz+Ucd7/bzL6XVg6Au99Q5NhERKRC5TpzaX4Sv11TvYiIyM4n16zIt5lZZ2Cdu/+8A2MSEZEKl3O0mLtvBs7qoFhERGQHkc9osf+JI8XuI86MDODufy5aVCIiUtHySS6D4vcfp5QlCdOuiIiIbCef5HKau+uhSRERyVuuocgjCFPgb4xzfZ3u7s91WGQiIlKxct3Qvxo42t33BUYB13RMSCIiUulyJZdNcalg3P1F9LyLiIjkKdc9lz5pT+dvs9/eJ/TN7CpgJNAErALOc/cVZvYpwuW4AwnT6Y9x91cytD8WuJ6QID+K7d8ws/1j+97AGuAcd69vT6wiItI6uc5cfk04W2n+St9vr+vd/VB3HwQ8CvxHLP8+sMjdDyVMkDktS/tbgLNj+/8CfhDLpwK/je1/jC7niYh0uFxP6P+omG+ctkLkboThzQADgWtjnaVm1s/MPu3uK9NeIgk0L228J2F1yub2zWdYfwR+V+DQRUSkBfkMRS4aM7uacHbyd+Crsfhl4BTgGTM7EtgfqAHSk8s4YI6ZrQfWAUeltZ8GnAzsbmY93X11tjgaGxtJJBJt7kdDQ0O72pdapccP6kM5qPT4QX0opKImFzN7Atgnw6Ep7j7b3acAU8xsMvAd4IeEs5ZpZrYIWAwsJCytnG4iMNzdXzSzScANhIRzCXCzmZ0HzCcsl5yp/RbV1dXU1ta2oYdBIpFoV/tSq/T4QX0oB5UeP6gPrVVXV5f1WIvJxcw6xznGWs3dh+ZZ9R5gDvDDeLns/PjeVcAy4K9pMfUGDouj2CBMTfP7+J4rCGcuxDVoRrn72rbELyIibZPPMsevm9n1ZjawkG9sZgel7I4ElsbyvcysWywfB8xPuz8D8AGwp5kNiPvDgERs38vMmvs1mTByTEREOlA+l8UOA84Epsdf2ncAMzL8wm+tay2sPNYEvEVYThmgFrjLzJLAEmBscwMzmwOMi0OWLwAeirMHfACMidW+AlwT288HLmpnnCIi0kr5LHP8IWEY8q/N7MuEYb8/N7MHgavc/Y22vLG7j8pS/jwwIMux4Snbs4BZGeo8CDzYlphERKQw8rrnApxAuA/SD/gZ4R7J0YT7JBkTgYiI7LzyuSz2OuF5kevTJq580My+VJywRESkkuWTXEa7+7OpBWb2z+7+P+7+3SLFJSIiFSyf0WI3Zii7qdCBiIjIjiPXei7/BHwB6J02geUeQOdiByYiIpUr12WxbkCPWCd1osp1wKnFDEpERCpbrokrnzazZ4FDiz2JpYiI7Fhy3nOJ077s20GxiIjIDiKf0WKLzOxh4AHg4+ZCd59ZtKhERKSi5ZNcugOrgWNSypKAkouIiGSUz/Qv53dEICIisuNo8TkXMxtgZn8ws1fi/qFm9oOW2omIyM4rn4cof02Yun4jgLv/L2GWZBERkYzyueeyq7u/FGbH32JToQIws38HpgK93f19M/sUYVr/A4EGYIy7v5Kh3bHA9YQE+RFwnru/YWb7x/a9gTXAOe5eX6h4RUSkZfmcubxvZgcSbuJjZqcC7xTizc2sL3Ac8LeU4u8Di9z9UGA0MC1L81uAs919EGEZgOZLdVOB38b2PwauKUSsIiKSv3ySy0XAbcDnzGw5cDFbF/Zqr58DlxITVzQQeBLA3ZcC/czs0xnaJglT0QDsCaxIb0+YzXlkgWIVEZE85TNa7K/AUDPbDegUFw9rNzMbCSx395fTLrm9DJwCPGNmRwL7AzXAyrSXGAfMMbP1hClpjkprPw04GdjdzHq6++pssTQ2NpJIJNrcl4aGhna1L7VKjx/Uh3JQ6fGD+lBI+SwW1hP4IfBFIBmnhPlxrl/WKW2fAPbJcGgK4fLXcRmOXQtMM7NFwGJgIbA5Q72JwHB3f9HMJgE3EBLOJcDNZnYeYZnj5Vnab1FdXU1tbW1L3ckqkUi0q32pVXr8oD6Ug0qPH9SH1qqrq8t6LJ8b+jMIv6SblyU+G7gPGNpSQ3fPWMfMDgH6A81nLTXAn83sSHd/l7DqJWZWBSwD/prWvjdwmLu/GIvuA34f33MF4cwFM+sBjHL3tXn0U0RECiSf5PIZd78qZf//mtkZ7XlTd18M9GneN7M3gSFxtNhewCfuvoFwJjLf3delvcQHwJ5mNsDdXwOGAYn4Wr2ANe7eRBhCfUd7YhURkdbLJ7k8ZmZnAvfH/VOBecULiVrgLjNLAkuAsc0HzGwOMM7dV5jZBcBDZtZESDZjYrWvANfE9vMJAxJERKQD5ZNcLiCMELs77ncCPjazbwFJd98jW8N8uXu/lO3ngQFZ6g1P2Z4FzMpQ50HgwfbGJCIibZfPaLHdW6ojIiKSKp8zF8zsJOBLcfcpd3+0eCGJiEily2fiymuBCcCr8WuCmempdxERySqfM5fhwKA4+gozu4vw7MnkYgYmIiKVK5/pXwD2StneswhxiIjIDiSfM5efAAvN7I9AFeHey+VFjUpERCpazuRiZp2AJsK8XUfE4sviU/QiIiIZ5Uwu7t5kZpe6+/3Awx0Uk4iIVLh8Los9YWaXEObv+ri50N3XFC0qERGpaPkkl+Z5xFKnUUkCBxQ+HBER2RHk84R+/44IREREdhxZk4uZ/R/gV4S17BcT1rIv/Qo0IiJS9nI95/JLwsJbPQkLcf2iIwISEZHKl+uyWCd3fzxuP2BmBXsi38yuJMy2/F4s+r67zzGzbsBtwBDCEOgJ7v5UhvaHAbcCPYA3gbOb13yJcY4lrD75XXcv5vIAIiKSQa7kspeZnZJt391ntvO9f+7uU9PKLoivfYiZ9QHmmtkRzVPPpJgOXOLuT5vZGGAScIWZDQTOBD4P7EsY6TbA3XMucywiIoWV67LY08CIlK/U/ROLFM9A4EkAd18FrCWcxaQbQFgIDOBxti7BPBKY4e6N7r4MeAM4skixiohIFlnPXNz9/CK/93fMbDSwAPh3d/8AeBk4yczuBfoCg+P3l9LaLiEkkt8Bp8U6APsBL6TUq49lOTU2NpJItH2sQkNDQ7val1qlxw/qQzmo9PhBfSikXKPFTmxp3ZZcdczsCWCfDIemALcAVxGel7kK+BlhmeI7CMscLwDeAp4j3DtJNwa40cyuIMwcsCFXnC2prq6mtra2ze0TiUS72pdapccP6kM5qPT4QX1orbq6uqzHct1zud7MlhMmq8zmJ0DG5OLuQ/MJzsx+3fwa7r4JmJhy7DngtQyvvRQ4LtYZAJwQDy1n61kMQE0sExGRDpQruawkDEHO5fW2vKmZfcbd34m7JwOvxPJdgSp3/9jMhgGb3P3VDO37uPuqOLHmDwgjxyCcxfyXmd1AuKF/ENtfUhMRkSLLdc/lK0V83+vMbBDhstibwLdieR9gnpk1Ec44zm1uYGbTgVvdfQFwlpk1T0czE7gzxrzEzO4nrJi5CbhII8VERDpePnOLFZy7n5ul/E3Ashwbl7I9DZiWpd7VwNXtj1JERNoq35UoRURE8pYzuZhZJzP7QkcFIyIiO4acySU+Gf/LDopFRER2EPlcFvuDmY0ys1xDkkVERLbI54b+t4DvAZvNbD3huZeku+9R1MhERKRi5bNY2O4dEYiIiOw48hqKbGYnAV+Ku0+1NC2MiIjs3Fq852Jm1wITCA8mvgpMMLNrih2YiIhUrnzOXIYDg5rXVDGzu4CFQMEWDxMRkR1Lvg9R7pWyvWcR4hARkR1IPmcuPwEWmtkfCSPFvgRcXtSoRESkouVMLnHW4SbgKOCIWHyZu79b7MBERKRy5Uwu7t5kZpe6+/2E6ewLxsz+DbiIsBjYf7v7pWbWDbiNsLRxEzDB3Z/K0PYwwjT7PQizKp/t7uviscnA2Pi633X3eYWMW0REWpbPPZcnzOwSM+trZns3f7XnTc3sq4Rlig9z988DU+OhCwDc/RBgGPCzePaUbjpweaw3C5gUX3cgcCbweeB44D/NrHN7YhURkdbLJ7mcQTjDmA/Uxa8F7Xzf8cC17t4I4O6rYvlA4MmUsrWEs5h0A2I8AI8Do+L2SGCGuze6+zLgDeDIdsYqIiKtlM89l8vd/b4Cv+8A4GgzuxpoAC5x9z8BLwMnmdm9hOWKB8fv6atJLiEkkt8Bp7F1aeP9gBdS6tXHMhER6UD53HOZBLQ6uZjZE8A+GQ5Nie+7N1sHCtxvZgcAdwC1hDOjt4DnCPdO0o0BbjSzKwj3gja0Nr5UjY2NJBKJNrdvaGhoV/tSq/T4QX0oB5UeP6gPhZTPUOQnzOwSQoL5uLnQ3dfkauTuQ7MdM7PxwEx3TwIvxWWNe7n7e8DElHrPAa9leO2lwHGxzgDghHhoOVvPYgBqYllO1dXV1NbWtlQtq0Qi0a72pVbp8YP6UA4qPX5QH1qrrq4u67FS3XP5HfBV2JIcugHvm9muZrZbLB8GbHL3V9Mbm1mf+L0T8APCyDEIZzFnmlm1mfUHDmL7S2oiIlJk+cyK3L8I73sHcIeZvUK4pPVNd0/GpDEvnsksB85tbmBm04Fb3X0BcJaZXRQPzQTujLEuMbP7CXOgbQIucvdMl9VERKSIsiaX+HzLdXH7NHd/IOXYT9z9+219U3ffAJyTofxNwLK0GZeyPQ2YlqXe1cDVbY1NRETaL9dlsTNTttMnqTy+CLGIiMgOIldyqcqynWlfRERki1zJJZllO9O+iIjIFrlu6B9mZusIZym7xG3ifveiRyYiIhUra3Jxd83JJSIibZLvYmEiIiJ5U3IREZGCU3IREZGCU3IREZGCU3IREZGCU3IREZGCU3IREZGCU3IREZGCy2exsIIzs/vYOvvxXsBadx9kZt2A24AhQBMwwd2fytD+MMIaLj2AN4Gz3X2dmfUEHiSsbvkbd/9OkbsiIiIZlOTMxd3PcPdB7j4IeIiwJgvABfH4IcAw4GdxQbB004HLY71ZwKRY3gBcAVxSxPBFRKQFJb0sZmZVwOnAvbFoIPAkgLuvAtYSzmLSDSCsjAnwODAqtvnY3Z8lJBkRESmRklwWS3E0sNLdX4/7LwMnmdm9QF9gcPyevlTxEmAkYbnk02KdNmtsbCSRSLS5fUNDQ7val1qlxw/qQzmo9PhBfSikoiUXM3sC2CfDoSnuPjtun8XWsxYIyx/XAguAt4DngEzLFI8BbjSzK4CHCUslt1l1dTW1tbVtbp9IJNrVvtQqPX5QH8pBpccP6kNr1dXVZT1WtOTi7kNzHTezLsAphLOT5jabgIkpdZ4DXsvw2kuB42KdAcAJhYlaREQKoZT3XIYCS929vrnAzHY1s93i9jBgk7u/mt7QzPrE752AHxBGjomISJkoZXI5k20viQH0Af5sZgngMuDc5gNmNt3Mmm/un2VmrwFLgRXAnSn13gRuAM4zs3ozG1i8LoiISCYlu6Hv7udlKHuTrc+/pB8bl7I9DZiWpV6/ggQoIiJtpif0RUSk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4JRcRESk4EoyK7KZDSKswdId2AR8291fMrNPEVajPBBoAMa4+ysZ2h8DTAW6AXXAWHffZGafI0y//4+EFS+ndkR/RERkW6U6c7kO+JG7DwL+I+4DfB9Y5O6HAqPJMK1+XCDsLuBMdz+YsBzyN+PhNcB3CYlHRERKpFTJJQnsEbf3JCz4BTAQeBK2LGXcz8w+nda2J7DB3ZuXP34cGBXbrHL3PwEbixi7iIi0oFSLhV0MzDOzqYQE94VY/jJwCvCMmR0J7A/UACtT2r4PdDGzIe6+ADgV6NueYBobG0kkEm1u39DQ0K72pVbp8YP6UA4qPX5QHwqpaMnFzJ4A9slwaApwLDDR3R8ys9OB24GhwLXANDNbBCwGFgKbUxu7e9LMzgR+bmbVwGPpdVqrurqa2traNrdPJBLtal9qlR4/qA/loNLjB/Whterq6rIeK1pycfeh2Y6Z2W+BCXH3AWB6bLMOOD/WqQKWAX/N8NrPA0fHescBAwoZu4iItE+p7rmsAL4ct48BXgcws73MrFssHwfMjwlnG2bWJ36vBi4jjDwTEZEyUap7LhcQLn91IQw5vjCW1wJ3mVkSWAKMbW5gZnOAce6+AphkZicSkuMt7v5krLMPsIAwWKDJzC4GBmZKUCIiUjwlSS7u/iwwOEP582S5xOXuw1O2JwGTMtR5lzAAQERESkhP6IuISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMEpuYiISMGVZFZkMzuMsAZLD+BN4Gx3XxfXcrkNGAI0ARPc/alWtO8JPAgcAfzG3b9T/N6IiEi6Up25TAcud/dDgFlsnT7/AoBYPgz4mZllijFb+wbgCuCSIsYuIiItKFVyGQDMj9uPA6Pi9kDgSQB3XwWsJZzF5NXe3T+Oa8U0FCVqERHJS6lWolwCjAR+B5wG9I3lLwMnmdm9sWxw/P5Snu3b5JNPPnm/rq7urfa8Rl1dXXual1ylxw/qQzmo9PhBfWil/bMdKFpyMbMngH0yHJoCjAFuNLMrgIeBDfHYHYSljhcAbwHPAZszvEa29m0yePDg3u1pLyIi2ypacnH3oS1UOQ7AzAYAJ8Q2m4CJzRXM7DngtQyvvTRTexERKQ8luediZn3i907ADwgjvzCzXc1st7g9DNjk7q/m215ERMpDqW7on2VmrwFLgRXAnbG8D/BnM0sAlwHnNjcws+lmNqSF9pjZm8ANwHlmVm9mA4vdGRER2VZVMpksdQwiIrKD0RP6IiJScEouIiJScKV6zmWHYGbHA9OAzsB0d7+2xCG1yMz6Ar8FPg0kgV+5+zQz2xu4D+hHmFLndHf/oFRxtsTMOhOGrC939xPNrD8wA+gJ1AHnunu7hqgXk5ntRZhp4mDC5zAGcCrrM5gIjCPEvxg4H/gMZfw5mNkdwInAKnc/OJZl/H/fzKoI/76HA58A57n7n0sRd7Ms8V8PjCA8kvEX4Hx3XxuPTQbGEh7p+K67z+uoWHXm0kbxl9svgX8hzCxwVoUMHtgE/Lu7DwSOAi6KcV8O/MHdDwL+EPfL2QQgkbL/U+Dn7v4PwAeEf1DlbBrwe3f/HHAYoS8V8xmY2X7Ad4Eh8ZdcZ+BMyv9z+A1wfFpZtp/7vwAHxa8LgVs6KMZcfsP28T8OHOzuhxIe3ZgMEP9dnwl8Prb5z/h7q0MoubTdkcAb7v7X+JfZDMKsAWXN3d9p/uvL3T8k/FLbjxD7XbHaXcDXSxJgHsyshvBs0/S4XwUcQ5i0FMo//j2BLwG3A7j7hviXZsV8BlEXYBcz6wLsCrxDmX8O7j4fWJNWnO3nPhL4rbsn3f0FYC8z+0yHBJpFpvjd/bH4jCDAC0BN3B4JzHD3RndfBrxB+L3VIZRc2m4/4O2U/fpYVjHMrB9wOPAi8Gl3fyceepdw2axc/QK4lDBzNoRLMGtT/oGV+2fRH3gPuNPMFsZh9rtRQZ+Buy8HpgJ/IySVvxMug1XS59As28+9Ev+NjwHmxu2Sxq/kspMysx7AQ8DF7r4u9Zi7JwnX0cuOmTVfb67kCaC6AP8I3OLuhwMfk3YJrJw/AwAz+xThL+P+wL7Abmx/uabilPvPPRczm0K47H1PqWMBJZf2WM62E2bWxLKyZ2ZdCYnlHnefGYtXNp/yx++rShVfC/6ZMLnpm4RLkccQ7l/sFS/PQPl/FvVAvbu/GPcfJCSbSvkMAIYCy9z9PXffCMwkfDaV9Dk0y/Zzr5h/42Z2HuFG/9kxQUKJ41dyabs/AQeZWf+4yNmZhEk0y1q8P3E7kHD3G1IOPQx8M25/E5jd0bHlw90nu3uNu/cj/MyfdPezgT8Cp8ZqZRs/gLu/C7xtZhaLjgVepUI+g+hvwFFxyqYqtvahYj6HFNl+7g8Do82sysyOAv6ecvmsbMRRq5cCJ7n7JymHHgbONLPqOJryILafYb5o9IR+O5jZcML1/87AHe5+dWkjapmZfRF4hjB0tPmexfcJ913uBz5LmJH6dHdPv/FZVszsK8AlcSjyAYQzmb2BhcA57t5YyvhyMbNBhAEJ3YC/EobxdqKCPgMz+xFwBuFSzELCsOT9KOPPIS7n8RWgF7AS+CFh6Y7tfu4xad5MuNz3CWGI74IShL1FlvgnA9XA6ljtBXf/11i/eRb6TYRL4HPTX7NYlFxERKTgdFlMREQKTslFREQKTslFREQKTslFREQKTslFREQKTrMii3QQM9tMGALebEahZtKOU/k82jxTrkipKbmIdJz17j6o1EGIdAQlF5ESi1PZ3E+Y4n098A13fyOejdxBeGDuPcJDfH8zs08DtwIHxJcYD6wAOpvZr4EvEKb5GOnu6zuyLyLNdM9FpOPsYmaLUr7OSDn2d3c/hPBE+C9i2U3AXXGdjnuAG2P5jcDT7n4YYU6yJbH8IOCX7v55YC0wqqi9EclBZy4iHSfXZbF7U77/PG7/E3BK3P5/wHVx+xhgNIC7bwb+HmcpXubui2KdOsLKiiIloTMXkfKQzLLdGqlzeG1GfzxKCSm5iJSHM1K+Px+3nyPM/AxwNmHCUQhL8Y6HsNx2XNlSpKzoLxuRjrOLmS1K2f+9uzcvEvYpM/tfwtnHWbHs3wirVU4i3tCP5ROAX5nZWMIZynjCapAiZUOzIouUWBwtNsTd3y91LCKFostiIiJScDpzERGRgtOZi4iIFJySi4iIFJySi4iIFJySi4iIFJySi4iIFNz/B+8NYoMqe1XiAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "    loss_fig, loss_ax = plt.subplots()\n",
    "    loss_ax.plot(history.history['loss'], label='loss')\n",
    "    loss_ax.plot(history.history['val_loss'], label='val_loss')\n",
    "    #plt.ylim([0, 10])\n",
    "    min_y = min(min(history.history['val_loss']),min(history.history['loss'])) - 100\n",
    "    #max_y = min(max(history.history['val_loss']),max(history.history['loss'])) + 500\n",
    "    #max_y = min(sorted(history.history['val_loss'])[-3],sorted(history.history['loss'])[-3]) + 100\n",
    "    max_y = min(sorted(history.history['val_loss'])[-1],sorted(history.history['val_loss'])[-1])\n",
    "    \n",
    "    print(max_y - min_y)\n",
    "    ticks = (max_y - min_y)/10\n",
    "    print(ticks)\n",
    "    \n",
    "    plt.ylim([min_y, max_y])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [Property Price]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yticks(np.arange(min_y, max_y, ticks))  # JHJH\n",
    "    return loss_fig, loss_ax\n",
    "\n",
    "loss_fig, loss_ax = plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:36.689854Z",
     "iopub.status.busy": "2022-12-07T14:51:36.689607Z",
     "iopub.status.idle": "2022-12-07T14:51:37.078758Z",
     "shell.execute_reply": "2022-12-07T14:51:37.077837Z",
     "shell.execute_reply.started": "2022-12-07T14:51:36.689832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = trainable_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.081734Z",
     "iopub.status.busy": "2022-12-07T14:51:37.081387Z",
     "iopub.status.idle": "2022-12-07T14:51:37.090508Z",
     "shell.execute_reply": "2022-12-07T14:51:37.089530Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.081700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Neural Network m15 mega + dropout----------\n",
      "R square Accuracy -15.994804906238336\n",
      "Mean Absolute Error Accuracy 212534587876.0771\n",
      "Mean Squared Error Accuracy 4.7995052450841444e+22\n",
      "Root Mean Squared Error 219077731526.60095\n"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('-' * 10 + ALGORITHM + '-' * 10)\n",
    "print('R square Accuracy', R2)\n",
    "print('Mean Absolute Error Accuracy', MAE)\n",
    "print('Mean Squared Error Accuracy', MSE)\n",
    "print('Root Mean Squared Error', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.091753Z",
     "iopub.status.busy": "2022-12-07T14:51:37.091525Z",
     "iopub.status.idle": "2022-12-07T14:51:37.095934Z",
     "shell.execute_reply": "2022-12-07T14:51:37.095157Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.091734Z"
    }
   },
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    print(y_test_index.reshape((-1, 1)).shape);\n",
    "    print(y_pred.reshape((-1, 1)).shape);\n",
    "    print(y_test.shape);\n",
    "    print(y_test_index.shape);\n",
    "    print(y_pred.shape);\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.096842Z",
     "iopub.status.busy": "2022-12-07T14:51:37.096647Z",
     "iopub.status.idle": "2022-12-07T14:51:37.131204Z",
     "shell.execute_reply": "2022-12-07T14:51:37.130413Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.096824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                 actual  predicted    difference  diff 1 %      diff 2 %  \\\n85476739   217500100000          0  2.175001e+11     100.0  7.069250e+13   \n85540923   275000100000          0  2.750001e+11     100.0  6.325818e+13   \n117403586  152500100000          0  1.525001e+11     100.0  6.031862e+13   \n103999217  241875100000          0  2.418751e+11     100.0  6.029209e+13   \n118948271  290000100000          0  2.900001e+11     100.0  5.972515e+13   \n...                 ...        ...           ...       ...           ...   \n123509066  116000100000          0  1.160001e+11     100.0  1.388397e+13   \n83639617    95000100000          0  9.500010e+10     100.0  1.249071e+13   \n109722224   54000100000          0  5.400010e+10     100.0  1.225851e+13   \n85667184    61000100000          0  6.100010e+10     100.0  1.182165e+13   \n122265818   80000100000          0  8.000010e+10     100.0  8.845586e+12   \n\n            Price  bedrooms  bathrooms  nearestStation  location.latitude  \\\n85476739   435000         1          1        0.865284          51.501799   \n85540923   550000         2          1        0.728866          51.474917   \n117403586  305000         1          1        0.340743          51.382480   \n103999217  483750         2          1        0.655006          51.508423   \n118948271  580000         1          1        0.231182          51.506587   \n...           ...       ...        ...             ...                ...   \n123509066  232000         1          1        0.153684          51.512190   \n83639617   190000         2          2        0.162816          51.491264   \n109722224  108000         1          1        0.302667          51.406803   \n85667184   122000         1          1        0.838896          51.540462   \n122265818  160000         3          1        0.166024          51.510420   \n\n           location.longitude  latitude_deviation  longitude_deviation  \\\n85476739             0.053130            0.002079             0.157550   \n85540923             0.026607            0.024803             0.131027   \n117403586           -0.156277            0.117240             0.051857   \n103999217            0.073162            0.008703             0.177582   \n118948271            0.075455            0.006867             0.179875   \n...                       ...                 ...                  ...   \n123509066           -0.134790            0.012470             0.030370   \n83639617             0.064660            0.008456             0.169080   \n109722224           -0.261328            0.092917             0.156908   \n85667184            -0.280301            0.040742             0.175881   \n122265818           -0.037730            0.010700             0.066690   \n\n          tenure.tenureType  \n85476739          LEASEHOLD  \n85540923          LEASEHOLD  \n117403586         LEASEHOLD  \n103999217         LEASEHOLD  \n118948271         LEASEHOLD  \n...                     ...  \n123509066         LEASEHOLD  \n83639617          LEASEHOLD  \n109722224         LEASEHOLD  \n85667184          LEASEHOLD  \n122265818          FREEHOLD  \n\n[4413 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>difference</th>\n      <th>diff 1 %</th>\n      <th>diff 2 %</th>\n      <th>Price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>nearestStation</th>\n      <th>location.latitude</th>\n      <th>location.longitude</th>\n      <th>latitude_deviation</th>\n      <th>longitude_deviation</th>\n      <th>tenure.tenureType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>85476739</th>\n      <td>217500100000</td>\n      <td>0</td>\n      <td>2.175001e+11</td>\n      <td>100.0</td>\n      <td>7.069250e+13</td>\n      <td>435000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.865284</td>\n      <td>51.501799</td>\n      <td>0.053130</td>\n      <td>0.002079</td>\n      <td>0.157550</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>85540923</th>\n      <td>275000100000</td>\n      <td>0</td>\n      <td>2.750001e+11</td>\n      <td>100.0</td>\n      <td>6.325818e+13</td>\n      <td>550000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.728866</td>\n      <td>51.474917</td>\n      <td>0.026607</td>\n      <td>0.024803</td>\n      <td>0.131027</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>117403586</th>\n      <td>152500100000</td>\n      <td>0</td>\n      <td>1.525001e+11</td>\n      <td>100.0</td>\n      <td>6.031862e+13</td>\n      <td>305000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.340743</td>\n      <td>51.382480</td>\n      <td>-0.156277</td>\n      <td>0.117240</td>\n      <td>0.051857</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>103999217</th>\n      <td>241875100000</td>\n      <td>0</td>\n      <td>2.418751e+11</td>\n      <td>100.0</td>\n      <td>6.029209e+13</td>\n      <td>483750</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.655006</td>\n      <td>51.508423</td>\n      <td>0.073162</td>\n      <td>0.008703</td>\n      <td>0.177582</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>118948271</th>\n      <td>290000100000</td>\n      <td>0</td>\n      <td>2.900001e+11</td>\n      <td>100.0</td>\n      <td>5.972515e+13</td>\n      <td>580000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.231182</td>\n      <td>51.506587</td>\n      <td>0.075455</td>\n      <td>0.006867</td>\n      <td>0.179875</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>123509066</th>\n      <td>116000100000</td>\n      <td>0</td>\n      <td>1.160001e+11</td>\n      <td>100.0</td>\n      <td>1.388397e+13</td>\n      <td>232000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.153684</td>\n      <td>51.512190</td>\n      <td>-0.134790</td>\n      <td>0.012470</td>\n      <td>0.030370</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>83639617</th>\n      <td>95000100000</td>\n      <td>0</td>\n      <td>9.500010e+10</td>\n      <td>100.0</td>\n      <td>1.249071e+13</td>\n      <td>190000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.162816</td>\n      <td>51.491264</td>\n      <td>0.064660</td>\n      <td>0.008456</td>\n      <td>0.169080</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>109722224</th>\n      <td>54000100000</td>\n      <td>0</td>\n      <td>5.400010e+10</td>\n      <td>100.0</td>\n      <td>1.225851e+13</td>\n      <td>108000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.302667</td>\n      <td>51.406803</td>\n      <td>-0.261328</td>\n      <td>0.092917</td>\n      <td>0.156908</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>85667184</th>\n      <td>61000100000</td>\n      <td>0</td>\n      <td>6.100010e+10</td>\n      <td>100.0</td>\n      <td>1.182165e+13</td>\n      <td>122000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.838896</td>\n      <td>51.540462</td>\n      <td>-0.280301</td>\n      <td>0.040742</td>\n      <td>0.175881</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>122265818</th>\n      <td>80000100000</td>\n      <td>0</td>\n      <td>8.000010e+10</td>\n      <td>100.0</td>\n      <td>8.845586e+12</td>\n      <td>160000</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.166024</td>\n      <td>51.510420</td>\n      <td>-0.037730</td>\n      <td>0.010700</td>\n      <td>0.066690</td>\n      <td>FREEHOLD</td>\n    </tr>\n  </tbody>\n</table>\n<p>4413 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = np.hstack((y_test_index, y_test, y_pred))\n",
    "compare_df = DataFrame(compare, columns=['reference', 'actual', 'predicted'])\n",
    "compare_df['difference'] = abs(compare_df['actual'] - compare_df['predicted'])\n",
    "compare_df['diff 1 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['actual'] * 100)\n",
    "compare_df['diff 2 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['predicted']) * 100\n",
    "compare_df['reference'] = compare_df['reference'].astype(int)\n",
    "compare_df.set_index('reference', inplace=True)\n",
    "\n",
    "combined = compare_df.merge(df[columns], how='inner', left_index=True, right_index=True).sort_values(['diff 1 %'],\n",
    "                                                                                                     ascending=False)\n",
    "#pd.options.display.float_format = '{:.4f}'.format\n",
    "combined[['predicted', 'actual', 'Price', 'bedrooms', 'bathrooms']] = combined[\n",
    "    ['predicted', 'actual', 'Price', 'bedrooms', 'bathrooms']].astype(int)\n",
    "combined['bedrooms'] = combined['bedrooms'].astype(int)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.132382Z",
     "iopub.status.busy": "2022-12-07T14:51:37.132155Z",
     "iopub.status.idle": "2022-12-07T14:51:37.267074Z",
     "shell.execute_reply": "2022-12-07T14:51:37.266314Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.132361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_fig, best_model_ax = plt.subplots()\n",
    "best_model_ax.scatter(y_test, y_pred, edgecolors=(0, 0, 1))\n",
    "best_model_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "best_model_ax.set_ylabel('Predicted')\n",
    "best_model_ax.set_xlabel('Actual')\n",
    "#ax.title.set_text(f'CV Chosen best option ({calculated_best_pipe[1]})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.268497Z",
     "iopub.status.busy": "2022-12-07T14:51:37.268238Z",
     "iopub.status.idle": "2022-12-07T14:51:37.638274Z",
     "shell.execute_reply": "2022-12-07T14:51:37.637596Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.268474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural network m15 mega + dropout (v09)\n",
      "mae +epochs=400 +learn=0.0003\n"
     ]
    }
   ],
   "source": [
    "cv_best_model_fit_time = estimated_time\n",
    "\n",
    "DD2 = \"(\" + \",\".join(DATA_DETAIL) + \")\" if len(DATA_DETAIL) >= 1 else \"\"\n",
    "key = f'{ALGORITHM} (v{VERSION})'.lower()\n",
    "\n",
    "method = f\"{ALGORITHM_DETAIL}{DD2}\"\n",
    "\n",
    "new_results = {\n",
    "    #'_score': score,\n",
    "    '_score': R2,\n",
    "    'R square Accuracy': R2,\n",
    "    'Mean Absolute Error Accuracy': MAE * price_divisor,\n",
    "    'Mean Squared Error Accuracy': MSE * price_divisor,\n",
    "    'Root Mean Squared Error': RMSE * price_divisor,\n",
    "    '_train time': cv_best_model_fit_time,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'date': str(datetime.now()),\n",
    "    #'_params': crossval_runner.best_params_ if not_catboost else cat_params,\n",
    "    #'_params': 'not available', # REPLACED - can't have different models all saying params not available\n",
    "    '_params': ALGORITHM_DETAIL,\n",
    "    '_method': more_detail, #ALGORITHM_DETAIL,\n",
    "    'run_env': run_env\n",
    "}\n",
    "\n",
    "if run_env not in ['colab']:\n",
    "    old_results_json = get_results()\n",
    "    try:\n",
    "        old_best_score = old_results_json[key]['best score']\n",
    "    except:\n",
    "        print(f\"haven't scored this model yet: {ALGORITHM}\")\n",
    "        old_best_score = -999\n",
    "    this_model_is_best = update_results(old_results_json, new_results, key)\n",
    "\n",
    "print(key)\n",
    "print(ALGORITHM_DETAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.639406Z",
     "iopub.status.busy": "2022-12-07T14:51:37.639158Z",
     "iopub.status.idle": "2022-12-07T14:51:37.644860Z",
     "shell.execute_reply": "2022-12-07T14:51:37.644222Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.639382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not updated saved model, the previous run was better\n",
      "-15.994804906238336 is worse than or equal to 0.6222346763480049\n"
     ]
    }
   ],
   "source": [
    "if this_model_is_best:\n",
    "    with open(f'../../../models/optimised_model_{ALGORITHM}_v{VERSION}{DD2}.pkl', 'wb') as f:\n",
    "        pickle.dump(trainable_model, f)\n",
    "        new_model_decision = f\"pickled new version of model\\n{old_results_json[key]['_score']} is new best score (it's better than {old_best_score})\"\n",
    "        #print(results_json[key]['_score'], 'is an improvement on', results_json[key]['second best score'])\n",
    "else:\n",
    "    new_model_decision = f\"not updated saved model, the previous run was better\\n{old_results_json[key]['_score']} is worse than or equal to {old_best_score}\"\n",
    "\n",
    "print(new_model_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Write the final report for this algorithm and dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:26.522741Z",
     "iopub.status.busy": "2022-12-07T14:52:26.522245Z",
     "iopub.status.idle": "2022-12-07T14:52:27.862657Z",
     "shell.execute_reply": "2022-12-07T14:52:27.861880Z",
     "shell.execute_reply.started": "2022-12-07T14:52:26.522708Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def include_in_html_report(type, section_header=None, section_figure=None, section_content=None, section_content_list=None):\n",
    "\n",
    "    # writePath_html = r'model_results/%s (html).html' % key\n",
    "    # writePath_md = r'model_results/%s (md).md' % key\n",
    "    results_root = '../../F_evaluate_model'\n",
    "    writePath_html = f'{results_root}/html/{key}.html'.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "    writePath_md = f'{results_root}/markdown/{key}.md'\n",
    "\n",
    "#isinstance(ini_list2, list)\n",
    "    if not section_content_list:\n",
    "        section_content_list = [section_content]\n",
    "\n",
    "    if type == 'header':\n",
    "        w = 'w' if section_figure <= 1 else 'a'\n",
    "        with open(writePath_html, w) as f1:\n",
    "            headers = f'<h{section_figure}>{section_content}</h{section_figure}>'\n",
    "            f1.write(headers)\n",
    "        with open(writePath_md, w) as f2:\n",
    "            headers = f'{\"#\" * int(section_figure)} {section_content }\\n'\n",
    "            f2.write(headers)\n",
    "    else:\n",
    "        if section_header:\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                f1.write(f'<h3>{section_header}</h3>')\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                f2.write(f'### {section_header}\\n')\n",
    "\n",
    "        if type=='dataframe':\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                dfAsString = section_content.to_html()\n",
    "                f1.write(dfAsString)\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                dfAsString = section_content.to_markdown()\n",
    "                f2.write(dfAsString + '\\n\\n')\n",
    "        elif type=='graph':\n",
    "            filename = key + \"_\" + section_content\n",
    "            #section_figure.savefig(f'model_results/artifacts/{filename.replace(\" \", \"_\")}')\n",
    "            section_figure.savefig(f'{results_root}/artifacts/{filename.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")}')\n",
    "\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                dfAsString = f'<img src=\"../artifacts/{filename.replace(\" \",\"_\").replace(\"(\", \"_\").replace(\")\", \"_\")}\"/>'\n",
    "                f1.write(dfAsString)\n",
    "\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                #dfAsString = f'(./model_results/artifacts/{filename}) \\n'\n",
    "                #dfAsString = f'![detail](./artifacts/{filename.replace(\" \",\"_\")})'\n",
    "                dfAsString = f'![detail](../artifacts/{filename.replace(\" \",\"_\").replace(\"(\", \"_\").replace(\")\", \"_\")})'\n",
    "                f2.write(dfAsString)\n",
    "                f2.write('\\n\\n')\n",
    "        elif type=='json':\n",
    "\n",
    "            # html_content_parsed = [[cell.text for cell in row(\"td\")]\n",
    "            #              for row in BeautifulSoup(content,features=\"html.parser\")(\"tr\")]\n",
    "            #\n",
    "            # html_content_dictionary = {element[0]:element[1:] for element in html_content_parsed}\n",
    "\n",
    "            #xxxprint(json.dumps(html_content_dictionary, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                #f.write(json.dumps(html_content_dictionary, indent=4))\n",
    "                soup = BeautifulSoup(section_content, \"html.parser\")\n",
    "                f1.write(str(soup.prettify()))\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                #f.write(json.dumps(html_content_dictionary, indent=4))\n",
    "                soup = BeautifulSoup(section_content, \"html.parser\")\n",
    "                #f2.write(str(soup.prettify()))\n",
    "\n",
    "\n",
    "                # html_content_dictionary = {element[0]:element[1:] for element in html_content_parsed}\n",
    "                # f2.write(json.dumps(html_content_dictionary, indent=4))\n",
    "\n",
    "                import ast\n",
    "                loads = ast.literal_eval(section_content)\n",
    "                #df = pd.DataFrame.from_dict(loads)\n",
    "                #df.drop(['dont'], axis=1, inplace=True)\n",
    "                #print(df.to_markdown(index=False,tablefmt='fancy_grid'))\n",
    "                for each in loads:\n",
    "                    f2.write(each + \" = \" + str(loads[each]) + \"\\n\\n\")\n",
    "\n",
    "        elif type=='dict':\n",
    "\n",
    "            for section_content in section_content_list:\n",
    "                if isinstance(section_content, str):\n",
    "                    import ast\n",
    "                    section_content = ast.literal_eval(section_content)\n",
    "\n",
    "                with open(writePath_html, 'a') as f1:\n",
    "                    soup = BeautifulSoup(str(section_content), \"html.parser\")\n",
    "                    f1.write(str(soup.prettify()))\n",
    "                with open(writePath_md, 'a') as f2:\n",
    "                    for each in section_content:\n",
    "                        f2.write(each + \" = \" + str(section_content[each]) + \"\\n\\n\")\n",
    "\n",
    "        elif type=='text':\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                for each_line in section_content_list:\n",
    "                    f1.write(each_line + '<br>')\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                for each_line in section_content_list:\n",
    "                    f2.write(each_line + '\\n\\n')\n",
    "\n",
    "        with open(writePath_html, 'a') as f1:\n",
    "            f1.write('<hr>')\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Results from {ALGORITHM}\", section_figure=1)\n",
    "\n",
    "end_timestamp = datetime.now()\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=f\"Dataset Version: {VERSION}\", section_content_list=[\n",
    "    f\"Date run: {datetime.now()}\"\n",
    "    \"\",\n",
    "    f\"Start time: {start_timestamp}\",\n",
    "    f\"End time: {end_timestamp}\",\n",
    "])\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Results\", section_figure=2)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Summary\", section_content=new_model_decision)\n",
    "\n",
    "\n",
    "include_in_html_report(type='graph', section_header=\"Best Model: Comparing model predictions to actual property values\", section_figure=best_model_fig, section_content='best_ann_model.png')\n",
    "\n",
    "#include_in_html_report(type=\"dataframe\",text_single=\"Tuned Models ranked by performance\", content=cv_results_df_sorted)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Model Specific Notes\", section_content_list=[\"can't display hyperparameter comparison for neural network\",\"can't display model performance graphs for neural network\",\"can't display model performance graphs for neural network\"])\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Neural Network Loss - Head\", section_content=hist.head())\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=None, section_content='')\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Neural Network Loss - Tail\", section_content=hist.tail())\n",
    "\n",
    "\n",
    "include_in_html_report(type='graph', section_header=None, section_figure=loss_fig, section_content='end_loss.png')\n",
    "\n",
    "import io\n",
    "def get_model_summary(model):\n",
    "    stream = io.StringIO()\n",
    "    model.summary(line_length=160, print_fn=lambda x: stream.write('>' + x.replace('-','').replace('=','') + '\\n'))\n",
    "    summary_string = stream.getvalue()\n",
    "    stream.close()\n",
    "    return summary_string\n",
    "\n",
    "short_model_summary = get_model_summary(trainable_model)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Model Structure\", section_content=short_model_summary)\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Comparison with other models\", section_figure=2)\n",
    "\n",
    "\n",
    "dff = pd.read_json('../../../results/results.json')\n",
    "\n",
    "version = VERSION\n",
    "\n",
    "\n",
    "all_models_df = dff[dff.columns].T.sort_values(\"best score\", ascending=False)\n",
    "version_models_df = dff[[c for c in dff.columns if version in c]].T.sort_values(\"best score\", ascending=False)\n",
    "\n",
    "version_models_summary = version_models_df[['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy', 'Root Mean Squared Error', 'best run date', 'best method']]\n",
    "all_models_summary = all_models_df[['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy', 'Root Mean Squared Error', 'best run date', 'best method']]\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=f\"Comparison with version {VERSION} performances\", section_content=version_models_summary)\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Comparison with all model performances\", section_content=all_models_summary)\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Appendix\", section_figure=2)\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Data Sample\", section_content=df.head(5))\n",
    "\n",
    "if False:\n",
    "    include_in_html_report(type=\"json\", section_header=\"Hyperparameter options for Randomized Grid Search\", section_content=f\"{param_options if not using_catboost else options_block}\")\n",
    "else:\n",
    "\n",
    "    include_in_html_report(type=\"text\", section_header=\"FIX THIS!!\", section_content=\"FIX THIS!\")\n",
    "\n",
    "include_in_html_report(type=\"dict\", section_header=\"Environment Variables\", section_content=env_vars)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Useful info\",\n",
    "                       section_content_list=[f\"Tensorflow version: {tf.__version__}\"\n",
    "                                        ])\n",
    "\n",
    "\n",
    "def print_and_report(text_single, title):\n",
    "    include_in_html_report(\"text\", section_content=title)\n",
    "    for each in text_single:\n",
    "        print(each)\n",
    "        include_in_html_report(\"text\", section_header=\"\", section_content=each)\n",
    "\n",
    "# if not catboost:\n",
    "#     print_and_report([\n",
    "#         'Best Index:' + str(crossval_runner.best_index_) + '<br>',\n",
    "#         'Best Score:' + str(crossval_runner.best_score_) + '<br>',\n",
    "#         'Best Params: ' + str(crossval_runner.best_params_) + '<br>'\n",
    "#     ], \"Best Model Details\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:35.954615Z",
     "iopub.status.busy": "2022-12-07T14:52:35.953737Z",
     "iopub.status.idle": "2022-12-07T14:52:35.958648Z",
     "shell.execute_reply": "2022-12-07T14:52:35.958014Z",
     "shell.execute_reply.started": "2022-12-07T14:52:35.954570Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearly finished...\n"
     ]
    }
   ],
   "source": [
    "print('Nearly finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:56:27.642256Z",
     "iopub.status.busy": "2022-12-07T14:56:27.641373Z",
     "iopub.status.idle": "2022-12-07T14:56:30.717635Z",
     "shell.execute_reply": "2022-12-07T14:56:30.716528Z",
     "shell.execute_reply.started": "2022-12-07T14:56:27.642218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook neural_networks_with_autoencoder_20221214.ipynb to script\r\n",
      "[NbConvertApp] Writing 41460 bytes to neural_networks_with_autoencoder_20221214.py\r\n"
     ]
    }
   ],
   "source": [
    "if create_python_script and is_jupyter:\n",
    "    !jupyter nbconvert --to script 'neural_networks_with_autoencoder_20221214.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:39.054084Z",
     "iopub.status.busy": "2022-12-07T14:52:39.053800Z",
     "iopub.status.idle": "2022-12-07T14:52:39.059415Z",
     "shell.execute_reply": "2022-12-07T14:52:39.058466Z",
     "shell.execute_reply.started": "2022-12-07T14:52:39.054055Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
