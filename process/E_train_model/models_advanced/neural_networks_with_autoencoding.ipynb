{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Decide which algorithm and version of the data we are going to use for model training\n",
    "(it'll be neural network in this file)\n",
    "\n",
    "Additionally, choose:\n",
    "* if we'll skip scaling the data\n",
    "* if we'll use full categories instead of dummies\n",
    "* what fraction of the data we'll use for testing (0.1)\n",
    "* if the data split will be randomised (it won't!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:06.592418Z",
     "iopub.status.busy": "2022-12-07T14:05:06.591775Z",
     "iopub.status.idle": "2022-12-07T14:05:06.602072Z",
     "shell.execute_reply": "2022-12-07T14:05:06.601361Z",
     "shell.execute_reply.started": "2022-12-07T14:05:06.592327Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#ALGORITHM = 'Neural Network'\n",
    "ALGORITHM = 'Neural Network with autoencoding [TYPE]'\n",
    "ALGORITHM_DETAIL = ''\n",
    "ALGORITHM_DETAIL_ORIG = ALGORITHM_DETAIL\n",
    "#ALGORITHM_DETAIL += ' tbc'\n",
    "DATA_DETAIL = []\n",
    "#DATA_DETAIL = ['no scale','no dummies']\n",
    "VERSION = '11'\n",
    "\n",
    "RANDOM_STATE = 101\n",
    "TRAINING_SIZE = 0.9\n",
    "\n",
    "CROSS_VALIDATION_SCORING = 'r2'\n",
    "\n",
    "price_divisor = 1\n",
    "\n",
    "\n",
    "# ---- 10th NEURAL NETWORK STRUCTURE DEFINITION ---- #\n",
    "selected_neural_network = selected_nn_code = \"m15 mega + dropout\"\n",
    "\n",
    "ALGORITHM = ALGORITHM.replace(\"[TYPE]\", selected_nn_code)\n",
    "\n",
    "create_python_script = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: loading all dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:18.573801Z",
     "iopub.status.busy": "2022-12-07T14:52:18.573463Z",
     "iopub.status.idle": "2022-12-07T14:52:21.551851Z",
     "shell.execute_reply": "2022-12-07T14:52:21.550879Z",
     "shell.execute_reply.started": "2022-12-07T14:52:18.573774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (0.9.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"JPY_PARENT_PID\" in os.environ:\n",
    "    is_jupyter = True\n",
    "else:\n",
    "    is_jupyter = False\n",
    "\n",
    "\n",
    "if is_jupyter:\n",
    "    #! pip install scikeras\n",
    "    !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:06.607583Z",
     "iopub.status.busy": "2022-12-07T14:05:06.607393Z",
     "iopub.status.idle": "2022-12-07T14:05:08.350118Z",
     "shell.execute_reply": "2022-12-07T14:05:08.349107Z",
     "shell.execute_reply.started": "2022-12-07T14:05:06.607565Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notebook_environment': 'gradient', 'use_gpu': True, 'debug_mode': False, 'quick_mode': False, 'quick_override_cv_splits': 2, 'quick_override_n_iter': 10, 'quick_override_n_jobs': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from termcolor import colored\n",
    "from time import time\n",
    "import sklearn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "with open('../../z_envs/_envs.json') as f:\n",
    "    env_vars = json.loads(f.read())\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    run_env = 'colab'\n",
    "except:\n",
    "    try:\n",
    "        run_env = env_vars['notebook_environment']\n",
    "    except:\n",
    "        run_env = 'unknown'\n",
    "\n",
    "if \"JPY_PARENT_PID\" in os.environ:\n",
    "    is_jupyter = True\n",
    "else:\n",
    "    is_jupyter = False\n",
    "\n",
    "use_gpu = env_vars.get('use_gpu', False)\n",
    "debug_mode = env_vars.get('debug_mode', False)\n",
    "quick_mode = env_vars.get('quick_mode', False)\n",
    "OVERRIDE_CV = env_vars.get('quick_override_cv_splits', None) if quick_mode else None\n",
    "OVERRIDE_N_ITER = env_vars.get('quick_override_n_iter', None) if quick_mode else None\n",
    "OVERRIDE_JOBS = env_vars.get('quick_override_n_jobs', None) if quick_mode else None\n",
    "OVERRIDE_VERBOSE = 1\n",
    "#if quick_mode:OVERRIDE_CV, OVERRIDE_N_ITER = 2, 10\n",
    "\n",
    "already_timed = False\n",
    "no_dummies = 'no dummies' in DATA_DETAIL\n",
    "no_scaling = 'no scaling' in DATA_DETAIL\n",
    "#not_catboost = 'catboost' not in ALGORITHM.lower() or not no_dummies\n",
    "using_catboost = 'catboost' in ALGORITHM.lower()\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..', '..'))\n",
    "if module_path not in sys.path:\n",
    "    #sys.path.append(module_path+\"\\\\zfunctions\")\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "if run_env not in ['colab', 'gradient', 'cloud']:\n",
    "    cloud_run = False\n",
    "    from functions_b__get_the_data_20221116 import set_csv_directory\n",
    "    set_csv_directory('final_split')\n",
    "else:\n",
    "    cloud_run = True\n",
    "\n",
    "from functions_0__common_20221116 import get_columns\n",
    "from functions_b__get_the_data_20221116 import get_combined_dataset, get_source_dataframe\n",
    "from functions_d1__prepare_cleanse_data_20221116 import tidy_dataset\n",
    "from functions_d2__transform_enrich_data_20221116 import preprocess, feature_engineer\n",
    "from functions_d3__prepare_store_data_20221116 import create_train_test_data\n",
    "from functions_e__train_model_20221116 import get_chosen_model, make_modelling_pipeline, get_cv_params, fit_model_with_cross_validation, get_hyperparameters\n",
    "from functions_f_evaluate_model_20221116 import get_best_estimator_average_time, get_results, update_results\n",
    "\n",
    "print(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include any overrides specific to the algorthm / python environment being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:08.351758Z",
     "iopub.status.busy": "2022-12-07T14:05:08.351382Z",
     "iopub.status.idle": "2022-12-07T14:05:08.355841Z",
     "shell.execute_reply": "2022-12-07T14:05:08.354812Z",
     "shell.execute_reply.started": "2022-12-07T14:05:08.351732Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#running_locally = True\n",
    "running_locally = run_env == 'local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: creating the ANN model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:08.358371Z",
     "iopub.status.busy": "2022-12-07T14:05:08.358157Z",
     "iopub.status.idle": "2022-12-07T14:05:10.449885Z",
     "shell.execute_reply": "2022-12-07T14:05:10.449187Z",
     "shell.execute_reply.started": "2022-12-07T14:05:08.358351Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "loss_dict = {\n",
    "    \"mean_squared_error\":'mse',\n",
    "    \"mean_absolute_error\":'mae'\n",
    "            }\n",
    "\n",
    "def make_simple_ann(key, inputs=-1):\n",
    "    if False:\n",
    "        pass\n",
    "    elif key == 'quite simple':\n",
    "\n",
    "        new_algorithm_detail = ALGORITHM_DETAIL_ORIG + 'quite simple model + normalise, mse'\n",
    "\n",
    "        learn_rate = 0.1\n",
    "        epochs, chosen_loss = 100, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'recommended simple v1':\n",
    "\n",
    "        learn_rate = 0.003 #0.3\n",
    "        epochs, chosen_loss = 50, 'mean_squared_error'\n",
    "\n",
    "        new_algorithm_detail = ALGORITHM_DETAIL_ORIG + 'recommended simple model/mse'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm02 two layers':\n",
    "\n",
    "        learn_rate = 0.003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(X_train.shape[1], activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "\n",
    "    elif key == 'm03 2 layers+wider':\n",
    "\n",
    "        learn_rate = 0.0003 # 0.003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm04 3 layers+wider':\n",
    "\n",
    "        learn_rate = 0.003\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            normalizer,\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            layers.Dense(40, activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm0x four layers,wider,batchnorm':\n",
    "\n",
    "        learn_rate = 0.0003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "        #from layers.normalization import BatchNormalization\n",
    "\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        batchnorm = layers.BatchNormalization()\n",
    "        activation = layers.Activation('relu')\n",
    "\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        #new_algorithm_detail += ' +norm'\n",
    "\n",
    "        chosen_model = tf.keras.Sequential([\n",
    "            layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1],), activation='relu'),\n",
    "            #normalizer,\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            batchnorm,\n",
    "            activation,\n",
    "            layers.Dense(40, activation='relu'),\n",
    "            layers.Dense(30, activation='relu'),\n",
    "            layers.Dense(units=1)\n",
    "        ])\n",
    "\n",
    "    elif key == 'm05 rec deep':\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        # Compile the network :\n",
    "        #chosen_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        learn_rate = 0.0003 #0.3\n",
    "        epochs, chosen_loss = 500, 'mean_squared_error'\n",
    "\n",
    "    elif key == 'm11 mega':\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(2148, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(2148, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        # Compile the network :\n",
    "        #chosen_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs, chosen_loss = 400, 'mean_squared_error'\n",
    "\n",
    "    elif key == 'm12 mega':\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        # Compile the network :\n",
    "        #chosen_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs, chosen_loss = 400, 'mean_squared_error'\n",
    "    elif key == 'm13 mega':\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        #normalizer.adapt(np.array(128))\n",
    "\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(normalizer),\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs = 400\n",
    "        chosen_loss = 'mean_absolute_error' # 'mean_squared_error'\n",
    "\n",
    "    elif key == 'm14 mega':\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        batchnorm = layers.BatchNormalization()\n",
    "        activation = layers.Activation('relu')\n",
    "\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(normalizer)\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs = 400\n",
    "        chosen_loss = 'mean_absolute_error' # 'mean_squared_error'\n",
    "\n",
    "    elif key == \"m15 mega + dropout\":\n",
    "        normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "        normalizer.adapt(np.array(X_train))\n",
    "        batchnorm = layers.BatchNormalization()\n",
    "        activation = layers.Activation('relu')\n",
    "\n",
    "        chosen_model = Sequential()\n",
    "\n",
    "        # The Input Layer :\n",
    "        chosen_model.add(normalizer)\n",
    "        chosen_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "        # The Hidden Layers :\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "\n",
    "        chosen_model.add(keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(1024, kernel_initializer='normal'))\n",
    "\n",
    "        chosen_model.add(keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(512, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "        chosen_model.add(Dense(256, kernel_initializer='normal'))\n",
    "        chosen_model.add(layers.BatchNormalization())\n",
    "        chosen_model.add(activation)\n",
    "\n",
    "        # The Output Layer :\n",
    "        chosen_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "        learn_rate = 0.0003\n",
    "        epochs = 400\n",
    "        chosen_loss = 'mean_absolute_error' # 'mean_squared_error'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"make_simple_ann: no entry for key:\", key)\n",
    "\n",
    "    if running_locally:\n",
    "        epochs = 8\n",
    "\n",
    "    # Compile the network :\n",
    "    chosen_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate),\n",
    "        loss=chosen_loss)\n",
    "\n",
    "\n",
    "    new_algorithm_detail = ALGORITHM_DETAIL_ORIG + loss_dict[chosen_loss]\n",
    "    new_algorithm_detail += f' +epochs={epochs}'\n",
    "    new_algorithm_detail += f' +learn={learn_rate}'\n",
    "\n",
    "    return chosen_model, new_algorithm_detail, epochs, {'learning_rate':learn_rate}\n",
    "\n",
    "#make_simple_ann('m04 four layers,wider,batchnorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: get the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:10.451452Z",
     "iopub.status.busy": "2022-12-07T14:05:10.450990Z",
     "iopub.status.idle": "2022-12-07T14:05:10.455099Z",
     "shell.execute_reply": "2022-12-07T14:05:10.454474Z",
     "shell.execute_reply.started": "2022-12-07T14:05:10.451427Z"
    }
   },
   "outputs": [],
   "source": [
    "columns, booleans, floats, categories, custom, wildcard = get_columns(version=VERSION)\n",
    "LABEL = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:10.456195Z",
     "iopub.status.busy": "2022-12-07T14:05:10.455983Z",
     "iopub.status.idle": "2022-12-07T14:05:11.107767Z",
     "shell.execute_reply": "2022-12-07T14:05:11.107128Z",
     "shell.execute_reply.started": "2022-12-07T14:05:10.456175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data from ../../../https://raw.githubusercontent.com/jayportfolio/capstone_streamlit/main/data/final/df_listings_v11.csv\n"
     ]
    }
   ],
   "source": [
    "df, retrieval_type = get_source_dataframe(cloud_run, VERSION, folder_prefix='../../../', row_limit=None)\n",
    "df_orig = df.copy()\n",
    "\n",
    "if retrieval_type != 'tidy':\n",
    "    df = tidy_dataset(df, version=int(VERSION))\n",
    "    df = feature_engineer(df, version=int(VERSION))\n",
    "\n",
    "\n",
    "    df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.108943Z",
     "iopub.status.busy": "2022-12-07T14:05:11.108690Z",
     "iopub.status.idle": "2022-12-07T14:05:11.113380Z",
     "shell.execute_reply": "2022-12-07T14:05:11.112760Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.108920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mfeatures\u001B[0m ->  ['bedrooms', 'bathrooms', 'nearestStation', 'location.latitude', 'location.longitude', 'latitude_deviation', 'longitude_deviation', 'tenure.tenureType']\n",
      "\u001B[1m\u001B[32mlabel\u001B[0m ->  Price\n"
     ]
    }
   ],
   "source": [
    "print(colored(f\"features\", \"blue\"), \"-> \", columns)\n",
    "columns.insert(0, LABEL)\n",
    "print(colored(f\"label\", \"green\", None, ['bold']), \"-> \", LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.114368Z",
     "iopub.status.busy": "2022-12-07T14:05:11.114163Z",
     "iopub.status.idle": "2022-12-07T14:05:11.147575Z",
     "shell.execute_reply": "2022-12-07T14:05:11.146653Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.114349Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = preprocess(df, version=VERSION)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.148796Z",
     "iopub.status.busy": "2022-12-07T14:05:11.148563Z",
     "iopub.status.idle": "2022-12-07T14:05:11.173891Z",
     "shell.execute_reply": "2022-12-07T14:05:11.173199Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.148775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             Price  bedrooms  bathrooms  nearestStation  location.latitude  \\\niddd                                                                         \n14520525  550000.0       3.0        1.0        0.274316          51.529950   \n27953107  400000.0       2.0        2.0        0.305845          51.549390   \n33593487  579950.0       2.0        1.0        0.438045          51.447180   \n35271294  370000.0       2.0        1.0        0.399307          51.449568   \n44749111  475000.0       2.0        1.0        0.410550          51.370050   \n46204665  435000.0       3.0        2.0        0.314779          51.539070   \n49020666  200000.0       1.0        1.0        0.875911          51.539959   \n49036279  275000.0       2.0        1.0        0.474368          51.541780   \n49303873  450000.0       3.0        2.0        0.577040          51.524880   \n52064391  349950.0       2.0        2.0        0.212734          51.470800   \n52187854  450000.0       1.0        1.0        0.446802          51.527199   \n52845963  200000.0       2.0        1.0        0.650562          51.398040   \n52913496  220000.0       1.0        1.0        0.945991          51.539383   \n53609433  489995.0       1.0        1.0        0.087081          51.532620   \n53938989  450000.0       2.0        1.0        0.775203          51.658287   \n54713232  332000.0       2.0        1.0        0.319226          51.612300   \n54904122  365000.0       2.0        1.0        0.260722          51.593595   \n54991934  430000.0       3.0        1.0        0.497268          51.528720   \n55043230  260000.0       1.0        1.0        0.384607          51.544430   \n55187658  430000.0       2.0        2.0        0.289033          51.507570   \n55805965  280000.0       2.0        1.0        0.742859          51.520910   \n55839051  599950.0       2.0        1.0        0.259168          51.579186   \n55940994  385000.0       2.0        2.0        0.403987          51.376930   \n56449305  380000.0       2.0        1.0        0.310271          51.600483   \n57221413  475000.0       3.0        2.0        0.409784          51.497260   \n57878227  490000.0       2.0        1.0        0.052498          51.580270   \n59258796  475000.0       3.0        1.0        0.424573          51.536335   \n59658138  499995.0       1.0        1.0        0.396544          51.462211   \n60741240  435000.0       2.0        1.0        0.162014          51.612150   \n61387062  375000.0       2.0        1.0        0.493102          51.448697   \n\n          location.longitude  latitude_deviation  longitude_deviation  \\\niddd                                                                    \n14520525           -0.207020            0.030230             0.102600   \n27953107           -0.482600            0.049670             0.378180   \n33593487           -0.338770            0.052540             0.234350   \n35271294           -0.140154            0.050152             0.035734   \n44749111           -0.212410            0.129670             0.107990   \n46204665           -0.198935            0.039350             0.094515   \n49020666           -0.380863            0.040239             0.276443   \n49036279            0.037890            0.042060             0.142310   \n49303873            0.187200            0.025160             0.291620   \n52064391           -0.361820            0.028920             0.257400   \n52187854           -0.202898            0.027479             0.098478   \n52845963           -0.076812            0.101680             0.027608   \n52913496           -0.382239            0.039663             0.277819   \n53609433           -0.107860            0.032900             0.003440   \n53938989           -0.207902            0.158567             0.103482   \n54713232           -0.119860            0.112580             0.015440   \n54904122            0.022046            0.093875             0.126466   \n54991934            0.039180            0.029000             0.143600   \n55043230            0.014500            0.044710             0.118920   \n55187658            0.078030            0.007850             0.182450   \n55805965            0.022680            0.021190             0.127100   \n55839051           -0.209020            0.079466             0.104600   \n55940994           -0.238870            0.122790             0.134450   \n56449305           -0.062096            0.100763             0.042324   \n57221413           -0.422530            0.002460             0.318110   \n57878227            0.022290            0.080550             0.126710   \n59258796           -0.068537            0.036615             0.035883   \n59658138           -0.196876            0.037509             0.092456   \n60741240           -0.277430            0.112430             0.173010   \n61387062           -0.174068            0.051023             0.069648   \n\n          tenure.tenureType  feature__1 bedroom  ...  feature__2__en suite  \\\niddd                                             ...                         \n14520525          LEASEHOLD                   0  ...                     0   \n27953107          LEASEHOLD                   0  ...                     0   \n33593487           FREEHOLD                   0  ...                     0   \n35271294          LEASEHOLD                   0  ...                     0   \n44749111           FREEHOLD                   0  ...                     0   \n46204665          LEASEHOLD                   0  ...                     1   \n49020666          LEASEHOLD                   0  ...                     0   \n49036279          LEASEHOLD                   0  ...                     0   \n49303873           FREEHOLD                   0  ...                     0   \n52064391          LEASEHOLD                   0  ...                     1   \n52187854          LEASEHOLD                   0  ...                     0   \n52845963          LEASEHOLD                   0  ...                     0   \n52913496          LEASEHOLD                   0  ...                     0   \n53609433          LEASEHOLD                   0  ...                     0   \n53938989           FREEHOLD                   0  ...                     0   \n54713232  SHARE_OF_FREEHOLD                   0  ...                     0   \n54904122  SHARE_OF_FREEHOLD                   0  ...                     0   \n54991934           FREEHOLD                   0  ...                     0   \n55043230          LEASEHOLD                   0  ...                     0   \n55187658          LEASEHOLD                   0  ...                     1   \n55805965          LEASEHOLD                   0  ...                     0   \n55839051          LEASEHOLD                   0  ...                     0   \n55940994          LEASEHOLD                   0  ...                     0   \n56449305           FREEHOLD                   0  ...                     0   \n57221413           FREEHOLD                   0  ...                     0   \n57878227  SHARE_OF_FREEHOLD                   0  ...                     0   \n59258796          LEASEHOLD                   0  ...                     0   \n59658138          LEASEHOLD                   0  ...                     0   \n60741240          LEASEHOLD                   0  ...                     0   \n61387062          LEASEHOLD                   0  ...                     0   \n\n          feature__2__penthouse  feature__2__balcony  \\\niddd                                                   \n14520525                      0                    1   \n27953107                      0                    1   \n33593487                      0                    0   \n35271294                      0                    1   \n44749111                      0                    0   \n46204665                      0                    0   \n49020666                      0                    0   \n49036279                      0                    0   \n49303873                      0                    0   \n52064391                      0                    1   \n52187854                      0                    0   \n52845963                      0                    0   \n52913496                      0                    0   \n53609433                      0                    0   \n53938989                      0                    0   \n54713232                      0                    0   \n54904122                      0                    0   \n54991934                      0                    0   \n55043230                      0                    0   \n55187658                      0                    0   \n55805965                      0                    0   \n55839051                      0                    0   \n55940994                      0                    0   \n56449305                      0                    0   \n57221413                      0                    0   \n57878227                      0                    1   \n59258796                      0                    0   \n59658138                      0                    0   \n60741240                      0                    1   \n61387062                      0                    0   \n\n          feature__2__double-glazing  feature__2__double glazing  \\\niddd                                                               \n14520525                           0                           0   \n27953107                           0                           0   \n33593487                           0                           0   \n35271294                           0                           1   \n44749111                           0                           1   \n46204665                           0                           0   \n49020666                           0                           0   \n49036279                           0                           0   \n49303873                           0                           0   \n52064391                           0                           0   \n52187854                           0                           0   \n52845963                           0                           0   \n52913496                           0                           0   \n53609433                           0                           0   \n53938989                           0                           0   \n54713232                           0                           0   \n54904122                           0                           0   \n54991934                           0                           0   \n55043230                           0                           0   \n55187658                           0                           0   \n55805965                           0                           1   \n55839051                           0                           0   \n55940994                           0                           1   \n56449305                           0                           0   \n57221413                           0                           0   \n57878227                           0                           0   \n59258796                           0                           0   \n59658138                           0                           0   \n60741240                           0                           0   \n61387062                           0                           0   \n\n          feature__2__off-road parking  feature__2__security  \\\niddd                                                           \n14520525                             0                     0   \n27953107                             0                     0   \n33593487                             0                     0   \n35271294                             0                     0   \n44749111                             0                     0   \n46204665                             0                     0   \n49020666                             0                     0   \n49036279                             0                     0   \n49303873                             0                     0   \n52064391                             0                     0   \n52187854                             0                     0   \n52845963                             0                     0   \n52913496                             0                     0   \n53609433                             0                     0   \n53938989                             0                     0   \n54713232                             0                     0   \n54904122                             0                     0   \n54991934                             0                     0   \n55043230                             0                     0   \n55187658                             0                     1   \n55805965                             0                     0   \n55839051                             0                     0   \n55940994                             0                     0   \n56449305                             0                     0   \n57221413                             0                     0   \n57878227                             0                     0   \n59258796                             0                     0   \n59658138                             0                     0   \n60741240                             0                     0   \n61387062                             0                     0   \n\n          feature__2__patio  feature__2__underfloor heating  \\\niddd                                                          \n14520525                  0                               0   \n27953107                  0                               0   \n33593487                  0                               0   \n35271294                  0                               0   \n44749111                  0                               0   \n46204665                  0                               0   \n49020666                  0                               0   \n49036279                  0                               0   \n49303873                  0                               0   \n52064391                  0                               0   \n52187854                  0                               0   \n52845963                  0                               0   \n52913496                  0                               0   \n53609433                  0                               0   \n53938989                  0                               0   \n54713232                  0                               0   \n54904122                  0                               0   \n54991934                  0                               0   \n55043230                  0                               0   \n55187658                  0                               0   \n55805965                  0                               0   \n55839051                  0                               0   \n55940994                  0                               0   \n56449305                  0                               0   \n57221413                  0                               0   \n57878227                  0                               0   \n59258796                  0                               0   \n59658138                  0                               0   \n60741240                  0                               0   \n61387062                  0                               0   \n\n          feature__2__marble  \niddd                          \n14520525                   0  \n27953107                   0  \n33593487                   0  \n35271294                   0  \n44749111                   0  \n46204665                   0  \n49020666                   0  \n49036279                   0  \n49303873                   0  \n52064391                   0  \n52187854                   0  \n52845963                   0  \n52913496                   0  \n53609433                   0  \n53938989                   0  \n54713232                   0  \n54904122                   0  \n54991934                   0  \n55043230                   0  \n55187658                   0  \n55805965                   0  \n55839051                   0  \n55940994                   0  \n56449305                   0  \n57221413                   0  \n57878227                   0  \n59258796                   0  \n59658138                   0  \n60741240                   0  \n61387062                   0  \n\n[30 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>nearestStation</th>\n      <th>location.latitude</th>\n      <th>location.longitude</th>\n      <th>latitude_deviation</th>\n      <th>longitude_deviation</th>\n      <th>tenure.tenureType</th>\n      <th>feature__1 bedroom</th>\n      <th>...</th>\n      <th>feature__2__en suite</th>\n      <th>feature__2__penthouse</th>\n      <th>feature__2__balcony</th>\n      <th>feature__2__double-glazing</th>\n      <th>feature__2__double glazing</th>\n      <th>feature__2__off-road parking</th>\n      <th>feature__2__security</th>\n      <th>feature__2__patio</th>\n      <th>feature__2__underfloor heating</th>\n      <th>feature__2__marble</th>\n    </tr>\n    <tr>\n      <th>iddd</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14520525</th>\n      <td>550000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.274316</td>\n      <td>51.529950</td>\n      <td>-0.207020</td>\n      <td>0.030230</td>\n      <td>0.102600</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27953107</th>\n      <td>400000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.305845</td>\n      <td>51.549390</td>\n      <td>-0.482600</td>\n      <td>0.049670</td>\n      <td>0.378180</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33593487</th>\n      <td>579950.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.438045</td>\n      <td>51.447180</td>\n      <td>-0.338770</td>\n      <td>0.052540</td>\n      <td>0.234350</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35271294</th>\n      <td>370000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.399307</td>\n      <td>51.449568</td>\n      <td>-0.140154</td>\n      <td>0.050152</td>\n      <td>0.035734</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44749111</th>\n      <td>475000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.410550</td>\n      <td>51.370050</td>\n      <td>-0.212410</td>\n      <td>0.129670</td>\n      <td>0.107990</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46204665</th>\n      <td>435000.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.314779</td>\n      <td>51.539070</td>\n      <td>-0.198935</td>\n      <td>0.039350</td>\n      <td>0.094515</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49020666</th>\n      <td>200000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.875911</td>\n      <td>51.539959</td>\n      <td>-0.380863</td>\n      <td>0.040239</td>\n      <td>0.276443</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49036279</th>\n      <td>275000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.474368</td>\n      <td>51.541780</td>\n      <td>0.037890</td>\n      <td>0.042060</td>\n      <td>0.142310</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49303873</th>\n      <td>450000.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.577040</td>\n      <td>51.524880</td>\n      <td>0.187200</td>\n      <td>0.025160</td>\n      <td>0.291620</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52064391</th>\n      <td>349950.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.212734</td>\n      <td>51.470800</td>\n      <td>-0.361820</td>\n      <td>0.028920</td>\n      <td>0.257400</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52187854</th>\n      <td>450000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.446802</td>\n      <td>51.527199</td>\n      <td>-0.202898</td>\n      <td>0.027479</td>\n      <td>0.098478</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52845963</th>\n      <td>200000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.650562</td>\n      <td>51.398040</td>\n      <td>-0.076812</td>\n      <td>0.101680</td>\n      <td>0.027608</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52913496</th>\n      <td>220000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.945991</td>\n      <td>51.539383</td>\n      <td>-0.382239</td>\n      <td>0.039663</td>\n      <td>0.277819</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53609433</th>\n      <td>489995.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.087081</td>\n      <td>51.532620</td>\n      <td>-0.107860</td>\n      <td>0.032900</td>\n      <td>0.003440</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53938989</th>\n      <td>450000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.775203</td>\n      <td>51.658287</td>\n      <td>-0.207902</td>\n      <td>0.158567</td>\n      <td>0.103482</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54713232</th>\n      <td>332000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.319226</td>\n      <td>51.612300</td>\n      <td>-0.119860</td>\n      <td>0.112580</td>\n      <td>0.015440</td>\n      <td>SHARE_OF_FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54904122</th>\n      <td>365000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.260722</td>\n      <td>51.593595</td>\n      <td>0.022046</td>\n      <td>0.093875</td>\n      <td>0.126466</td>\n      <td>SHARE_OF_FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54991934</th>\n      <td>430000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.497268</td>\n      <td>51.528720</td>\n      <td>0.039180</td>\n      <td>0.029000</td>\n      <td>0.143600</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55043230</th>\n      <td>260000.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.384607</td>\n      <td>51.544430</td>\n      <td>0.014500</td>\n      <td>0.044710</td>\n      <td>0.118920</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55187658</th>\n      <td>430000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.289033</td>\n      <td>51.507570</td>\n      <td>0.078030</td>\n      <td>0.007850</td>\n      <td>0.182450</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55805965</th>\n      <td>280000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.742859</td>\n      <td>51.520910</td>\n      <td>0.022680</td>\n      <td>0.021190</td>\n      <td>0.127100</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55839051</th>\n      <td>599950.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.259168</td>\n      <td>51.579186</td>\n      <td>-0.209020</td>\n      <td>0.079466</td>\n      <td>0.104600</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55940994</th>\n      <td>385000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.403987</td>\n      <td>51.376930</td>\n      <td>-0.238870</td>\n      <td>0.122790</td>\n      <td>0.134450</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56449305</th>\n      <td>380000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.310271</td>\n      <td>51.600483</td>\n      <td>-0.062096</td>\n      <td>0.100763</td>\n      <td>0.042324</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57221413</th>\n      <td>475000.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.409784</td>\n      <td>51.497260</td>\n      <td>-0.422530</td>\n      <td>0.002460</td>\n      <td>0.318110</td>\n      <td>FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57878227</th>\n      <td>490000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.052498</td>\n      <td>51.580270</td>\n      <td>0.022290</td>\n      <td>0.080550</td>\n      <td>0.126710</td>\n      <td>SHARE_OF_FREEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59258796</th>\n      <td>475000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.424573</td>\n      <td>51.536335</td>\n      <td>-0.068537</td>\n      <td>0.036615</td>\n      <td>0.035883</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59658138</th>\n      <td>499995.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.396544</td>\n      <td>51.462211</td>\n      <td>-0.196876</td>\n      <td>0.037509</td>\n      <td>0.092456</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>60741240</th>\n      <td>435000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.162014</td>\n      <td>51.612150</td>\n      <td>-0.277430</td>\n      <td>0.112430</td>\n      <td>0.173010</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61387062</th>\n      <td>375000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.493102</td>\n      <td>51.448697</td>\n      <td>-0.174068</td>\n      <td>0.051023</td>\n      <td>0.069648</td>\n      <td>LEASEHOLD</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>30 rows × 81 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Price'] = df['Price'] / price_divisor # potentially making the price smaller to make the ANN perform better\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.175019Z",
     "iopub.status.busy": "2022-12-07T14:05:11.174797Z",
     "iopub.status.idle": "2022-12-07T14:05:11.211907Z",
     "shell.execute_reply": "2022-12-07T14:05:11.211335Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.174998Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_index, X_test_index, y_train_index, y_test_index, df_features, df_labels = create_train_test_data(\n",
    "    df,\n",
    "    categories=categories,\n",
    "    RANDOM_STATE=RANDOM_STATE, return_index=True,\n",
    "    drop_nulls=True,\n",
    "    no_dummies=no_dummies\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<code style=\"background:red;color:red\">**********************************************************************************************************</code>\n",
    "\n",
    "## NEW Stage: do autoencoding\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "# Load the model from file\n",
    "from keras.models import Model, load_model # for creating a Neural Network Autoencoder model\n",
    "main_dir=os.path.dirname(sys.path[0])\n",
    "encoder = load_model(main_dir+f'/data/encoder_v{VERSION}.h5')\n",
    "\n",
    "X_train_orig = X_train.copy()\n",
    "X_test_orig = X_test.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39714, 83) (4413, 83) (39714, 1) (4413, 1) (39714, 1) (4413, 1) (39714, 1) (4413, 1)\n",
      "1242/1242 [==============================] - 1s 1ms/step\n",
      "138/138 [==============================] - 0s 1ms/step\n",
      "(44127, 81)\n",
      "(39714, 42) (4413, 42) (39714, 1) (4413, 1) (39714, 1) (4413, 1) (39714, 1) (4413, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train_index.shape, X_test_index.shape,\n",
    "      y_train_index.shape, y_test_index.shape)\n",
    "\n",
    "# Encode train and test data\n",
    "X_train = encoder.predict(X_train)\n",
    "X_test = encoder.predict(X_test)\n",
    "\n",
    "#print(X_train[0])\n",
    "print(df.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train_index.shape, X_test_index.shape,\n",
    "      y_train_index.shape, y_test_index.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage:\n",
    "* #### retrieve the hyperparameters for this model, and\n",
    "* #### train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:11.213435Z",
     "iopub.status.busy": "2022-12-07T14:05:11.212918Z",
     "iopub.status.idle": "2022-12-07T14:05:12.601138Z",
     "shell.execute_reply": "2022-12-07T14:05:12.600488Z",
     "shell.execute_reply.started": "2022-12-07T14:05:11.213410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'mae +epochs=400 +learn=0.0003'"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_model, ALGORITHM_DETAIL, chosen_epochs, chosen_params = make_simple_ann(selected_neural_network)\n",
    "\n",
    "ALGORITHM_DETAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:12.602626Z",
     "iopub.status.busy": "2022-12-07T14:05:12.602064Z",
     "iopub.status.idle": "2022-12-07T14:05:12.617161Z",
     "shell.execute_reply": "2022-12-07T14:05:12.616542Z",
     "shell.execute_reply.started": "2022-12-07T14:05:12.602602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_neural_network m15 mega + dropout\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normalizat  (None, 42)               85        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               5504      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   multiple                  0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,415,830\n",
      "Trainable params: 2,408,577\n",
      "Non-trainable params: 7,253\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"selected_neural_network\",selected_neural_network)\n",
    "trainable_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:05:12.621501Z",
     "iopub.status.busy": "2022-12-07T14:05:12.621019Z",
     "iopub.status.idle": "2022-12-07T14:16:30.291615Z",
     "shell.execute_reply": "2022-12-07T14:16:30.290697Z",
     "shell.execute_reply.started": "2022-12-07T14:05:12.621480Z"
    },
    "pycharm": {
     "name": "#%%time\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1117/1117 [==============================] - 9s 7ms/step - loss: 425120.8750 - val_loss: 425865.8438\n",
      "Epoch 2/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 424949.4375 - val_loss: 425609.4062\n",
      "Epoch 3/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 424659.2500 - val_loss: 425198.5625\n",
      "Epoch 4/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 424274.2812 - val_loss: 424797.9062\n",
      "Epoch 5/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 423803.5000 - val_loss: 424233.6250\n",
      "Epoch 6/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 423252.3438 - val_loss: 423743.8125\n",
      "Epoch 7/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 422624.9375 - val_loss: 423064.7188\n",
      "Epoch 8/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 421920.7188 - val_loss: 422121.4688\n",
      "Epoch 9/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 421141.3125 - val_loss: 421490.5000\n",
      "Epoch 10/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 420289.0000 - val_loss: 420577.7500\n",
      "Epoch 11/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 419361.5000 - val_loss: 419518.2812\n",
      "Epoch 12/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 418360.0312 - val_loss: 418460.4375\n",
      "Epoch 13/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 417289.6562 - val_loss: 417282.0938\n",
      "Epoch 14/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 416140.6250 - val_loss: 416073.8750\n",
      "Epoch 15/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 414918.0312 - val_loss: 415305.0312\n",
      "Epoch 16/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 413624.8438 - val_loss: 413297.9062\n",
      "Epoch 17/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 412263.7500 - val_loss: 411250.5000\n",
      "Epoch 18/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 410844.1562 - val_loss: 411307.0312\n",
      "Epoch 19/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 409320.1250 - val_loss: 409013.3750\n",
      "Epoch 20/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 407736.5312 - val_loss: 407895.2500\n",
      "Epoch 21/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 406080.3750 - val_loss: 405998.4375\n",
      "Epoch 22/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 404359.8438 - val_loss: 404187.8438\n",
      "Epoch 23/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 402558.8750 - val_loss: 401980.4688\n",
      "Epoch 24/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 400692.5625 - val_loss: 399976.5000\n",
      "Epoch 25/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 398755.9688 - val_loss: 399142.6562\n",
      "Epoch 26/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 396735.2500 - val_loss: 395499.8438\n",
      "Epoch 27/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 394655.7188 - val_loss: 394020.1562\n",
      "Epoch 28/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 392511.7188 - val_loss: 390855.0000\n",
      "Epoch 29/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 390288.9688 - val_loss: 388218.0000\n",
      "Epoch 30/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 387994.9688 - val_loss: 385692.6562\n",
      "Epoch 31/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 385612.3750 - val_loss: 384856.0312\n",
      "Epoch 32/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 383186.5625 - val_loss: 382704.8125\n",
      "Epoch 33/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 380667.5000 - val_loss: 377797.4062\n",
      "Epoch 34/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 378092.0625 - val_loss: 376623.2188\n",
      "Epoch 35/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 375442.7812 - val_loss: 374228.7188\n",
      "Epoch 36/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 372792.9062 - val_loss: 369917.9375\n",
      "Epoch 37/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 369993.2500 - val_loss: 368331.1875\n",
      "Epoch 38/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 367102.4375 - val_loss: 363529.5625\n",
      "Epoch 39/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 364162.7812 - val_loss: 358931.7812\n",
      "Epoch 40/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 361167.1250 - val_loss: 360238.1562\n",
      "Epoch 41/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 358088.2188 - val_loss: 359117.2500\n",
      "Epoch 42/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 354937.1562 - val_loss: 355469.9062\n",
      "Epoch 43/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 351719.8750 - val_loss: 352659.6250\n",
      "Epoch 44/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 348440.7188 - val_loss: 346946.5625\n",
      "Epoch 45/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 345077.5000 - val_loss: 341495.4688\n",
      "Epoch 46/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 341677.3750 - val_loss: 339245.0000\n",
      "Epoch 47/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 338179.6875 - val_loss: 334637.5938\n",
      "Epoch 48/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 334597.8750 - val_loss: 332231.2500\n",
      "Epoch 49/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 330957.7812 - val_loss: 333421.3438\n",
      "Epoch 50/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 327254.4062 - val_loss: 325323.5938\n",
      "Epoch 51/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 323449.5625 - val_loss: 319896.9688\n",
      "Epoch 52/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 319624.7500 - val_loss: 320242.0312\n",
      "Epoch 53/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 315714.3438 - val_loss: 314344.4688\n",
      "Epoch 54/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 311753.1562 - val_loss: 309963.9688\n",
      "Epoch 55/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 307721.7188 - val_loss: 303674.4688\n",
      "Epoch 56/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 303588.0000 - val_loss: 300909.4688\n",
      "Epoch 57/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 299318.0000 - val_loss: 297168.1875\n",
      "Epoch 58/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 295066.7812 - val_loss: 292906.5000\n",
      "Epoch 59/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 290782.1250 - val_loss: 291510.0312\n",
      "Epoch 60/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 286412.9688 - val_loss: 282808.4688\n",
      "Epoch 61/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 281915.1875 - val_loss: 277672.5938\n",
      "Epoch 62/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 277357.7500 - val_loss: 278060.5938\n",
      "Epoch 63/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 272766.2500 - val_loss: 263992.1875\n",
      "Epoch 64/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 268154.7188 - val_loss: 265263.5625\n",
      "Epoch 65/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 263441.3750 - val_loss: 255239.0781\n",
      "Epoch 66/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 258661.1094 - val_loss: 254635.9688\n",
      "Epoch 67/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 253780.7656 - val_loss: 247256.9844\n",
      "Epoch 68/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 248868.6094 - val_loss: 239371.1719\n",
      "Epoch 69/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 243949.0469 - val_loss: 243162.7969\n",
      "Epoch 70/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 238890.1250 - val_loss: 224007.3906\n",
      "Epoch 71/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 233792.2969 - val_loss: 235061.3125\n",
      "Epoch 72/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 228759.0000 - val_loss: 219943.8750\n",
      "Epoch 73/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 223531.3125 - val_loss: 221901.3281\n",
      "Epoch 74/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 218392.3438 - val_loss: 216383.9375\n",
      "Epoch 75/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 213027.0938 - val_loss: 215284.8125\n",
      "Epoch 76/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 207645.0625 - val_loss: 194383.7500\n",
      "Epoch 77/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 202208.3125 - val_loss: 189205.2500\n",
      "Epoch 78/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 196736.3438 - val_loss: 185393.6250\n",
      "Epoch 79/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 191289.9375 - val_loss: 193184.3438\n",
      "Epoch 80/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 185645.2031 - val_loss: 177803.3906\n",
      "Epoch 81/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 180175.3594 - val_loss: 173498.4219\n",
      "Epoch 82/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 174502.2656 - val_loss: 168108.0781\n",
      "Epoch 83/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 168787.6094 - val_loss: 167365.4062\n",
      "Epoch 84/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 163054.5625 - val_loss: 157309.2188\n",
      "Epoch 85/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 157536.0469 - val_loss: 144860.2812\n",
      "Epoch 86/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 151834.0312 - val_loss: 149554.6719\n",
      "Epoch 87/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 146282.7812 - val_loss: 138054.9219\n",
      "Epoch 88/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 140579.5312 - val_loss: 139676.4375\n",
      "Epoch 89/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 135077.7031 - val_loss: 125569.4297\n",
      "Epoch 90/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 129349.1719 - val_loss: 111108.1562\n",
      "Epoch 91/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 123575.2266 - val_loss: 117691.6641\n",
      "Epoch 92/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 118547.2578 - val_loss: 110482.7812\n",
      "Epoch 93/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 113238.5469 - val_loss: 103934.5000\n",
      "Epoch 94/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 108089.1094 - val_loss: 100120.0000\n",
      "Epoch 95/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 103293.1016 - val_loss: 98385.0000\n",
      "Epoch 96/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 98551.4375 - val_loss: 92401.7891\n",
      "Epoch 97/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 94204.3828 - val_loss: 89214.3828\n",
      "Epoch 98/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 90331.8359 - val_loss: 81606.2031\n",
      "Epoch 99/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 86534.6094 - val_loss: 78744.1406\n",
      "Epoch 100/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 83021.4297 - val_loss: 77627.0469\n",
      "Epoch 101/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 79987.3047 - val_loss: 77547.2344\n",
      "Epoch 102/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 76738.5469 - val_loss: 72253.3125\n",
      "Epoch 103/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 74338.4297 - val_loss: 72096.2500\n",
      "Epoch 104/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 72582.6094 - val_loss: 71365.5859\n",
      "Epoch 105/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 70538.9297 - val_loss: 69414.6328\n",
      "Epoch 106/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 69007.6641 - val_loss: 64553.1680\n",
      "Epoch 107/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 67457.4922 - val_loss: 64933.2461\n",
      "Epoch 108/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 66493.8516 - val_loss: 64599.3828\n",
      "Epoch 109/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 65597.5781 - val_loss: 62841.2031\n",
      "Epoch 110/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 64924.9297 - val_loss: 63312.6758\n",
      "Epoch 111/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 64283.3398 - val_loss: 63124.8047\n",
      "Epoch 112/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 63663.7578 - val_loss: 63932.4492\n",
      "Epoch 113/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 63625.6914 - val_loss: 61966.0234\n",
      "Epoch 114/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 63076.5195 - val_loss: 63330.2656\n",
      "Epoch 115/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 62779.3750 - val_loss: 61891.3438\n",
      "Epoch 116/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 62559.8203 - val_loss: 61434.9531\n",
      "Epoch 117/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 62008.1328 - val_loss: 61331.4609\n",
      "Epoch 118/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 61530.8828 - val_loss: 62839.6680\n",
      "Epoch 119/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 61373.7344 - val_loss: 61452.1562\n",
      "Epoch 120/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 61140.3867 - val_loss: 61358.8516\n",
      "Epoch 121/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 60799.0820 - val_loss: 61231.1211\n",
      "Epoch 122/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 60618.4609 - val_loss: 60528.3047\n",
      "Epoch 123/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 60327.4727 - val_loss: 60933.3516\n",
      "Epoch 124/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 60156.9336 - val_loss: 60531.8867\n",
      "Epoch 125/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 59990.7773 - val_loss: 61081.6094\n",
      "Epoch 126/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 59670.9453 - val_loss: 60238.4805\n",
      "Epoch 127/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 59468.5039 - val_loss: 60762.2695\n",
      "Epoch 128/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 59171.8281 - val_loss: 59738.8398\n",
      "Epoch 129/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 58925.1406 - val_loss: 60169.6719\n",
      "Epoch 130/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 58560.5430 - val_loss: 60760.5039\n",
      "Epoch 131/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 58624.1953 - val_loss: 60322.7500\n",
      "Epoch 132/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 58361.0508 - val_loss: 61355.2344\n",
      "Epoch 133/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 58377.1367 - val_loss: 60084.4766\n",
      "Epoch 134/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 57737.0547 - val_loss: 60622.6602\n",
      "Epoch 135/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 57913.9258 - val_loss: 59309.5938\n",
      "Epoch 136/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 57319.9219 - val_loss: 60757.7539\n",
      "Epoch 137/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 57168.1328 - val_loss: 61959.4570\n",
      "Epoch 138/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 57262.5586 - val_loss: 59781.2461\n",
      "Epoch 139/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 57227.1445 - val_loss: 59920.9922\n",
      "Epoch 140/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 56980.0898 - val_loss: 60291.7930\n",
      "Epoch 141/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 56580.4141 - val_loss: 61622.4766\n",
      "Epoch 142/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 56484.0938 - val_loss: 61155.9219\n",
      "Epoch 143/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 56413.7383 - val_loss: 60524.5273\n",
      "Epoch 144/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 56225.4258 - val_loss: 59273.6836\n",
      "Epoch 145/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 56315.9766 - val_loss: 60191.9219\n",
      "Epoch 146/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 56076.7422 - val_loss: 59831.2500\n",
      "Epoch 147/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 55951.6836 - val_loss: 59631.8789\n",
      "Epoch 148/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 55742.5391 - val_loss: 60512.3516\n",
      "Epoch 149/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 55817.4688 - val_loss: 59047.8516\n",
      "Epoch 150/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 55495.2578 - val_loss: 59146.5469\n",
      "Epoch 151/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 55169.6719 - val_loss: 60185.1641\n",
      "Epoch 152/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 55136.9297 - val_loss: 60276.5312\n",
      "Epoch 153/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 55268.4219 - val_loss: 59117.6250\n",
      "Epoch 154/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 55127.0703 - val_loss: 59854.8828\n",
      "Epoch 155/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 55031.9844 - val_loss: 60373.0742\n",
      "Epoch 156/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 54741.5820 - val_loss: 59172.4883\n",
      "Epoch 157/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 54634.3359 - val_loss: 58919.0039\n",
      "Epoch 158/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 54580.0234 - val_loss: 60149.1289\n",
      "Epoch 159/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 54556.7969 - val_loss: 59768.6250\n",
      "Epoch 160/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 54425.3164 - val_loss: 59044.7773\n",
      "Epoch 161/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 54264.6172 - val_loss: 59466.4648\n",
      "Epoch 162/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 54538.8477 - val_loss: 59013.5039\n",
      "Epoch 163/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 54056.1172 - val_loss: 59152.6445\n",
      "Epoch 164/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 54193.9023 - val_loss: 59245.3086\n",
      "Epoch 165/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 53786.1875 - val_loss: 58969.3984\n",
      "Epoch 166/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 53832.5195 - val_loss: 58994.2461\n",
      "Epoch 167/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 53358.9766 - val_loss: 59970.3008\n",
      "Epoch 168/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 53478.3672 - val_loss: 59591.8984\n",
      "Epoch 169/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 53572.4414 - val_loss: 59338.8945\n",
      "Epoch 170/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 53267.8320 - val_loss: 59464.2148\n",
      "Epoch 171/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 53389.6797 - val_loss: 58844.1055\n",
      "Epoch 172/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 53137.1562 - val_loss: 58835.2070\n",
      "Epoch 173/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 53176.0391 - val_loss: 58908.4766\n",
      "Epoch 174/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 52902.9922 - val_loss: 59139.1133\n",
      "Epoch 175/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 52832.1367 - val_loss: 60344.6016\n",
      "Epoch 176/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 52798.5391 - val_loss: 60508.5508\n",
      "Epoch 177/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 52594.5859 - val_loss: 59969.0430\n",
      "Epoch 178/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 52671.3086 - val_loss: 59614.7812\n",
      "Epoch 179/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 52341.5352 - val_loss: 60183.0781\n",
      "Epoch 180/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 52335.8086 - val_loss: 59499.4844\n",
      "Epoch 181/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 52113.6797 - val_loss: 59581.0547\n",
      "Epoch 182/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 52202.7891 - val_loss: 59281.2500\n",
      "Epoch 183/400\n",
      "1117/1117 [==============================] - 7s 6ms/step - loss: 51953.6094 - val_loss: 59629.5078\n",
      "Epoch 184/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 51850.0508 - val_loss: 59953.9648\n",
      "Epoch 185/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51699.0664 - val_loss: 59667.0938\n",
      "Epoch 186/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 51453.4844 - val_loss: 59496.2617\n",
      "Epoch 187/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51459.7539 - val_loss: 60701.5352\n",
      "Epoch 188/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 51573.7148 - val_loss: 58951.5586\n",
      "Epoch 189/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51478.4805 - val_loss: 60242.8555\n",
      "Epoch 190/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51264.1875 - val_loss: 59148.5391\n",
      "Epoch 191/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51286.5898 - val_loss: 59929.5391\n",
      "Epoch 192/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51220.8945 - val_loss: 59089.6758\n",
      "Epoch 193/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 51002.3789 - val_loss: 59424.9336\n",
      "Epoch 194/400\n",
      "1117/1117 [==============================] - 7s 7ms/step - loss: 50760.5430 - val_loss: 60440.4258\n",
      "Epoch 195/400\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 50754.4531 - val_loss: 59666.8008\n",
      "Epoch 196/400\n",
      "1112/1117 [============================>.] - ETA: 0s - loss: 50705.8789Restoring model weights from the end of the best epoch: 171.\n",
      "1117/1117 [==============================] - 8s 7ms/step - loss: 50733.8125 - val_loss: 59598.6758\n",
      "Epoch 196: early stopping\n"
     ]
    }
   ],
   "source": [
    "val_split = 0.1\n",
    "min_delta=50 #0 #10, #50, #10, #50,\n",
    "val_delta_patience = 25 # 10\n",
    "\n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", #\"loss\", #\"val_loss\",\n",
    "    min_delta=min_delta, \n",
    "    patience=val_delta_patience,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True # False,\n",
    ")\n",
    "\n",
    "pipe_start = time()\n",
    "\n",
    "history = trainable_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=chosen_epochs,\n",
    "    # verbose=0 to suppress logging.\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split=val_split,  #0.2,\n",
    "    callbacks=[callback],\n",
    ")\n",
    "pipe_end = time()\n",
    "estimated_time = round((pipe_end - pipe_start), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:16:30.293337Z",
     "iopub.status.busy": "2022-12-07T14:16:30.292573Z",
     "iopub.status.idle": "2022-12-07T14:16:30.296473Z",
     "shell.execute_reply": "2022-12-07T14:16:30.295872Z",
     "shell.execute_reply.started": "2022-12-07T14:16:30.293311Z"
    }
   },
   "outputs": [],
   "source": [
    "#ALGORITHM_DETAIL.replace(\"epochs=\", f\"epochs={len(hist)}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Get the results and print some graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:36.528561Z",
     "iopub.status.busy": "2022-12-07T14:51:36.527933Z",
     "iopub.status.idle": "2022-12-07T14:51:36.543328Z",
     "shell.execute_reply": "2022-12-07T14:51:36.542380Z",
     "shell.execute_reply.started": "2022-12-07T14:51:36.528531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped at 196, loss=50733.81 valloss=58835.21\n",
      "loss=5.07e+04 valloss=5.88e+04 +valsplit=0.1 +patn=25 stop=196/400 \n",
      "mae +epochs=400 +learn=0.0003\n"
     ]
    },
    {
     "data": {
      "text/plain": "             loss      val_loss  epoch\n191  51220.894531  59089.675781    191\n192  51002.378906  59424.933594    192\n193  50760.542969  60440.425781    193\n194  50754.453125  59666.800781    194\n195  50733.812500  59598.675781    195",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n      <th>epoch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>191</th>\n      <td>51220.894531</td>\n      <td>59089.675781</td>\n      <td>191</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>51002.378906</td>\n      <td>59424.933594</td>\n      <td>192</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>50760.542969</td>\n      <td>60440.425781</td>\n      <td>193</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>50754.453125</td>\n      <td>59666.800781</td>\n      <td>194</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>50733.812500</td>\n      <td>59598.675781</td>\n      <td>195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "early_end_lossX = hist.iloc[-1]['loss']\n",
    "early_end_loss = hist['loss'].min()\n",
    "early_end_valloss = hist['val_loss'].min()\n",
    "\n",
    "#more_detail = f\"loss={round(early_end_loss,2)} valloss={round(early_end_valloss,2)}\"\n",
    "more_detail = f\"loss={early_end_loss:.2e} valloss={early_end_valloss:.2e}\"\n",
    "more_detail += f' +valsplit={val_split} +patn={val_delta_patience}'\n",
    "\n",
    "# f\"{x:.2e}\"\n",
    "\n",
    "if len(hist) != chosen_epochs:\n",
    "    print(f'stopped at {len(hist)}, loss={round(early_end_loss,2)} valloss={round(early_end_valloss,2)}')\n",
    "    #ALGORITHM_DETAIL += f\" +stop={len(hist)}\"\n",
    "    more_detail += f\" stop={len(hist)}/{chosen_epochs} \"\n",
    "    #more_detail += ALGORITHM_DETAIL.replace(\"epochs=\", f\"epochs={len(hist)}/\")\n",
    "\n",
    "\n",
    "if price_divisor!=1:\n",
    "    print('in preprocessing, divided all Prices by ', price_divisor)\n",
    "    more_detail += f' div={price_divisor}'\n",
    "\n",
    "\n",
    "print(more_detail)\n",
    "print(ALGORITHM_DETAIL)\n",
    "    \n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:36.545715Z",
     "iopub.status.busy": "2022-12-07T14:51:36.544947Z",
     "iopub.status.idle": "2022-12-07T14:51:36.688602Z",
     "shell.execute_reply": "2022-12-07T14:51:36.687744Z",
     "shell.execute_reply.started": "2022-12-07T14:51:36.545690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375232.03125\n",
      "37523.203125\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "    loss_fig, loss_ax = plt.subplots()\n",
    "    loss_ax.plot(history.history['loss'], label='loss')\n",
    "    loss_ax.plot(history.history['val_loss'], label='val_loss')\n",
    "    #plt.ylim([0, 10])\n",
    "    min_y = min(min(history.history['val_loss']),min(history.history['loss'])) - 100\n",
    "    #max_y = min(max(history.history['val_loss']),max(history.history['loss'])) + 500\n",
    "    #max_y = min(sorted(history.history['val_loss'])[-3],sorted(history.history['loss'])[-3]) + 100\n",
    "    max_y = min(sorted(history.history['val_loss'])[-1],sorted(history.history['val_loss'])[-1])\n",
    "    \n",
    "    print(max_y - min_y)\n",
    "    ticks = (max_y - min_y)/10\n",
    "    print(ticks)\n",
    "    \n",
    "    plt.ylim([min_y, max_y])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [Property Price]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yticks(np.arange(min_y, max_y, ticks))  # JHJH\n",
    "    return loss_fig, loss_ax\n",
    "\n",
    "loss_fig, loss_ax = plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:36.689854Z",
     "iopub.status.busy": "2022-12-07T14:51:36.689607Z",
     "iopub.status.idle": "2022-12-07T14:51:37.078758Z",
     "shell.execute_reply": "2022-12-07T14:51:37.077837Z",
     "shell.execute_reply.started": "2022-12-07T14:51:36.689832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = trainable_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.081734Z",
     "iopub.status.busy": "2022-12-07T14:51:37.081387Z",
     "iopub.status.idle": "2022-12-07T14:51:37.090508Z",
     "shell.execute_reply": "2022-12-07T14:51:37.089530Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.081700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Neural Network with autoencoding m15 mega + dropout----------\n",
      "R square Accuracy 0.47045952223189136\n",
      "Mean Absolute Error Accuracy 59807.54617394629\n",
      "Mean Squared Error Accuracy 5981904033.742625\n",
      "Root Mean Squared Error 77342.76975737697\n"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('-' * 10 + ALGORITHM + '-' * 10)\n",
    "print('R square Accuracy', R2)\n",
    "print('Mean Absolute Error Accuracy', MAE)\n",
    "print('Mean Squared Error Accuracy', MSE)\n",
    "print('Root Mean Squared Error', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.091753Z",
     "iopub.status.busy": "2022-12-07T14:51:37.091525Z",
     "iopub.status.idle": "2022-12-07T14:51:37.095934Z",
     "shell.execute_reply": "2022-12-07T14:51:37.095157Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.091734Z"
    }
   },
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    print(y_test_index.reshape((-1, 1)).shape);\n",
    "    print(y_pred.reshape((-1, 1)).shape);\n",
    "    print(y_test.shape);\n",
    "    print(y_test_index.shape);\n",
    "    print(y_pred.shape);\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.096842Z",
     "iopub.status.busy": "2022-12-07T14:51:37.096647Z",
     "iopub.status.idle": "2022-12-07T14:51:37.131204Z",
     "shell.execute_reply": "2022-12-07T14:51:37.130413Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.096824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           actual  predicted    difference    diff 1 %   diff 2 %   Price  \\\n109722224  108000     370488  262488.43750  243.044850  70.849293  108000   \n122265818  160000     543984  383984.50000  239.990312  70.587397  160000   \n83639617   190000     513960  323960.65625  170.505609  63.032190  190000   \n85667184   122000     322542  200542.53125  164.379124  62.175531  122000   \n123427112  175000     374016  199016.18750  113.723536  53.210581  175000   \n...           ...        ...           ...         ...        ...     ...   \n121806992  550000     550112     112.18750    0.020398   0.020394  550000   \n116011700  580000     580081      81.81250    0.014106   0.014104  580000   \n124320671  418000     417944      55.81250    0.013352   0.013354  418000   \n85543506   425000     425040      40.09375    0.009434   0.009433  425000   \n123333368  400000     399988      11.21875    0.002805   0.002805  400000   \n\n           bedrooms  bathrooms  nearestStation  location.latitude  \\\n109722224         1          1        0.302667          51.406803   \n122265818         3          1        0.166024          51.510420   \n83639617          2          2        0.162816          51.491264   \n85667184          1          1        0.838896          51.540462   \n123427112         1          1        0.069943          51.501222   \n...             ...        ...             ...                ...   \n121806992         3          1        0.488917          51.411369   \n116011700         5          3        0.533252          51.543142   \n124320671         1          1        0.158676          51.526031   \n85543506          1          1        0.082873          51.530955   \n123333368         2          1        0.554916          51.418740   \n\n           location.longitude  latitude_deviation  longitude_deviation  \\\n109722224           -0.261328            0.092917             0.156908   \n122265818           -0.037730            0.010700             0.066690   \n83639617             0.064660            0.008456             0.169080   \n85667184            -0.280301            0.040742             0.175881   \n123427112            0.060959            0.001502             0.165379   \n...                       ...                 ...                  ...   \n121806992           -0.180923            0.088351             0.076503   \n116011700            0.031340            0.043422             0.135760   \n124320671           -0.059580            0.026311             0.044840   \n85543506            -0.122292            0.031235             0.017872   \n123333368           -0.293210            0.080980             0.188790   \n\n          tenure.tenureType  \n109722224         LEASEHOLD  \n122265818          FREEHOLD  \n83639617          LEASEHOLD  \n85667184          LEASEHOLD  \n123427112         LEASEHOLD  \n...                     ...  \n121806992          FREEHOLD  \n116011700          FREEHOLD  \n124320671         LEASEHOLD  \n85543506          LEASEHOLD  \n123333368         LEASEHOLD  \n\n[4413 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>difference</th>\n      <th>diff 1 %</th>\n      <th>diff 2 %</th>\n      <th>Price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>nearestStation</th>\n      <th>location.latitude</th>\n      <th>location.longitude</th>\n      <th>latitude_deviation</th>\n      <th>longitude_deviation</th>\n      <th>tenure.tenureType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>109722224</th>\n      <td>108000</td>\n      <td>370488</td>\n      <td>262488.43750</td>\n      <td>243.044850</td>\n      <td>70.849293</td>\n      <td>108000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.302667</td>\n      <td>51.406803</td>\n      <td>-0.261328</td>\n      <td>0.092917</td>\n      <td>0.156908</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>122265818</th>\n      <td>160000</td>\n      <td>543984</td>\n      <td>383984.50000</td>\n      <td>239.990312</td>\n      <td>70.587397</td>\n      <td>160000</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.166024</td>\n      <td>51.510420</td>\n      <td>-0.037730</td>\n      <td>0.010700</td>\n      <td>0.066690</td>\n      <td>FREEHOLD</td>\n    </tr>\n    <tr>\n      <th>83639617</th>\n      <td>190000</td>\n      <td>513960</td>\n      <td>323960.65625</td>\n      <td>170.505609</td>\n      <td>63.032190</td>\n      <td>190000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.162816</td>\n      <td>51.491264</td>\n      <td>0.064660</td>\n      <td>0.008456</td>\n      <td>0.169080</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>85667184</th>\n      <td>122000</td>\n      <td>322542</td>\n      <td>200542.53125</td>\n      <td>164.379124</td>\n      <td>62.175531</td>\n      <td>122000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.838896</td>\n      <td>51.540462</td>\n      <td>-0.280301</td>\n      <td>0.040742</td>\n      <td>0.175881</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>123427112</th>\n      <td>175000</td>\n      <td>374016</td>\n      <td>199016.18750</td>\n      <td>113.723536</td>\n      <td>53.210581</td>\n      <td>175000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.069943</td>\n      <td>51.501222</td>\n      <td>0.060959</td>\n      <td>0.001502</td>\n      <td>0.165379</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>121806992</th>\n      <td>550000</td>\n      <td>550112</td>\n      <td>112.18750</td>\n      <td>0.020398</td>\n      <td>0.020394</td>\n      <td>550000</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.488917</td>\n      <td>51.411369</td>\n      <td>-0.180923</td>\n      <td>0.088351</td>\n      <td>0.076503</td>\n      <td>FREEHOLD</td>\n    </tr>\n    <tr>\n      <th>116011700</th>\n      <td>580000</td>\n      <td>580081</td>\n      <td>81.81250</td>\n      <td>0.014106</td>\n      <td>0.014104</td>\n      <td>580000</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0.533252</td>\n      <td>51.543142</td>\n      <td>0.031340</td>\n      <td>0.043422</td>\n      <td>0.135760</td>\n      <td>FREEHOLD</td>\n    </tr>\n    <tr>\n      <th>124320671</th>\n      <td>418000</td>\n      <td>417944</td>\n      <td>55.81250</td>\n      <td>0.013352</td>\n      <td>0.013354</td>\n      <td>418000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.158676</td>\n      <td>51.526031</td>\n      <td>-0.059580</td>\n      <td>0.026311</td>\n      <td>0.044840</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>85543506</th>\n      <td>425000</td>\n      <td>425040</td>\n      <td>40.09375</td>\n      <td>0.009434</td>\n      <td>0.009433</td>\n      <td>425000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.082873</td>\n      <td>51.530955</td>\n      <td>-0.122292</td>\n      <td>0.031235</td>\n      <td>0.017872</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>123333368</th>\n      <td>400000</td>\n      <td>399988</td>\n      <td>11.21875</td>\n      <td>0.002805</td>\n      <td>0.002805</td>\n      <td>400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.554916</td>\n      <td>51.418740</td>\n      <td>-0.293210</td>\n      <td>0.080980</td>\n      <td>0.188790</td>\n      <td>LEASEHOLD</td>\n    </tr>\n  </tbody>\n</table>\n<p>4413 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = np.hstack((y_test_index, y_test, y_pred))\n",
    "compare_df = DataFrame(compare, columns=['reference', 'actual', 'predicted'])\n",
    "compare_df['difference'] = abs(compare_df['actual'] - compare_df['predicted'])\n",
    "compare_df['diff 1 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['actual'] * 100)\n",
    "compare_df['diff 2 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['predicted']) * 100\n",
    "compare_df['reference'] = compare_df['reference'].astype(int)\n",
    "compare_df.set_index('reference', inplace=True)\n",
    "\n",
    "combined = compare_df.merge(df[columns], how='inner', left_index=True, right_index=True).sort_values(['diff 1 %'],\n",
    "                                                                                                     ascending=False)\n",
    "#pd.options.display.float_format = '{:.4f}'.format\n",
    "combined[['predicted', 'actual', 'Price', 'bedrooms', 'bathrooms']] = combined[\n",
    "    ['predicted', 'actual', 'Price', 'bedrooms', 'bathrooms']].astype(int)\n",
    "combined['bedrooms'] = combined['bedrooms'].astype(int)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.132382Z",
     "iopub.status.busy": "2022-12-07T14:51:37.132155Z",
     "iopub.status.idle": "2022-12-07T14:51:37.267074Z",
     "shell.execute_reply": "2022-12-07T14:51:37.266314Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.132361Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_model_fig, best_model_ax = plt.subplots()\n",
    "best_model_ax.scatter(y_test, y_pred, edgecolors=(0, 0, 1))\n",
    "best_model_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "best_model_ax.set_ylabel('Predicted')\n",
    "best_model_ax.set_xlabel('Actual')\n",
    "#ax.title.set_text(f'CV Chosen best option ({calculated_best_pipe[1]})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.268497Z",
     "iopub.status.busy": "2022-12-07T14:51:37.268238Z",
     "iopub.status.idle": "2022-12-07T14:51:37.638274Z",
     "shell.execute_reply": "2022-12-07T14:51:37.637596Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.268474Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cv_best_model_fit_time = estimated_time\n",
    "\n",
    "DD2 = \"(\" + \",\".join(DATA_DETAIL) + \")\" if len(DATA_DETAIL) >= 1 else \"\"\n",
    "key = f'{ALGORITHM} (v{VERSION})'.lower()\n",
    "\n",
    "method = f\"{ALGORITHM_DETAIL}{DD2}\"\n",
    "\n",
    "new_results = {\n",
    "    #'_score': score,\n",
    "    '_score': R2,\n",
    "    'R square Accuracy': R2,\n",
    "    'Mean Absolute Error Accuracy': MAE * price_divisor,\n",
    "    'Mean Squared Error Accuracy': MSE * price_divisor,\n",
    "    'Root Mean Squared Error': RMSE * price_divisor,\n",
    "    '_train time': cv_best_model_fit_time,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'date': str(datetime.now()),\n",
    "    #'_params': crossval_runner.best_params_ if not_catboost else cat_params,\n",
    "    #'_params': 'not available', # REPLACED - can't have different models all saying params not available\n",
    "    '_params': ALGORITHM_DETAIL,\n",
    "    '_method': more_detail, #ALGORITHM_DETAIL,\n",
    "    'run_env': run_env\n",
    "}\n",
    "\n",
    "if run_env not in ['colab']:\n",
    "    old_results_json = get_results()\n",
    "    try:\n",
    "        old_best_score = old_results_json[key]['best score']\n",
    "    except:\n",
    "        print(f\"haven't scored this model yet: {ALGORITHM}\")\n",
    "        old_best_score = -999\n",
    "    this_model_is_best = update_results(old_results_json, new_results, key)\n",
    "\n",
    "print(key)\n",
    "print(ALGORITHM_DETAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T14:51:37.639406Z",
     "iopub.status.busy": "2022-12-07T14:51:37.639158Z",
     "iopub.status.idle": "2022-12-07T14:51:37.644860Z",
     "shell.execute_reply": "2022-12-07T14:51:37.644222Z",
     "shell.execute_reply.started": "2022-12-07T14:51:37.639382Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if this_model_is_best:\n",
    "    with open(f'../../../models/optimised_model_{ALGORITHM}_v{VERSION}{DD2}.pkl', 'wb') as f:\n",
    "        pickle.dump(trainable_model, f)\n",
    "        new_model_decision = f\"pickled new version of model\\n{old_results_json[key]['_score']} is new best score (it's better than {old_best_score})\"\n",
    "        #print(results_json[key]['_score'], 'is an improvement on', results_json[key]['second best score'])\n",
    "else:\n",
    "    new_model_decision = f\"not updated saved model, the previous run was better\\n{old_results_json[key]['_score']} is worse than or equal to {old_best_score}\"\n",
    "\n",
    "print(new_model_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Write the final report for this algorithm and dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:26.522741Z",
     "iopub.status.busy": "2022-12-07T14:52:26.522245Z",
     "iopub.status.idle": "2022-12-07T14:52:27.862657Z",
     "shell.execute_reply": "2022-12-07T14:52:27.861880Z",
     "shell.execute_reply.started": "2022-12-07T14:52:26.522708Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def include_in_html_report(type, section_header=None, section_figure=None, section_content=None, section_content_list=None):\n",
    "\n",
    "    # writePath_html = r'model_results/%s (html).html' % key\n",
    "    # writePath_md = r'model_results/%s (md).md' % key\n",
    "    results_root = '../../F_evaluate_model'\n",
    "    writePath_html = f'{results_root}/html/{key}.html'.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "    writePath_md = f'{results_root}/markdown/{key}.md'\n",
    "\n",
    "#isinstance(ini_list2, list)\n",
    "    if not section_content_list:\n",
    "        section_content_list = [section_content]\n",
    "\n",
    "    if type == 'header':\n",
    "        w = 'w' if section_figure <= 1 else 'a'\n",
    "        with open(writePath_html, w) as f1:\n",
    "            headers = f'<h{section_figure}>{section_content}</h{section_figure}>'\n",
    "            f1.write(headers)\n",
    "        with open(writePath_md, w) as f2:\n",
    "            headers = f'{\"#\" * int(section_figure)} {section_content }\\n'\n",
    "            f2.write(headers)\n",
    "    else:\n",
    "        if section_header:\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                f1.write(f'<h3>{section_header}</h3>')\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                f2.write(f'### {section_header}\\n')\n",
    "\n",
    "        if type=='dataframe':\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                dfAsString = section_content.to_html()\n",
    "                f1.write(dfAsString)\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                dfAsString = section_content.to_markdown()\n",
    "                f2.write(dfAsString + '\\n\\n')\n",
    "        elif type=='graph':\n",
    "            filename = key + \"_\" + section_content\n",
    "            #section_figure.savefig(f'model_results/artifacts/{filename.replace(\" \", \"_\")}')\n",
    "            section_figure.savefig(f'{results_root}/artifacts/{filename.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")}')\n",
    "\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                dfAsString = f'<img src=\"../artifacts/{filename.replace(\" \",\"_\").replace(\"(\", \"_\").replace(\")\", \"_\")}\"/>'\n",
    "                f1.write(dfAsString)\n",
    "\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                #dfAsString = f'(./model_results/artifacts/{filename}) \\n'\n",
    "                #dfAsString = f'![detail](./artifacts/{filename.replace(\" \",\"_\")})'\n",
    "                dfAsString = f'![detail](../artifacts/{filename.replace(\" \",\"_\").replace(\"(\", \"_\").replace(\")\", \"_\")})'\n",
    "                f2.write(dfAsString)\n",
    "                f2.write('\\n\\n')\n",
    "        elif type=='json':\n",
    "\n",
    "            # html_content_parsed = [[cell.text for cell in row(\"td\")]\n",
    "            #              for row in BeautifulSoup(content,features=\"html.parser\")(\"tr\")]\n",
    "            #\n",
    "            # html_content_dictionary = {element[0]:element[1:] for element in html_content_parsed}\n",
    "\n",
    "            #xxxprint(json.dumps(html_content_dictionary, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                #f.write(json.dumps(html_content_dictionary, indent=4))\n",
    "                soup = BeautifulSoup(section_content, \"html.parser\")\n",
    "                f1.write(str(soup.prettify()))\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                #f.write(json.dumps(html_content_dictionary, indent=4))\n",
    "                soup = BeautifulSoup(section_content, \"html.parser\")\n",
    "                #f2.write(str(soup.prettify()))\n",
    "\n",
    "\n",
    "                # html_content_dictionary = {element[0]:element[1:] for element in html_content_parsed}\n",
    "                # f2.write(json.dumps(html_content_dictionary, indent=4))\n",
    "\n",
    "                import ast\n",
    "                loads = ast.literal_eval(section_content)\n",
    "                #df = pd.DataFrame.from_dict(loads)\n",
    "                #df.drop(['dont'], axis=1, inplace=True)\n",
    "                #print(df.to_markdown(index=False,tablefmt='fancy_grid'))\n",
    "                for each in loads:\n",
    "                    f2.write(each + \" = \" + str(loads[each]) + \"\\n\\n\")\n",
    "\n",
    "        elif type=='dict':\n",
    "\n",
    "            for section_content in section_content_list:\n",
    "                if isinstance(section_content, str):\n",
    "                    import ast\n",
    "                    section_content = ast.literal_eval(section_content)\n",
    "\n",
    "                with open(writePath_html, 'a') as f1:\n",
    "                    soup = BeautifulSoup(str(section_content), \"html.parser\")\n",
    "                    f1.write(str(soup.prettify()))\n",
    "                with open(writePath_md, 'a') as f2:\n",
    "                    for each in section_content:\n",
    "                        f2.write(each + \" = \" + str(section_content[each]) + \"\\n\\n\")\n",
    "\n",
    "        elif type=='text':\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                for each_line in section_content_list:\n",
    "                    f1.write(each_line + '<br>')\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                for each_line in section_content_list:\n",
    "                    f2.write(each_line + '\\n\\n')\n",
    "\n",
    "        with open(writePath_html, 'a') as f1:\n",
    "            f1.write('<hr>')\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Results from {ALGORITHM}\", section_figure=1)\n",
    "\n",
    "end_timestamp = datetime.now()\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=f\"Dataset Version: {VERSION}\", section_content_list=[\n",
    "    f\"Date run: {datetime.now()}\"\n",
    "    \"\",\n",
    "    f\"Start time: {start_timestamp}\",\n",
    "    f\"End time: {end_timestamp}\",\n",
    "])\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Results\", section_figure=2)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Summary\", section_content=new_model_decision)\n",
    "\n",
    "\n",
    "include_in_html_report(type='graph', section_header=\"Best Model: Comparing model predictions to actual property values\", section_figure=best_model_fig, section_content='best_ann_model.png')\n",
    "\n",
    "#include_in_html_report(type=\"dataframe\",text_single=\"Tuned Models ranked by performance\", content=cv_results_df_sorted)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Model Specific Notes\", section_content_list=[\"can't display hyperparameter comparison for neural network\",\"can't display model performance graphs for neural network\",\"can't display model performance graphs for neural network\"])\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Neural Network Loss - Head\", section_content=hist.head())\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=None, section_content='')\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Neural Network Loss - Tail\", section_content=hist.tail())\n",
    "\n",
    "\n",
    "include_in_html_report(type='graph', section_header=None, section_figure=loss_fig, section_content='end_loss.png')\n",
    "\n",
    "import io\n",
    "def get_model_summary(model):\n",
    "    stream = io.StringIO()\n",
    "    model.summary(line_length=160, print_fn=lambda x: stream.write('>' + x.replace('-','').replace('=','') + '\\n'))\n",
    "    summary_string = stream.getvalue()\n",
    "    stream.close()\n",
    "    return summary_string\n",
    "\n",
    "short_model_summary = get_model_summary(trainable_model)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Model Structure\", section_content=short_model_summary)\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Comparison with other models\", section_figure=2)\n",
    "\n",
    "\n",
    "dff = pd.read_json('../../../results/results.json')\n",
    "\n",
    "version = VERSION\n",
    "\n",
    "\n",
    "all_models_df = dff[dff.columns].T.sort_values(\"best score\", ascending=False)\n",
    "version_models_df = dff[[c for c in dff.columns if version in c]].T.sort_values(\"best score\", ascending=False)\n",
    "\n",
    "version_models_summary = version_models_df[['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy', 'Root Mean Squared Error', 'best run date', 'best method']]\n",
    "all_models_summary = all_models_df[['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy', 'Root Mean Squared Error', 'best run date', 'best method']]\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=f\"Comparison with version {VERSION} performances\", section_content=version_models_summary)\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Comparison with all model performances\", section_content=all_models_summary)\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Appendix\", section_figure=2)\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Data Sample\", section_content=df.head(5))\n",
    "\n",
    "if False:\n",
    "    include_in_html_report(type=\"json\", section_header=\"Hyperparameter options for Randomized Grid Search\", section_content=f\"{param_options if not using_catboost else options_block}\")\n",
    "else:\n",
    "\n",
    "    include_in_html_report(type=\"text\", section_header=\"FIX THIS!!\", section_content=\"FIX THIS!\")\n",
    "\n",
    "include_in_html_report(type=\"dict\", section_header=\"Environment Variables\", section_content=env_vars)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Useful info\",\n",
    "                       section_content_list=[f\"Tensorflow version: {tf.__version__}\"\n",
    "                                        ])\n",
    "\n",
    "\n",
    "def print_and_report(text_single, title):\n",
    "    include_in_html_report(\"text\", section_content=title)\n",
    "    for each in text_single:\n",
    "        print(each)\n",
    "        include_in_html_report(\"text\", section_header=\"\", section_content=each)\n",
    "\n",
    "# if not catboost:\n",
    "#     print_and_report([\n",
    "#         'Best Index:' + str(crossval_runner.best_index_) + '<br>',\n",
    "#         'Best Score:' + str(crossval_runner.best_score_) + '<br>',\n",
    "#         'Best Params: ' + str(crossval_runner.best_params_) + '<br>'\n",
    "#     ], \"Best Model Details\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:35.954615Z",
     "iopub.status.busy": "2022-12-07T14:52:35.953737Z",
     "iopub.status.idle": "2022-12-07T14:52:35.958648Z",
     "shell.execute_reply": "2022-12-07T14:52:35.958014Z",
     "shell.execute_reply.started": "2022-12-07T14:52:35.954570Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Nearly finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:56:27.642256Z",
     "iopub.status.busy": "2022-12-07T14:56:27.641373Z",
     "iopub.status.idle": "2022-12-07T14:56:30.717635Z",
     "shell.execute_reply": "2022-12-07T14:56:30.716528Z",
     "shell.execute_reply.started": "2022-12-07T14:56:27.642218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if create_python_script and is_jupyter:\n",
    "    !jupyter nbconvert --to script 'neural_networks_model.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T14:52:39.054084Z",
     "iopub.status.busy": "2022-12-07T14:52:39.053800Z",
     "iopub.status.idle": "2022-12-07T14:52:39.059415Z",
     "shell.execute_reply": "2022-12-07T14:52:39.058466Z",
     "shell.execute_reply.started": "2022-12-07T14:52:39.054055Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
