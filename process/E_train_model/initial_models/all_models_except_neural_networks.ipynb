{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Decide which algorithm and version of the data we are going to use for model training\n",
    "\n",
    "Additionally, choose:\n",
    "* if we'll skip scaling the data\n",
    "* if we'll use full categories instead of dummies\n",
    "* what fraction of the data we'll use for testing (0.1)\n",
    "* if the data split will be randomised (it won't!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-02T00:17:10.740849Z",
     "iopub.status.busy": "2023-01-02T00:17:10.740027Z",
     "iopub.status.idle": "2023-01-02T00:17:10.751300Z",
     "shell.execute_reply": "2023-01-02T00:17:10.750380Z",
     "shell.execute_reply.started": "2023-01-02T00:17:10.740742Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALGORITHM: Linear Regression (Ridge)\n",
      "ALGORITHM_DETAIL: rerun best\n",
      "DATA VERSION: 09\n",
      "DATA_DETAIL: []\n"
     ]
    }
   ],
   "source": [
    "FILENAME = 'all_models_except_neural_networks'\n",
    "\n",
    "ALGORITHM = 'Linear Regression (Ridge)'\n",
    "#ALGORITHM = 'KNN'\n",
    "#ALGORITHM = 'Decision Tree'\n",
    "#ALGORITHM = 'Random Forest'\n",
    "#ALGORITHM = 'XG Boost (tree)'\n",
    "#ALGORITHM = 'CatBoost'\n",
    "#ALGORITHM = 'Light Gradient Boosting'\n",
    "\n",
    "ALGORITHM_DETAIL = 'random search'\n",
    "ALGORITHM_DETAIL = 'rerun best'\n",
    "#DATA_DETAIL = ['no scale','no dummies']\n",
    "#DATA_DETAIL = ['explore param']\n",
    "DATA_DETAIL = ['no dummies'] if 'catboost' in ALGORITHM.lower() else []\n",
    "#VERSION = '06'\n",
    "VERSION = '09'\n",
    "\n",
    "RANDOM_STATE = 101\n",
    "TRAINING_SIZE = 0.9\n",
    "\n",
    "CROSS_VALIDATION_SCORING = 'r2'\n",
    "\n",
    "print(f'ALGORITHM: {ALGORITHM}')\n",
    "print(f'ALGORITHM_DETAIL: {ALGORITHM_DETAIL}')\n",
    "print(f'DATA VERSION: {VERSION}')\n",
    "print(f'DATA_DETAIL: {DATA_DETAIL}')\n",
    "\n",
    "model_uses_feature_importances = 'tree' in ALGORITHM.lower() or 'forest' in ALGORITHM.lower() or 'boost' in ALGORITHM.lower()\n",
    "create_python_script = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: loading all dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-02T00:17:11.586720Z",
     "iopub.status.busy": "2023-01-02T00:17:11.586366Z",
     "iopub.status.idle": "2023-01-02T00:17:13.009134Z",
     "shell.execute_reply": "2023-01-02T00:17:13.008425Z",
     "shell.execute_reply.started": "2023-01-02T00:17:11.586693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'notebook_environment': 'gradient', 'use_gpu': True, 'debug_mode': False, 'quick_mode': False, 'quick_override_cv_splits': 2, 'quick_override_n_iter': 10, 'quick_override_n_jobs': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from termcolor import colored\n",
    "from time import time\n",
    "import sklearn\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "start_timestamp = datetime.now()\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..', '..', '..'))\n",
    "if module_path not in sys.path:\n",
    "    #sys.path.append(module_path+\"\\\\zfunctions\")\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "with open('../../z_envs/_envs.json') as f:\n",
    "    env_vars = json.loads(f.read())\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    run_env = 'colab'\n",
    "except:\n",
    "    try:\n",
    "        run_env = env_vars['notebook_environment']\n",
    "    except:\n",
    "        run_env = 'unknown'\n",
    "\n",
    "if \"JPY_PARENT_PID\" in os.environ:\n",
    "    is_jupyter = True\n",
    "else:\n",
    "    is_jupyter = False\n",
    "\n",
    "use_gpu = env_vars.get('use_gpu', False)\n",
    "debug_mode = env_vars.get('debug_mode', False)\n",
    "quick_mode = env_vars.get('quick_mode', False)\n",
    "OVERRIDE_CV = env_vars.get('quick_override_cv_splits', None) if quick_mode else None\n",
    "OVERRIDE_N_ITER = env_vars.get('quick_override_n_iter', None) if quick_mode else None\n",
    "OVERRIDE_JOBS = env_vars.get('quick_override_n_jobs', None) if quick_mode else None\n",
    "OVERRIDE_VERBOSE = 1\n",
    "#if quick_mode:OVERRIDE_CV, OVERRIDE_N_ITER = 2, 10\n",
    "\n",
    "already_timed = False\n",
    "no_dummies = 'no dummies' in DATA_DETAIL\n",
    "no_scaling = 'no scaling' in DATA_DETAIL\n",
    "#not_catboost = 'catboost' not in ALGORITHM.lower() or not no_dummies\n",
    "using_catboost = 'catboost' in ALGORITHM.lower()\n",
    "\n",
    "if run_env not in ['colab', 'gradient', 'cloud']:\n",
    "    cloud_run = False\n",
    "    from functions_b__get_the_data_20221116 import set_csv_directory\n",
    "    set_csv_directory('final_split')\n",
    "else:\n",
    "    cloud_run = True\n",
    "\n",
    "from functions_0__common_20221116 import get_columns\n",
    "from functions_b__get_the_data_20221116 import get_combined_dataset, get_source_dataframe\n",
    "from functions_d1__prepare_cleanse_data_20221116 import tidy_dataset\n",
    "from functions_d2__transform_enrich_data_20221116 import preprocess, feature_engineer\n",
    "from functions_d3__prepare_store_data_20221116 import create_train_test_data\n",
    "from functions_e__train_model_20221116 import get_chosen_model, make_modelling_pipeline, get_cv_params, fit_model_with_cross_validation, get_hyperparameters\n",
    "from functions_f_evaluate_model_20221116 import get_best_estimator_average_time, get_results, update_results\n",
    "\n",
    "print(env_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-02T00:17:13.011220Z",
     "iopub.status.busy": "2023-01-02T00:17:13.010579Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "if is_jupyter:\n",
    "    %pip install tabulate\n",
    "\n",
    "    if ALGORITHM == 'CatBoost':\n",
    "        %pip install catboost\n",
    "\n",
    "    if ALGORITHM == 'Light Gradient Boosting':\n",
    "        %pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include any overrides specific to the algorthm / python environment being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#running_locally = True\n",
    "running_locally = run_env == 'local'\n",
    "\n",
    "\n",
    "if 'forest' in ALGORITHM.lower():\n",
    "    #OVERRIDE_N_ITER = 5\n",
    "    OVERRIDE_N_ITER = 50\n",
    "    if use_gpu:\n",
    "        #OVERRIDE_JOBS = 8\n",
    "        OVERRIDE_JOBS = 4\n",
    "\n",
    "if running_locally:\n",
    "    if ALGORITHM.lower() in ['random forest','xg boost','xg boost (linear)','xg boost (tree)' ]:\n",
    "        OVERRIDE_N_ITER = 3\n",
    "    elif 'linear regression' in ALGORITHM.lower():\n",
    "        OVERRIDE_N_ITER = 15\n",
    "    else:\n",
    "        OVERRIDE_N_ITER = 5\n",
    "\n",
    "if ALGORITHM.lower() in ['xg boost','xg boost (linear)','xg boost (tree)']:\n",
    "        OVERRIDE_N_ITER = 20\n",
    "\n",
    "if 'forest' in ALGORITHM.lower() or True:\n",
    "    OVERRIDE_VERBOSE = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: defining the model pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "def make_pipeline():\n",
    "    return Pipeline([\n",
    "        #('mms', MinMaxScaler()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('model', get_chosen_model(ALGORITHM))\n",
    "    ])\n",
    "\n",
    "\n",
    "starter_pipe = make_pipeline()\n",
    "starter_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns, booleans, floats, categories, custom, wildcard = get_columns(version=VERSION)\n",
    "LABEL = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, retrieval_type = get_source_dataframe(cloud_run, VERSION, folder_prefix='../../../', row_limit=None)\n",
    "df_orig = df.copy()\n",
    "\n",
    "if retrieval_type != 'tidy':\n",
    "    df = tidy_dataset(df, version=int(VERSION))\n",
    "    df = feature_engineer(df, version=int(VERSION))\n",
    "\n",
    "\n",
    "    df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(f\"features\", \"blue\"), \"-> \", columns)\n",
    "columns.insert(0, LABEL)\n",
    "print(colored(f\"label\", \"green\", None, ['bold']), \"-> \", LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df, version=VERSION)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_index, X_test_index, y_train_index, y_test_index, df_features, df_labels = create_train_test_data(\n",
    "    df,\n",
    "    categories=categories,\n",
    "    RANDOM_STATE=RANDOM_STATE, return_index=True,\n",
    "    drop_nulls=True,\n",
    "    no_dummies=no_dummies\n",
    ")\n",
    "\n",
    "\n",
    "if 'forest' in ALGORITHM.lower() or ALGORITHM.lower()=='light gradient boosting':\n",
    "    y_train_orig = y_train\n",
    "    y_train = y_train.ravel()\n",
    "\n",
    "#print(X_train[0])\n",
    "print(df.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train_index.shape, X_test_index.shape,\n",
    "      y_train_index.shape, y_test_index.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer = SimpleImputer(strategy='mean')\n",
    "#imputer.fit(X_train[6])\n",
    "#X_train[6] = imputer.transform(X_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_model = starter_pipe[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage:\n",
    "* #### retrieve the hyperparameters for this model, and\n",
    "* #### train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "options_block = get_hyperparameters(ALGORITHM, use_gpu, prefix='../../../')\n",
    "\n",
    "if 'explore param' in DATA_DETAIL:\n",
    "    def automl_step(param_options, vary):\n",
    "        for key2, value in param_options.items():\n",
    "            #print(key2, value, vary)\n",
    "            if key2 != vary and key2 != 'model__' + vary:\n",
    "                try:\n",
    "                    param_options[key2] = [param_options[key2][0]]\n",
    "                except:\n",
    "                    # value probably wasn't an list of options\n",
    "                    param_options[key2] = [param_options[key2]]\n",
    "        return param_options\n",
    "\n",
    "    #options_block = automl_step(options_block, \"model__epochs\")\n",
    "    explore_param = \"n_estimators\"\n",
    "    options_block = automl_step(options_block, explore_param)\n",
    "    \n",
    "    ALGORITHM_DETAIL = 'grid search (implied)'\n",
    "\n",
    "param_options, cv, n_jobs, refit, n_iter, verbose = get_cv_params(options_block, debug_mode=debug_mode,\n",
    "                                                                  override_cv=OVERRIDE_CV,\n",
    "                                                                  override_niter=OVERRIDE_N_ITER,\n",
    "                                                                  override_njobs=OVERRIDE_JOBS,\n",
    "                                                                  override_verbose=OVERRIDE_VERBOSE\n",
    "                                                                  )\n",
    "\n",
    "\n",
    "if not using_catboost and len(param_options.keys()) > 2 and not already_timed and debug_mode:\n",
    "    already_timed = True\n",
    "    %timeit starter_pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"cv:\", cv, \"n_jobs:\", n_jobs, \"refit:\", refit, \"n_iter:\", n_iter, \"verbose:\", verbose)\n",
    "#print('\\n\\nHyperparameters:')\n",
    "#param_options if not using_catboost else options_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = f'{ALGORITHM} (v{VERSION})'.lower()\n",
    "\n",
    "def fit_model_with_cross_validation(gs, X_train, y_train, fits):\n",
    "    pipe_start = time()\n",
    "    cv_result = gs.fit(X_train, y_train)\n",
    "    gs.fit(X_train, y_train)\n",
    "    pipe_end = time()\n",
    "    average_time = round((pipe_end - pipe_start) / (fits), 2)\n",
    "\n",
    "    print(f\"Total fit/CV time      : {int(pipe_end - pipe_start)} seconds   ({pipe_start} ==> {pipe_end})\")\n",
    "    print()\n",
    "    print(\n",
    "        f'average fit/score time = {round(cv_result.cv_results_[\"mean_fit_time\"].mean(), 2)}s/{round(cv_result.cv_results_[\"mean_score_time\"].mean(), 2)}s')\n",
    "    print(\n",
    "        f'max fit/score time     = {round(cv_result.cv_results_[\"mean_fit_time\"].max(), 2)}s/{round(cv_result.cv_results_[\"mean_score_time\"].max(), 2)}s')\n",
    "    print(f'refit time             = {round(cv_result.refit_time_, 2)}s')\n",
    "\n",
    "    #return cv_result, average_time, cv_result.refit_time_, len(cv_result.cv_results_[\"mean_fit_time\"])\n",
    "    return average_time, cv_result.refit_time_, len(cv_result.cv_results_[\"mean_fit_time\"])\n",
    "\n",
    "\n",
    "if not using_catboost:\n",
    "    if ALGORITHM_DETAIL == 'grid search' or ALGORITHM_DETAIL == 'grid search (implied)':\n",
    "        print('grid search (or implied)')\n",
    "        crossval_runner = GridSearchCV(\n",
    "            estimator=starter_pipe,\n",
    "            param_grid=param_options,\n",
    "            cv=cv, n_jobs=n_jobs, # get the AVX/AVX2 info if use n_jobs > 2\n",
    "            verbose=verbose, scoring=CROSS_VALIDATION_SCORING,\n",
    "            refit=refit,\n",
    "            return_train_score=True, #n_iter=n_iter,\n",
    "            #error_score='raise'\n",
    "        )\n",
    "    elif ALGORITHM_DETAIL == 'rerun best':\n",
    "        results_for_best_results = get_results()\n",
    "        model_for_best_results = results_for_best_results[key]\n",
    "        \n",
    "        params_for_best_results = model_for_best_results['best params']\n",
    "        method_for_best_results = model_for_best_results['best method']\n",
    "\n",
    "        print(method_for_best_results)\n",
    "        print(DATA_DETAIL)\n",
    "        \n",
    "        if 'pca' in method_for_best_results and 'pca' not in DATA_DETAIL:\n",
    "            raise ValueError(\"can't rerun this here, pca encoding was used\")\n",
    "            \n",
    "        for each in params_for_best_results:\n",
    "            if type(params_for_best_results[each]) != list:\n",
    "                params_for_best_results[each] = [params_for_best_results[each]]\n",
    "        \n",
    "        crossval_runner = GridSearchCV(\n",
    "            estimator=starter_pipe,\n",
    "            #param_grid=params_for_best_results,\n",
    "            param_grid=params_for_best_results,\n",
    "            cv=cv, n_jobs=n_jobs, # get the AVX/AVX2 info if use n_jobs > 2\n",
    "            verbose=verbose, scoring=CROSS_VALIDATION_SCORING,\n",
    "            refit=refit,\n",
    "            return_train_score=True, #n_iter=n_iter,\n",
    "            #error_score='raise'\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print('random search')\n",
    "        crossval_runner = RandomizedSearchCV(\n",
    "            estimator=starter_pipe,\n",
    "            param_distributions=param_options,\n",
    "            cv=cv, n_jobs=n_jobs,  # get the AVX/AVX2 info if use n_jobs > 2\n",
    "            verbose=verbose, scoring=CROSS_VALIDATION_SCORING,\n",
    "            refit=refit,\n",
    "            return_train_score=True,  #n_iter=n_iter,\n",
    "            n_iter=n_iter,  # 1, #3\n",
    "            #error_score='raise'\n",
    "    )\n",
    "    cv_average_fit_time, cv_best_model_fit_time, total_fits = fit_model_with_cross_validation(\n",
    "        crossval_runner, X_train, y_train, fits=cv * n_iter)\n",
    "\n",
    "else:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "    #pool = Pool(df, cat_features=['tenure.tenureType'], label=df['Price'].values)\n",
    "    pool_Xtrain = Pool(X_train, cat_features=[7], label=y_train)\n",
    "    #pool_Xtest = Pool(X_train, cat_features=[7], label=y_train)\n",
    "    pool_Xtest = Pool(X_test, cat_features=[7], label=y_test)\n",
    "    import sys\n",
    "    starter_model = model=CatBoostRegressor(iterations=3, depth=3, learning_rate=0.1, loss_function='RMSE', objective='RMSE')\n",
    "\n",
    "    output = starter_model.randomized_search(options_block, # param_options,\n",
    "                                    X=pool_Xtrain, # X_train,\n",
    "                                    #y=y_train,\n",
    "                                    #cat_features=[],\n",
    "                                    cv=5,\n",
    "                                    n_iter=100,\n",
    "                                    partition_random_seed=101,\n",
    "                                    calc_cv_statistics=True,\n",
    "                                    #search_by_train_test_split=True,\n",
    "                                    refit=True,\n",
    "                                    shuffle=True,\n",
    "                                    stratified=None,\n",
    "                                    #train_size=0.8,\n",
    "                                    #train_size=1,\n",
    "                                    verbose=True,\n",
    "                                    plot=True,\n",
    "                                    log_cout=sys.stdout,\n",
    "                                    log_cerr=sys.stderr)\n",
    "\n",
    "    cat_params, cat_cv_results = output['params'], output['cv_results']\n",
    "    crossval_runner = {\"best_params_\": cat_params, \"cv_results_\": cat_cv_results, \"best_estimator_\": None}\n",
    "crossval_runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if ALGORITHM_DETAIL == 'grid search' or ALGORITHM_DETAIL == 'grid search (implied)':\n",
    "    print(crossval_runner.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Get the results and print some graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not using_catboost:\n",
    "    best_estimator_pipe = crossval_runner.best_estimator_\n",
    "    cv_results_df = pd.DataFrame(crossval_runner.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "    print(\"Best Params\\n\",crossval_runner.best_params_, \"\\n---------------------\")\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"CV results\\n\",crossval_runner.cv_results_, \"\\n---------------------\")\n",
    "        #print(\"Best Params\\n\",crossval_runner[\"best_params_\"], \"\\n---------------------\")\n",
    "\n",
    "else:\n",
    "    print(cat_params)\n",
    "    print(cat_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not using_catboost:\n",
    "    cv_results_df['params2'] = cv_results_df['params'].apply(lambda l: '/'.join([str(c) for c in l.values()]))\n",
    "\n",
    "    cv_columns = ['params2', 'rank_test_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time', 'params']\n",
    "    # if 'Neural' not in ALGORITHM:\n",
    "    #     cv_columns.insert(2, 'mean_train_score')\n",
    "    cv_results_df_full_sorted = cv_results_df.sort_values('rank_test_score')[cv_columns].reset_index(drop=True)\n",
    "\n",
    "    cv_results_df_sorted = cv_results_df_full_sorted[cv_results_df_full_sorted['mean_test_score'] > -2]\n",
    "    if len(cv_results_df_sorted) != len(cv_results_df_full_sorted):\n",
    "        print(-len(cv_results_df_sorted) + len(cv_results_df_full_sorted), \"fits were total failures\")\n",
    "        total_fits = len(cv_results_df_sorted)\n",
    "\n",
    "if not using_catboost:\n",
    "    if is_jupyter:display(cv_results_df_sorted)\n",
    "\n",
    "    orig_debug_mode, orig_display_df_cols = debug_mode, pd.get_option('display.max_columns')\n",
    "    debug_mode = True\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    if debug_mode:\n",
    "        debug_cols = ['rank_test_score', 'mean_test_score', 'mean_fit_time', 'mean_score_time']\n",
    "        debug_cols.extend([c for c in cv_results_df.columns if 'param' in c and c != 'params'])\n",
    "\n",
    "    cv_results_df_summary = cv_results_df[debug_cols].head(7)\n",
    "    cv_results_df_summary.set_index('rank_test_score', inplace=True)\n",
    "\n",
    "    if is_jupyter:display(cv_results_df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**************</code>\n",
    "\n",
    "#### Mini Stage: Make predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not using_catboost:\n",
    "    y_pred = best_estimator_pipe.predict(X_test)\n",
    "else:\n",
    "    y_pred = starter_model.predict(pool_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('-' * 10 + ALGORITHM + '-' * 10)\n",
    "print('R square Accuracy', R2)\n",
    "print('Mean Absolute Error Accuracy', MAE)\n",
    "print('Mean Squared Error Accuracy', MSE)\n",
    "print('Root Mean Squared Error', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = np.hstack((y_test_index, y_test, y_pred))\n",
    "compare_df = DataFrame(compare, columns=['reference', 'actual', 'predicted'])\n",
    "compare_df['difference'] = abs(compare_df['actual'] - compare_df['predicted'])\n",
    "compare_df['diff 1 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['actual'] * 100)\n",
    "compare_df['diff 2 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['predicted']) * 100\n",
    "compare_df['reference'] = compare_df['reference'].astype(int)\n",
    "compare_df.set_index('reference', inplace=True)\n",
    "\n",
    "combined = compare_df.merge(df[columns], how='inner', left_index=True, right_index=True).sort_values(['diff 1 %'],\n",
    "                                                                                                     ascending=False)\n",
    "#pd.options.display.float_format = '{:.4f}'.format\n",
    "combined[['predicted', 'actual', 'Price', 'bedrooms', 'bathrooms']] = combined[\n",
    "    ['predicted', 'actual', 'Price', 'bedrooms', 'bathrooms']].astype(int)\n",
    "combined['bedrooms'] = combined['bedrooms'].astype(int)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_fig, best_model_ax = plt.subplots()\n",
    "best_model_ax.scatter(y_test, y_pred, edgecolors=(0, 0, 1))\n",
    "best_model_ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "best_model_ax.set_ylabel('Predicted')\n",
    "best_model_ax.set_xlabel('Actual')\n",
    "#ax.title.set_text(f'CV Chosen best option ({calculated_best_pipe[1]})')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not using_catboost:\n",
    "    def custom_model_and_predictions(model, graph_params, X_train, y_train, X_test):\n",
    "        graph_model = model\n",
    "        graph_model.set_params(**graph_params)\n",
    "        graph_model.fit(X_train, y_train)\n",
    "        y_pred_graph = model.predict(X_test)\n",
    "\n",
    "        return model, y_pred_graph\n",
    "\n",
    "\n",
    "    best_model_count = 10 if not quick_mode else 2\n",
    "    best_model_count = 3 if not quick_mode else 1\n",
    "    best_models = {}\n",
    "    best_model_predictions = {}\n",
    "    best_model_scores = {}\n",
    "\n",
    "    showable_increment = total_fits // (4 if not quick_mode else 2)\n",
    "    if showable_increment==0:showable_increment=1\n",
    "    for i in range(0, total_fits, showable_increment):\n",
    "        if debug_mode: print(f'{i} ==> {i}')\n",
    "\n",
    "        if i == 0:\n",
    "            fitted_graph_model = crossval_runner.best_estimator_\n",
    "            y_pred_graph = y_pred\n",
    "        else:\n",
    "            graph_pipe_params = cv_results_df_sorted['params'][i]\n",
    "            print(graph_pipe_params)\n",
    "            # would always return the best! graph_pipe_params = cv_results_df_sorted.loc[cv_results_df_sorted['rank_test_score'] == 1, 'params'].values[0]\n",
    "\n",
    "            graph_params = {}\n",
    "            for key2, value in graph_pipe_params.items():\n",
    "                graph_params[key2.replace('model__', '')] = value\n",
    "\n",
    "            fitted_graph_model, y_pred_graph = custom_model_and_predictions(make_pipeline(), graph_pipe_params, X_train,\n",
    "                                                                            y_train, X_test)\n",
    "\n",
    "        best_models[i] = fitted_graph_model[-1].get_params()\n",
    "        best_model_predictions[i] = y_pred_graph\n",
    "        best_model_scores[i] = fitted_graph_model.score(X_test, y_test)\n",
    "\n",
    "    if debug_mode: print(f'{-1} ==> {-1}')\n",
    "    graph_pipe_params = cv_results_df_sorted['params'][total_fits - 1]\n",
    "    print(graph_pipe_params)\n",
    "    graph_params = {}\n",
    "    for key2, value in graph_pipe_params.items():\n",
    "        graph_params[key2.replace('model__', '')] = value\n",
    "    fitted_graph_model, y_pred_graph = custom_model_and_predictions(make_pipeline(), graph_pipe_params, X_train,\n",
    "                                                                    y_train, X_test)\n",
    "    best_models[-1] = fitted_graph_model[-1].get_params()\n",
    "    best_model_predictions[-1] = y_pred_graph\n",
    "    best_model_scores[-1] = fitted_graph_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not using_catboost:\n",
    "    evolution_of_models_fig, evolution_of_models_axes = plt.subplots(nrows=len(best_model_scores.keys()), figsize=(15, 45))\n",
    "\n",
    "    ax_index=-1\n",
    "    #for i in best_model_scores.keys():\n",
    "    for i, ax_index in zip(best_model_scores.keys(), range(0, len(best_model_scores.keys()))):\n",
    "        #ax_index += 1\n",
    "        #print(len(best_model_scores.keys()))\n",
    "        #print('i',i, \"ax_index\",ax_index)\n",
    "        if i >= 0:\n",
    "            # plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "            # plt.scatter(y_test, best_model_predictions[i])\n",
    "            # # plt.title(str(i) + \" \" + str(round(best_model_scores[i], 4)) + \" for \" + str(best_models[i]))\n",
    "            # if len(best_models[i].keys()) < 30:\n",
    "            #     plt.title(str(i) + \" \" + str(round(best_model_scores[i], 4)) + \" for \" + str(best_models[i]))\n",
    "            # else:\n",
    "            #     plt.title(str(i) + \" \" + str(round(best_model_scores[i], 4)) + \" for entry \" + str(i))\n",
    "            # plt.show()\n",
    "\n",
    "            #>>>\n",
    "\n",
    "            plt.subplots_adjust(hspace=0.2)\n",
    "            plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "            #.flatten()\n",
    "            #coordinates = evolution_of_models_axes[i]\n",
    "\n",
    "            if len(best_models[i].keys()) < 30:\n",
    "                eom_title = str(i) + \" \" + str(round(best_model_scores[i], 4)) + \" for \" + str(best_models[i])\n",
    "            else:\n",
    "                eom_title = str(i) + \" \" + str(round(best_model_scores[i], 4)) + \" for entry \" + str(i)\n",
    "\n",
    "            print (ax_index)\n",
    "            sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=evolution_of_models_axes[ax_index], color='red')\n",
    "            sns.scatterplot(x=y_test.flatten(), y=best_model_predictions[0].flatten(), ax=evolution_of_models_axes[ax_index],\n",
    "                            s=100).set(title=eom_title)\n",
    "\n",
    "\n",
    "            #<<<\n",
    "\n",
    "    #plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "    #plt.scatter(y_test, best_model_predictions[-1])\n",
    "\n",
    "    if len(best_models[i].keys()) < 30:\n",
    "        eom_title = str(i) + \" \" + str(round(best_model_scores[-1], 4)) + \" for (worst)\" + str(best_models[-1])\n",
    "    else:\n",
    "        eom_title = str(i) + \" \" + str(round(best_model_scores[-1], 4)) + \" for (worst) entry \" + str(i)\n",
    "\n",
    "    print (ax_index)\n",
    "    sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=evolution_of_models_axes[ax_index], color='red')\n",
    "    sns.scatterplot(x=y_test.flatten(), y=best_model_predictions[-1].flatten(), ax=evolution_of_models_axes[ax_index],\n",
    "                    s=100).set(title=eom_title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not using_catboost:\n",
    "    sns.set_theme(font_scale=2, rc=None)\n",
    "    sns.set_theme(font_scale=1, rc=None)\n",
    "\n",
    "    worst_and_best_model_fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    #.flatten()\n",
    "    coordinates = axes[0]\n",
    "    sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=axes[0], color='red')\n",
    "    sns.scatterplot(x=y_test.flatten(), y=best_model_predictions[0].flatten(), ax=axes[0],\n",
    "                    s=100).set(title=f'\"BEST\" model')\n",
    "\n",
    "    sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=axes[1], color='red')\n",
    "    sns.scatterplot(x=y_test.flatten(), y=best_model_predictions[-1].flatten(), ax=axes[1],\n",
    "                    s=100).set(title=f'\"WORST\" model')\n",
    "\n",
    "    sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=axes[2], color='red')\n",
    "    sns.scatterplot(x=y_test.flatten(), y=best_model_predictions[-1].flatten(), ax=axes[2],\n",
    "                    s=120, color='orange')\n",
    "    sns.scatterplot(x=y_test.flatten(), y=best_model_predictions[0].flatten(), ax=axes[2],\n",
    "                    s=30, alpha=0.6, color='black').set(\n",
    "        title='best (black) vs worst (orange)')\n",
    "    #title='best (orange) vs worst (black)')\n",
    "\n",
    "\n",
    "    worst_and_best_model_fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <catboost.core.CatBoostRegressor object at 0x7fb167387490>\n",
    "# {'depth': 6}\n",
    "# defaultdict(<class 'list'>, {'iterations': [0, 1, 2],\n",
    "# 'test-RMSE-mean': [396884.9605444017, 359548.6632536235, 326027.84885587444],\n",
    "# 'test-RMSE-std': [308.9495320039113, 260.0967808594464, 219.65856329246023],\n",
    "# 'train-RMSE-mean': [396884.77936957515, 359542.3612912551, 326018.9404460669],\n",
    "# 'train-RMSE-std': [91.44140078375503, 86.77961380623475, 69.4038638987425]})\n",
    "\n",
    "cv_best_model_fit_time = cv_best_model_fit_time if not using_catboost else 999\n",
    "\n",
    "DD2 = \"(\" + \",\".join(DATA_DETAIL) + \")\" if len(DATA_DETAIL) >= 1 else \"\"\n",
    "\n",
    "method =  f\"{ALGORITHM_DETAIL}{DD2}\"\n",
    "if method == 'rerun best':\n",
    "    print(\"method:\", method)\n",
    "    method += \": \" + method_for_best_results\n",
    "    print(\"method:\", method)\n",
    "    method = method.replace('rerun best: rerun best: ','rerun best: ')\n",
    "    print(\"method:\", method)\n",
    "\n",
    "new_results = {\n",
    "    '_score': R2,\n",
    "    'R square Accuracy': R2,\n",
    "    'Mean Absolute Error Accuracy': MAE,\n",
    "    'Mean Squared Error Accuracy': MSE,\n",
    "    'Root Mean Squared Error': RMSE,\n",
    "    '_train time': cv_best_model_fit_time,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'date': str(datetime.now()),\n",
    "    '_params': crossval_runner.best_params_ if not using_catboost else cat_params,\n",
    "    '_method':method,\n",
    "    'run_env': run_env\n",
    "}\n",
    "\n",
    "if run_env not in ['colab']:\n",
    "    old_results_json = get_results()\n",
    "    try:\n",
    "        old_best_score = old_results_json[key]['best score']\n",
    "    except:\n",
    "        print(f\"haven't scored this model yet: {ALGORITHM}\")\n",
    "        old_best_score = -999\n",
    "    this_model_is_best = update_results(old_results_json, new_results, key)\n",
    "\n",
    "print(key)\n",
    "new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_runner.best_estimator_  if not using_catboost else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if this_model_is_best:\n",
    "    with open(f'../../../models/optimised_model_{ALGORITHM}_v{VERSION}{DD2}.pkl', 'wb') as f:\n",
    "        if not using_catboost:\n",
    "            pickle.dump(crossval_runner.best_estimator_, f)\n",
    "        else:\n",
    "            pickle.dump(starter_model, f)\n",
    "        new_model_decision = f\"pickled new version of model\\n{old_results_json[key]['_score']} is new best score (it's better than {old_best_score})\"\n",
    "        #print(results_json[key]['_score'], 'is an improvement on', results_json[key]['second best score'])\n",
    "else:\n",
    "    new_model_decision = f\"not updated saved model, the previous run was better\\n{old_results_json[key]['_score']} is worse than or equal to {old_best_score}\"\n",
    "\n",
    "print(new_model_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Investigate the feature importances (if applicable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_uses_feature_importances:\n",
    "    feature_importances = crossval_runner.best_estimator_[-1].feature_importances_ if not using_catboost else starter_model.get_feature_importance()\n",
    "    #std = np.std([tree.feature_importances_ for tree in model.estimators_], axis = 0)\n",
    "\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    print('Feature Ranking:')\n",
    "\n",
    "    feature_importances_output = \"\"\n",
    "    for f in range(X_train.shape[1]):\n",
    "        #print('%d. features %d (%f)' % (f + 1, indices[f], feature_importances[indices[f]]), df_features.columns[indices[f] + 1])\n",
    "        feature_importances_output += ('%d. features %d (%f)' % (f + 1, indices[f], feature_importances[indices[f]]))\n",
    "        feature_importances_output += '\\t\\t'\n",
    "        feature_importances_output += (df_features.columns[indices[f] + 1])\n",
    "        feature_importances_output += '\\n'\n",
    "    print(feature_importances_output)\n",
    "else:\n",
    "    print(f'{ALGORITHM} does not have feature_importances, skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_uses_feature_importances:\n",
    "    indices = np.argsort(feature_importances)\n",
    "\n",
    "    feature_importance_fig, best_model_ax = plt.subplots(figsize=(20, 20))\n",
    "    best_model_ax.barh(range(len(feature_importances)), feature_importances[indices])\n",
    "    best_model_ax.set_yticks(range(len(feature_importances)))\n",
    "    _ = best_model_ax.set_yticklabels(df_features.columns[[c + 1 for c in indices]])\n",
    "else:\n",
    "    print(f'{ALGORITHM} does not have feature_importances, skipping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:blue;color:blue\">**********************************************************************************************************</code>\n",
    "\n",
    "## Stage: Write the final report for this algorithm and dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def include_in_html_report(type, section_header=None, section_figure=None, section_content=None, section_content_list=None):\n",
    "\n",
    "    # writePath_html = r'model_results/%s (html).html' % key\n",
    "    # writePath_md = r'model_results/%s (md).md' % key\n",
    "    results_root = '../../F_evaluate_model'\n",
    "    writePath_html = f'{results_root}/html/{key}.html'.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "    writePath_md = f'{results_root}/markdown/{key}.md'\n",
    "\n",
    "#isinstance(ini_list2, list)\n",
    "    if not section_content_list:\n",
    "        section_content_list = [section_content]\n",
    "\n",
    "    if type == 'header':\n",
    "        w = 'w' if section_figure <= 1 else 'a'\n",
    "        with open(writePath_html, w) as f1:\n",
    "            headers = f'<h{section_figure}>{section_content}</h{section_figure}>'\n",
    "            f1.write(headers)\n",
    "        with open(writePath_md, w) as f2:\n",
    "            headers = f'{\"#\" * int(section_figure)} {section_content }\\n'\n",
    "            f2.write(headers)\n",
    "    else:\n",
    "        if section_header:\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                f1.write(f'<h3>{section_header}</h3>')\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                f2.write(f'### {section_header}\\n')\n",
    "\n",
    "        if type=='dataframe':\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                dfAsString = section_content.to_html()\n",
    "                f1.write(dfAsString)\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                dfAsString = section_content.to_markdown()\n",
    "                f2.write(dfAsString + '\\n')\n",
    "        elif type=='graph':\n",
    "            filename = key + \"_\" + section_content\n",
    "            #section_figure.savefig(f'model_results/artifacts/{filename.replace(\" \", \"_\")}')\n",
    "            section_figure.savefig(f'{results_root}/artifacts/{filename.replace(\" \", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")}')\n",
    "\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                dfAsString = f'<img src=\"../artifacts/{filename.replace(\" \",\"_\").replace(\"(\", \"_\").replace(\")\", \"_\")}\"/>'\n",
    "                f1.write(dfAsString)\n",
    "\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                #dfAsString = f'(./model_results/artifacts/{filename}) \\n'\n",
    "                #dfAsString = f'![detail](./artifacts/{filename.replace(\" \",\"_\")})'\n",
    "                dfAsString = f'![detail](../artifacts/{filename.replace(\" \",\"_\").replace(\"(\", \"_\").replace(\")\", \"_\")})'\n",
    "                f2.write(dfAsString)\n",
    "                f2.write('\\n')\n",
    "        elif type=='json':\n",
    "\n",
    "            # html_content_parsed = [[cell.text for cell in row(\"td\")]\n",
    "            #              for row in BeautifulSoup(content,features=\"html.parser\")(\"tr\")]\n",
    "            #\n",
    "            # html_content_dictionary = {element[0]:element[1:] for element in html_content_parsed}\n",
    "\n",
    "            #xxxprint(json.dumps(html_content_dictionary, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                #f.write(json.dumps(html_content_dictionary, indent=4))\n",
    "                soup = BeautifulSoup(section_content, \"html.parser\")\n",
    "                f1.write(str(soup.prettify()))\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                #f.write(json.dumps(html_content_dictionary, indent=4))\n",
    "                soup = BeautifulSoup(section_content, \"html.parser\")\n",
    "                #f2.write(str(soup.prettify()))\n",
    "\n",
    "\n",
    "                # html_content_dictionary = {element[0]:element[1:] for element in html_content_parsed}\n",
    "                # f2.write(json.dumps(html_content_dictionary, indent=4))\n",
    "\n",
    "                import ast\n",
    "                loads = ast.literal_eval(section_content)\n",
    "                #df = pd.DataFrame.from_dict(loads)\n",
    "                #df.drop(['dont'], axis=1, inplace=True)\n",
    "                #print(df.to_markdown(index=False,tablefmt='fancy_grid'))\n",
    "                for each in loads:\n",
    "                    f2.write(each + \" = \" + str(loads[each]) + \"\\n\\n\")\n",
    "\n",
    "        elif type=='dict':\n",
    "\n",
    "            for section_content in section_content_list:\n",
    "                if isinstance(section_content, str):\n",
    "                    import ast\n",
    "                    section_content = ast.literal_eval(section_content)\n",
    "\n",
    "                with open(writePath_html, 'a') as f1:\n",
    "                    soup = BeautifulSoup(str(section_content), \"html.parser\")\n",
    "                    f1.write(str(soup.prettify()))\n",
    "                with open(writePath_md, 'a') as f2:\n",
    "                    for each in section_content:\n",
    "                        f2.write(each + \" = \" + str(section_content[each]) + \"\\n\\n\")\n",
    "\n",
    "        elif type=='text':\n",
    "            with open(writePath_html, 'a') as f1:\n",
    "                for each_line in section_content_list:\n",
    "                    f1.write(each_line + '<br>')\n",
    "            with open(writePath_md, 'a') as f2:\n",
    "                for each_line in section_content_list:\n",
    "                    f2.write(each_line + '\\n\\n')\n",
    "\n",
    "        with open(writePath_html, 'a') as f1:\n",
    "            f1.write('<hr>')\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Results from {ALGORITHM}\", section_figure=1)\n",
    "\n",
    "end_timestamp = datetime.now()\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=f\"Dataset Version: {VERSION}\", section_content_list=[\n",
    "    f\"Date run: {datetime.now()}\"\n",
    "    \"\",\n",
    "    f\"Start time: {start_timestamp}\",\n",
    "    f\"End time: {end_timestamp}\",\n",
    "])\n",
    "include_in_html_report(\"header\", section_content=f\"Results\", section_figure=2)\n",
    "\n",
    "include_in_html_report(type=\"text\", section_header=\"Summary\", section_content=new_model_decision)\n",
    "\n",
    "#include_in_html_report(type=\"dataframe\",text_single=\"Tuned Models ranked by performance\", content=cv_results_df_sorted)\n",
    "\n",
    "if not using_catboost:\n",
    "    include_in_html_report(type='dataframe', section_header='Tuned Models ranked by performance, with parameter details', section_content=cv_results_df_summary)\n",
    "\n",
    "    include_in_html_report(type='graph', section_header='Best and worst models obtained by tuning', section_figure=worst_and_best_model_fig, section_content=\"best_and_worst.png\")\n",
    "\n",
    "    include_in_html_report(type='graph', section_header='Best Model: Comparing model predictions to actual property values', section_figure=best_model_fig, section_content='best_model_correlation.png')\n",
    "else: #if using_catboost:\n",
    "    include_in_html_report(type=\"text\", section_header=\"Model Specific Notes\", section_content_list=[\"can't display hyperparameter comparison for catboost\",\"can't display model performance graphs for catboost\",\"can't display model performance graphs for catboost\"])\n",
    "\n",
    "\n",
    "if model_uses_feature_importances:\n",
    "    include_in_html_report(\"header\", section_content=f\"Feature Importances\", section_figure=2)\n",
    "    include_in_html_report(type=\"text\", section_header=\"Feature Importances\", section_content=feature_importances_output)\n",
    "    include_in_html_report(type=\"graph\", section_header=f\"Feature Importances ({ALGORITHM})\", section_figure=feature_importance_fig, section_content='best_model_feature_importances.png')\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Comparison with other models\", section_figure=2)\n",
    "\n",
    "\n",
    "dff = pd.read_json('../../../results/results.json')\n",
    "\n",
    "version = VERSION\n",
    "\n",
    "\n",
    "all_models_df = dff[dff.columns].T.sort_values(\"best score\", ascending=False)\n",
    "version_models_df = dff[[c for c in dff.columns if version in c]].T.sort_values(\"best score\", ascending=False)\n",
    "\n",
    "version_models_summary = version_models_df[['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy', 'Root Mean Squared Error', 'best run date', 'best method']]\n",
    "all_models_summary = all_models_df[['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy', 'Root Mean Squared Error', 'best run date', 'best method']]\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=f\"Comparison with version {VERSION} performances\", section_content=version_models_summary)\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Comparison with all model performances\", section_content=all_models_summary)\n",
    "\n",
    "\n",
    "include_in_html_report(\"header\", section_content=f\"Appendix\", section_figure=2)\n",
    "\n",
    "include_in_html_report(type=\"dataframe\", section_header=\"Data Sample\", section_content=df.head(5))\n",
    "\n",
    "include_in_html_report(type=\"json\", section_header=\"Hyperparameter options for Randomized Grid Search\", section_content=f\"{param_options if not using_catboost else options_block}\")\n",
    "\n",
    "if not using_catboost:\n",
    "    include_in_html_report(type=\"graph\", section_header=f\"Range of hyperparameter results\", section_figure=evolution_of_models_fig,\n",
    "        section_content='evolution_of_models_fig.png')\n",
    "\n",
    "include_in_html_report(type=\"dict\", section_header=\"Environment Variables\", section_content=env_vars)\n",
    "\n",
    "\n",
    "def print_and_report(text_single, title):\n",
    "    include_in_html_report(\"text\", section_content=title)\n",
    "    for each in text_single:\n",
    "        print(each)\n",
    "        include_in_html_report(\"text\", section_header=\"\", section_content=each)\n",
    "\n",
    "# if not catboost:\n",
    "#     print_and_report([\n",
    "#         'Best Index:' + str(crossval_runner.best_index_) + '<br>',\n",
    "#         'Best Score:' + str(crossval_runner.best_score_) + '<br>',\n",
    "#         'Best Params: ' + str(crossval_runner.best_params_) + '<br>'\n",
    "#     ], \"Best Model Details\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Nearly finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script mycode.ipynb\n",
    "# with open('it10_all_models_20221203.ipynb', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# with open('mycode.py', 'w') as f:\n",
    "#     for line in lines:\n",
    "#         if 'nbconvert --to script' in line:\n",
    "#             break\n",
    "#         else:\n",
    "#             f.write(line)\n",
    "\n",
    "if create_python_script and is_jupyter:\n",
    "    filename = FILENAME+'.ipynb'\n",
    "    !jupyter nbconvert --to script $filename\n",
    "\n",
    "#!jupyter nbconvert --to script 'it10_all_models_20221203.ipynb' &&  \\\n",
    "#!mv ./folder/notebooks/*.py ./folder/python_scripts && \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
