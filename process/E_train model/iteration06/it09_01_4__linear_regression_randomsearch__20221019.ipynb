{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "ALGORITHM = 'Linear Regression (Ridge)'\n",
    "ALGORITHM_DETAIL = 'random search'\n",
    "VERSION = '06'\n",
    "\n",
    "RANDOM_STATE = 101\n",
    "TRAINING_SIZE = 0.9\n",
    "\n",
    "CROSS_VALIDATION_SCORING = 'r2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "pipe = Pipeline([\n",
    "    #('mms', MinMaxScaler()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "from termcolor import colored\n",
    "\n",
    "confirm_colab = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = confirm_colab\n",
    "\n",
    "if not IN_COLAB:\n",
    "    from functions_20221021B import set_csv_directory, get_combined_dataset, get_columns\n",
    "    from functions_20221021B import add_supplements, tidy_dataset, feature_engineer, preprocess\n",
    "    from functions_20221021B import update_results\n",
    "\n",
    "    set_csv_directory('final_split')\n",
    "\n",
    "debug_mode = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no columns data available for version 09",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m cutdown_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      4\u001B[0m LABEL \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 6\u001B[0m columns, booleans, floats, categories \u001B[38;5;241m=\u001B[39m get_columns(version\u001B[38;5;241m=\u001B[39mVERSION)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(colored(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-> \u001B[39m\u001B[38;5;124m\"\u001B[39m, columns)\n\u001B[1;32m      9\u001B[0m columns\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, LABEL)\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/process/E_train model/iteration06/functions_20221021B.py:324\u001B[0m, in \u001B[0;36mget_columns\u001B[0;34m(version)\u001B[0m\n\u001B[1;32m    321\u001B[0m     categories \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtenure.tenureType\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 324\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno columns data available for version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    326\u001B[0m columns \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    327\u001B[0m columns\u001B[38;5;241m.\u001B[39mextend(booleans)\n",
      "\u001B[0;31mValueError\u001B[0m: no columns data available for version 09"
     ]
    }
   ],
   "source": [
    "#cutdown_rows = 1000\n",
    "cutdown_rows = 0\n",
    "\n",
    "LABEL = 'Price'\n",
    "\n",
    "columns, booleans, floats, categories = get_columns(version=VERSION)\n",
    "\n",
    "print(colored(f\"features\", \"blue\"), \"-> \", columns)\n",
    "columns.insert(0, LABEL)\n",
    "print(colored(f\"label\", \"green\", None, ['bold']), \"-> \", LABEL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_source_dataframe(rows=cutdown_rows, folder_prefix='../../../'):\n",
    "    retrieval_type = None\n",
    "\n",
    "    filename = f'df_listings_v{VERSION}.csv'\n",
    "    remote_pathname = f'https://raw.githubusercontent.com/jayportfolio/capstone_streamlit/main/data/final/{filename}'\n",
    "    df_pathname_raw = folder_prefix + f'data/source/{filename}'\n",
    "    df_pathname_tidy = folder_prefix + f'data/final/{filename}'\n",
    "\n",
    "    if IN_COLAB:\n",
    "        inDF = pd.read_csv(remote_pathname, on_bad_lines='error', index_col=0)\n",
    "        retrieval_type = 'tidy'\n",
    "        print('loaded data from', folder_prefix + remote_pathname)\n",
    "    else:\n",
    "        inDF = pd.read_csv(df_pathname_tidy, on_bad_lines='error', index_col=0)\n",
    "        retrieval_type = 'tidy'\n",
    "        print('loaded data from', df_pathname_tidy)\n",
    "\n",
    "    if rows and rows > 0:\n",
    "        inDF = inDF[:rows]\n",
    "    return inDF, retrieval_type\n",
    "\n",
    "\n",
    "def create_train_test_data(df_orig, return_index=False, drop_nulls=True):\n",
    "    df = df_orig.copy()\n",
    "\n",
    "    if drop_nulls:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    if return_index:\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "    for column in categories:\n",
    "        df = pd.concat([df, pd.get_dummies(df[column], prefix=column)], axis=1)\n",
    "        df.drop([column], axis=1, inplace=True)  # now drop the original column (you don't need it anymore),\n",
    "\n",
    "    ins = df.pop('index')\n",
    "    df.insert(1, 'index2', ins)\n",
    "    df.insert(0, 'index', ins)\n",
    "\n",
    "    features = df[df.columns[2:]].values\n",
    "    labels = df.iloc[:, 0:2].values\n",
    "\n",
    "    if not return_index:\n",
    "        return train_test_split(features, labels, train_size=0.9, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(features, labels, train_size=0.9,\n",
    "                                                                random_state=RANDOM_STATE)\n",
    "        X_train_index = X_train1[:, 0].reshape(-1, 1)\n",
    "        y_train_index = y_train1[:, 0].reshape(-1, 1)\n",
    "        X_test_index = X_test1[:, 0].reshape(-1, 1)\n",
    "        y_test_index = y_test1[:, 0].reshape(-1, 1)\n",
    "        X_train1 = X_train1[:, 1:]\n",
    "        y_train1 = y_train1[:, 1].reshape(-1, 1)\n",
    "        X_test1 = X_test1[:, 1:]\n",
    "        y_test1 = y_test1[:, 1].reshape(-1, 1)\n",
    "\n",
    "        return X_train1, X_test1, y_train1, y_test1, X_train_index, X_test_index, y_train_index, y_test_index\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df, retrieval_type = get_source_dataframe()\n",
    "df_orig = df.copy()\n",
    "\n",
    "if retrieval_type != 'tidy':\n",
    "    df = tidy_dataset(df, version=int(VERSION))\n",
    "    df = feature_engineer(df, version=int(VERSION))\n",
    "\n",
    "    df = df[columns]\n",
    "\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_length = len(df)\n",
    "\n",
    "df = preprocess(df, version=int(VERSION))\n",
    "\n",
    "print(f\"dataframe contract due to cleaning: {old_length} ==> {len(df)}\")\n",
    "old_length = len(df)\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print(f\"{old_length} ==> {len(df)}\")\n",
    "old_length = len(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_index, X_test_index, y_train_index, y_test_index = create_train_test_data(df,\n",
    "                                                                                                                    return_index=True,\n",
    "                                                                                                                    drop_nulls=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train_index.shape, X_test_index.shape,\n",
    "      y_train_index.shape, y_test_index.shape)\n",
    "#print(type(X_train))\n",
    "#X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer = SimpleImputer(strategy='mean')\n",
    "#imputer.fit(X_train[6])\n",
    "#X_train[6] = imputer.transform(X_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "#pipe.fit(X_train, y_train)\n",
    "\n",
    "model = LinearRegression()\n",
    "#model.fit(X_train, y_train)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Best Score:  0.30582573121661794\n",
    "# Best Score:  {'alpha': 10, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'selection': 'cyclic', 'tol': 0.001, 'warm_start': True}\n",
    "# Best Score:  Lasso(alpha=10, tol=0.001, warm_start=True)\n",
    "# Best Score:  138\n",
    "\n",
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "#alpha = [1, 10, 100]\n",
    "fit_intercept = [True, False]\n",
    "max_iter = [100, 1000, 10000]\n",
    "positive = [True, False]\n",
    "solver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "tol = [0.00001, 0.0001, 0.001, 0.01]\n",
    "warm_start = [True, False]\n",
    "# ['alpha', 'copy_X', 'fit_intercept', 'max_iter', 'normalize', 'positive', 'precompute', 'random_state', 'selection', 'tol', 'warm_start'].\n",
    "\n",
    "options__n_neighbours = [3, 5, 7, 9, 15, 31]\n",
    "options__leafsize = [2, 3, 4, 57, 9, 13, 21]\n",
    "\n",
    "param_grid = dict(model__alpha=alpha, model__fit_intercept=fit_intercept, model__max_iter=max_iter,\n",
    "                  model__positive=positive,\n",
    "                  model__tol=tol, model__solver=solver)\n",
    "#param_grid = dict(estimator__n_neighbors=options__n_neighbours, estimator__leaf_size= options__leafsize)\n",
    "\n",
    "#param_grid = {'model__n_neighbors': options__n_neighbours,'model__leaf_size': options__leafsize},\n",
    "#param_grid = {'n_neighbors': options__n_neighbours,                   'leaf_size': options__leafsize},\n",
    "cv = 2\n",
    "n_jobs = 1\n",
    "verbose = 1\n",
    "refit = True\n",
    "\n",
    "#grid = RandomizedSearchCV(estimator=model, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "gs = RandomizedSearchCV(pipe, param_grid, cv=cv, n_jobs=n_jobs,\n",
    "                        verbose=verbose, scoring=CROSS_VALIDATION_SCORING, refit=refit,\n",
    "                        return_train_score=True),\n",
    "gs\n",
    "\n",
    "grid_result = gs[0].fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = grid_result.best_estimator_\n",
    "timings = []\n",
    "\n",
    "t0 = time()\n",
    "pipe.fit(X_train, y_train)\n",
    "timings.append(time() - t0)\n",
    "\n",
    "print(timings)\n",
    "average_time = sum(timings) / len(timings)\n",
    "print(average_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print(f'BEST PARAMS: {results.best_params_}')\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print(f'{round(mean, 3)} (+/-{round(std * 2, 3)}) for {params}')\n",
    "\n",
    "\n",
    "print_results(grid_result)\n",
    "print('Best Index: ', grid_result.best_index_)\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)\n",
    "#print('Best Model: ', grid_result.)\n",
    "#print('Best Params: ', grid_result.best_params_)[out]\n",
    "### Best Score:  0.4883436188936269\n",
    "### Best Params:  {'alpha': 0.01}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print('-' * 10 + ALGORITHM + '-' * 10)\n",
    "print('R square Accuracy', R2)\n",
    "print('Mean Absolute Error Accuracy', MAE)\n",
    "print('Mean Squared Error Accuracy', MSE)\n",
    "print('Root Mean Squared Error', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_mode:\n",
    "    print(y_test_index.reshape((-1, 1)).shape);\n",
    "    print(y_pred.reshape((-1, 1)).shape);\n",
    "    print(y_test.shape);\n",
    "    print(y_test_index.shape);\n",
    "    print(y_pred.shape);\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = np.hstack((y_test_index, y_test, y_pred))\n",
    "compare_df = DataFrame(compare, columns=['reference', 'actual', 'predicted'])\n",
    "compare_df['difference'] = abs(compare_df['actual'] - compare_df['predicted'])\n",
    "compare_df['diff 1 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['actual'] * 100)\n",
    "compare_df['diff 2 %'] = abs((compare_df['actual'] - compare_df['predicted']) / compare_df['predicted']) * 100\n",
    "compare_df['reference'] = compare_df['reference'].astype(str)\n",
    "compare_df.set_index('reference', inplace=True)\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.merge(df[columns], how='inner', left_index=True, right_index=True).sort_values(['diff 1 %'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pipe.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, pipe.predict(X_test), edgecolors=(0, 0, 1))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_xlabel('Actual')\n",
    "#ax.title.set_text(f'CV Chosen best option ({calculated_best_pipe[1]})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if False:\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            #return pd.Series({**params, **d})\n",
    "            return pd.Series({**params, **d, **{'params_full': str(params)}})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    score_summary = score_summary(self=gs, sort_by='max_score')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    #sns.set(rc={\"figure.figsize\": (10, 10)})\n",
    "    sns.set_theme(font_scale=2, rc=None)\n",
    "    sns.set_theme(font_scale=1, rc=None)\n",
    "\n",
    "    #total_graphs = len(score_summary)\n",
    "    # max_horizontal = 4\n",
    "    # index2 = 0\n",
    "    # resultant_rows = math.ceil(total_graphs / max_horizontal)\n",
    "    # #subplots_adjust()\n",
    "    #\n",
    "    # #fig, axes = plt.subplots(nrows=resultant_rows, ncols=max_horizontal)\n",
    "    fig, axes = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    best_estimator = score_summary.iloc[0]\n",
    "    worst_estimator = score_summary.iloc[-1]\n",
    "\n",
    "    name_best = best_estimator[\"estimator\"]\n",
    "    params_str = best_estimator[\"params_full\"]\n",
    "    params_best = ast.literal_eval(params_str)\n",
    "\n",
    "    name_worst = worst_estimator[\"estimator\"]\n",
    "    params_str = worst_estimator[\"params_full\"]\n",
    "    params_worst = ast.literal_eval(params_str)\n",
    "\n",
    "    KNeighborsRegressor().set_params()\n",
    "\n",
    "    best_pipe = make_pipe(name_best)\n",
    "    worst_pipe = make_pipe(name_worst)\n",
    "\n",
    "    coordinates = axes[0]\n",
    "    sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=axes[0], color='red')\n",
    "    sns.scatterplot(x=y_test, y=best_pipe.set_params(**params_best).fit(X_train, y_train).predict(X_test), ax=axes[0],\n",
    "                    s=100).set(\n",
    "        title=f'\"BEST\" model: {name_best} \\n{params_best}')\n",
    "\n",
    "    sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=axes[1], color='red')\n",
    "    sns.scatterplot(x=y_test, y=worst_pipe.set_params(**params_worst).fit(X_train, y_train).predict(X_test), ax=axes[1],\n",
    "                    s=100).set(\n",
    "        title=f'\"WORST\" model: {name_worst} \\n{params_worst}')\n",
    "\n",
    "    sns.scatterplot(x=y_test, y=worst_pipe.set_params(**params_worst).fit(X_train, y_train).predict(X_test), ax=axes[2],\n",
    "                    s=100, color='orange')\n",
    "    sns.scatterplot(x=y_test, y=best_pipe.set_params(**params_best).fit(X_train, y_train).predict(X_test), ax=axes[2],\n",
    "                    s=100, alpha=0.6, color='black').set(\n",
    "        title='best (black) vs worst (orange)')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if False:\n",
    "    max_horizontal = 3\n",
    "\n",
    "    #sns.set()\n",
    "    #sns.set_theme(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n",
    "    sns.set(rc={\"figure.figsize\": (20, 20)})\n",
    "    sns.set_theme(font_scale=2, rc=None)\n",
    "    sns.set_theme(font_scale=1, rc=None)\n",
    "\n",
    "    total_graphs = len(score_summary)\n",
    "    index2 = 0\n",
    "    resultant_rows = math.ceil(total_graphs / max_horizontal)\n",
    "    #subplots_adjust()\n",
    "\n",
    "    #fig, axes = plt.subplots(nrows=resultant_rows, ncols=max_horizontal)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=max_horizontal, figsize=(15, 10))\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    for (key, next_estimator), index in zip(score_summary.iterrows(), range(total_graphs)):\n",
    "        if index % (max_horizontal * 2) == 0 and index != 0:\n",
    "            index2 = 0\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            #fig, axes = plt.subplots(nrows=resultant_rows, ncols=max_horizontal)\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=max_horizontal, figsize=(15, 10))\n",
    "\n",
    "        name_next = next_estimator[\"estimator\"]\n",
    "        params_str = next_estimator[\"params_full\"]\n",
    "        params_next = ast.literal_eval(params_str)\n",
    "        #print(\"next\", params_next)\n",
    "\n",
    "        if 'noscale' in name_next:\n",
    "            pipe = Pipeline(steps=[\n",
    "                ('preprocessor', features_noscale_preprocessor),  # preprocess features\n",
    "                ('estimator', models_and_params[name_next][\"model\"]),\n",
    "            ])  # start the training\n",
    "        else:\n",
    "            pipe = Pipeline(steps=[\n",
    "                ('preprocessor', features_preprocessor),\n",
    "                ('estimator', models_and_params[name_next][\"model\"]),  # preprocess features\n",
    "            ])  # start the training\n",
    "\n",
    "        # 0 ==> 0,0\n",
    "        # 1 ==> 0,1\n",
    "        # 2 ==> 1,0\n",
    "        x_coor = index2 // max_horizontal\n",
    "        y_coor = index2 % max_horizontal\n",
    "\n",
    "        coordinates = axes[x_coor][y_coor]\n",
    "        #sns.lineplot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], hue='red', lw=3)\n",
    "        sns.lineplot(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], ax=coordinates, color='red')\n",
    "        sns.scatterplot(x=y_test, y=pipe.set_params(**params_next).fit(X_train, y_train).predict(X_test),\n",
    "                        ax=coordinates, s=100).set(\n",
    "            title=f'({index}) {\"BEST\" if index == 0 else \"next\"} model: {name_next} \\n{params_next}')\n",
    "        #if index == 11: break\n",
    "        index2 += 1\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    'Score': score,\n",
    "    'R square Accuracy': R2,\n",
    "    'Mean Absolute Error Accuracy': MAE,\n",
    "    'Mean Squared Error Accuracy': MSE,\n",
    "    'Root Mean Squared Error': RMSE,\n",
    "    'Training Time': average_time,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'date': str(datetime.now()),\n",
    "    'params': grid_result.best_params_\n",
    "}\n",
    "import json\n",
    "\n",
    "if not IN_COLAB:\n",
    "\n",
    "    results_filename = '../../../results/results.json'\n",
    "\n",
    "    with open(results_filename) as f:\n",
    "        raw_audit = f.read()\n",
    "\n",
    "    results_json = json.loads(raw_audit)\n",
    "\n",
    "    key = f'{ALGORITHM} - {ALGORITHM_DETAIL} (v{VERSION})'.lower()\n",
    "    updated_results_json = update_results(key, results_json, results)\n",
    "\n",
    "    with open(results_filename, 'w') as file:\n",
    "        file.write(json.dumps(updated_results_json, indent=4, sort_keys=True))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'../../../models/optimised_model_{ALGORITHM}_v{VERSION}.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_result.best_estimator_, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
