{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "from https://www.kaggle.com/code/hugosjoberg/house-prices-prediction-using-keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import StandardScaler # Used for scaling of data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 14:53:09.741091: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "show_graphs = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true,
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# Read in train data\n",
    "!wget https://raw.githubusercontent.com/jayportfolio/capstone_streamlit/main/data/final/df_listings_v06.csv\n",
    "\n",
    "df_train = pd.read_csv('df_listings_v06.csv', index_col=0)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-17 14:53:15--  https://raw.githubusercontent.com/jayportfolio/capstone_streamlit/main/data/final/df_listings_v06.csv\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 5116878 (4.9M) [text/plain]\r\n",
      "Saving to: ‘df_listings_v06.csv.2’\r\n",
      "\r\n",
      "df_listings_v06.csv 100%[===================>]   4.88M  2.03MB/s    in 2.4s    \r\n",
      "\r\n",
      "2022-11-17 14:53:19 (2.03 MB/s) - ‘df_listings_v06.csv.2’ saved [5116878/5116878]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "scrolled": true,
    "_uuid": "cb0db41d4fc18a4992301ae08bf61fad4f489f56"
   },
   "cell_type": "code",
   "source": [
    "df_train.head()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "             Price  bedrooms  bathrooms  nearestStation  location.latitude  \\\n14520525  550000.0       3.0        1.0        0.274316          51.529950   \n27953107  400000.0       2.0        2.0        0.305845          51.549390   \n33593487  579950.0       2.0        1.0        0.438045          51.447180   \n35271294  370000.0       2.0        1.0        0.399307          51.449568   \n35429088  599950.0       2.0        1.0        0.238187          51.577030   \n\n          location.longitude  latitude_deviation  longitude_deviation  \\\n14520525           -0.207020            0.030230             0.102600   \n27953107           -0.482600            0.049670             0.378180   \n33593487           -0.338770            0.052540             0.234350   \n35271294           -0.140154            0.050152             0.035734   \n35429088           -0.141230            0.077310             0.036810   \n\n         tenure.tenureType  \n14520525         LEASEHOLD  \n27953107         LEASEHOLD  \n33593487          FREEHOLD  \n35271294         LEASEHOLD  \n35429088               NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>bedrooms</th>\n      <th>bathrooms</th>\n      <th>nearestStation</th>\n      <th>location.latitude</th>\n      <th>location.longitude</th>\n      <th>latitude_deviation</th>\n      <th>longitude_deviation</th>\n      <th>tenure.tenureType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14520525</th>\n      <td>550000.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.274316</td>\n      <td>51.529950</td>\n      <td>-0.207020</td>\n      <td>0.030230</td>\n      <td>0.102600</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>27953107</th>\n      <td>400000.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.305845</td>\n      <td>51.549390</td>\n      <td>-0.482600</td>\n      <td>0.049670</td>\n      <td>0.378180</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>33593487</th>\n      <td>579950.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.438045</td>\n      <td>51.447180</td>\n      <td>-0.338770</td>\n      <td>0.052540</td>\n      <td>0.234350</td>\n      <td>FREEHOLD</td>\n    </tr>\n    <tr>\n      <th>35271294</th>\n      <td>370000.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.399307</td>\n      <td>51.449568</td>\n      <td>-0.140154</td>\n      <td>0.050152</td>\n      <td>0.035734</td>\n      <td>LEASEHOLD</td>\n    </tr>\n    <tr>\n      <th>35429088</th>\n      <td>599950.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.238187</td>\n      <td>51.577030</td>\n      <td>-0.141230</td>\n      <td>0.077310</td>\n      <td>0.036810</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "f073ac12db41eb616c7d3116537783d890286afd",
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Prepare data\n",
    "    Investigate what data that has a linear or some kind of relation to the sale price\n",
    "    Drop the unimportant features or less unimportant features\n",
    "    Drop features which has many NaN values"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "f41e1f2f85b2beda54623e15555c4aebca349522"
   },
   "cell_type": "code",
   "source": [
    "#descriptive statistics summary\n",
    "df_train['Price'].describe()"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "count     46871.000000\nmean     425069.390775\nstd      107227.324906\nmin      100000.000000\n25%      349950.000000\n50%      425000.000000\n75%      515000.000000\nmax      600000.000000\nName: Price, dtype: float64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "1cfcee21e591d2e426d3ac9b3d78e96564bfbb87"
   },
   "cell_type": "code",
   "source": [
    "#histogram\n",
    "if show_graphs:\n",
    "    sns.distplot(df_train[''                          'Price']);"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "4d96bb824f0a8b9994a9c312e0032e7fddb2c964"
   },
   "cell_type": "code",
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df_train['Price'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['Price'].kurt())"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness: -0.123988\n",
      "Kurtosis: -0.933832\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "3e04a673e79b42e251535b523cf4fdd375c948e4"
   },
   "cell_type": "markdown",
   "source": [
    "    - Skewness means the top of the iceberg is not in the middle but rather towards left or right.\n",
    "    - Kurtosis describe if the gaussian distrubution is very small and narrow or very wide"
   ]
  },
  {
   "metadata": {
    "_uuid": "b037115d4df4b5a57ab16204e15d954173ffefa0"
   },
   "cell_type": "markdown",
   "source": [
    "Use a heatmap to see which features have strongest correlation with house price"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "e1b24a67d4ba435393dfe8a94bac1b46bb5b2653"
   },
   "cell_type": "code",
   "source": [
    "if show_graphs:\n",
    "    #correlation matrix\n",
    "    corrmat = df_train.corr()\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "    sns.heatmap(corrmat, vmax=.8, square=True);"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "69c5af34c8ab83091cfbc91ab77bf3145901b6a6"
   },
   "cell_type": "markdown",
   "source": [
    "Here we can detect multicollinearity for example basement area and the area of the first floor so these hold more or less the same kind of data. The same goes for garage variables, for example if you have a big garage you also have more cars in it.\n",
    "\n",
    "Some variables are also important for the SalePrice with the biggest one being OverallQual\n",
    "\n",
    "Let's plot top 10 most important for correlating with SalePrice"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "ef685a7c0285e63735037ff7aff16b58b7aff810"
   },
   "cell_type": "code",
   "source": [
    "if show_graphs:\n",
    "    #saleprice correlation matrix\n",
    "    k = 10 #number of variables for heatmap\n",
    "    cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "    cm = np.corrcoef(df_train[cols].values.T)\n",
    "    sns.set(font_scale=1.25)\n",
    "    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "    plt.show()"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "eb0f81a1f65090603bc258fffed8177ab193a5fe"
   },
   "cell_type": "markdown",
   "source": [
    "From this plot we can draw the conclusion that:\n",
    "    - OverallQual is important\n",
    "    - GrLivArea is also important\n",
    "    - TotalBsmtSF is important\n",
    "    - GarageCars and GarageArea are two important features but we drop GarageArea since it is more or less the same information as GarageCars\n",
    "    - TotalBsmtSF and 1stFlrSF are also more or less the same so we drop 1StFlrSF\n",
    "    - TotRmsAbvGrd and GrLivArea are also strongly correlated to let's drop TotRmsAbvGrd\n",
    " \n",
    " Let's scatterplot these important features."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "20308da96c6fe321ad8eaa36936b1f4f310e246b"
   },
   "cell_type": "code",
   "source": [
    "if show_graphs:\n",
    "    #scatterplot\n",
    "    sns.set()\n",
    "    cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "    sns.pairplot(df_train[cols], size = 2.5)\n",
    "    plt.show();"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "ac7fec49425bb709149872260a913ca04eda50e8"
   },
   "cell_type": "markdown",
   "source": [
    "The basement area and total living area seems to have similarities their saleprice plot looks almost identical, let's drop basement area.\n",
    "\n",
    "Maybe also remove year built data since this data can be tricky to use."
   ]
  },
  {
   "metadata": {
    "_uuid": "394c7fab3b8a90176635ef240466257c45b0b549"
   },
   "cell_type": "markdown",
   "source": [
    "Let's have a  look at the missing data.\n",
    "\n",
    "Let's display a % of the data that is missing from some columns."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "a5f4b941925a23b99e781dc5ce921aa789cbc9a7"
   },
   "cell_type": "code",
   "source": [
    "#missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                     Total   Percent\ntenure.tenureType     2744  0.058544\nPrice                    0  0.000000\nbedrooms                 0  0.000000\nbathrooms                0  0.000000\nnearestStation           0  0.000000\nlocation.latitude        0  0.000000\nlocation.longitude       0  0.000000\nlatitude_deviation       0  0.000000\nlongitude_deviation      0  0.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Total</th>\n      <th>Percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tenure.tenureType</th>\n      <td>2744</td>\n      <td>0.058544</td>\n    </tr>\n    <tr>\n      <th>Price</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>bedrooms</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>bathrooms</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>nearestStation</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>location.latitude</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>location.longitude</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>latitude_deviation</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>longitude_deviation</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "e81bfe548fff5b1a48f1591d25a84f55e6b8b830"
   },
   "cell_type": "markdown",
   "source": [
    "Some of theese features are of interest for us and they don't show a massive shortage of data so lets create mean data for those values."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "a16226a47ed6d2fec9a6e69d8f7d982e4589e1e1",
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "df_train = df_train.fillna(df_train.mean())"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10630/1407479518.py:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_train = df_train.fillna(df_train.mean())\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "5d2f7b9502cb2cccc0f34642a042a227f703fddd"
   },
   "cell_type": "markdown",
   "source": [
    "Now let's remove outliers for example data that doesn't match what we expect like an insane price for a house\n",
    "\n",
    "To do this we standardize the data so that the mean is 0 and a standard deviation of 1. "
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "5fd1ebd08dc93d5b68d519ac7d2b695b212612f1"
   },
   "cell_type": "code",
   "source": [
    "#standardizing data\n",
    "saleprice_scaled = StandardScaler().fit_transform(df_train['Price'][:,np.newaxis]);\n",
    "low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\n",
    "high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n",
    "print('outer range (low) of the distribution:')\n",
    "print(low_range)\n",
    "print('\\nouter range (high) of the distribution:')\n",
    "print(high_range)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer range (low) of the distribution:\n",
      "[[-3.03162332]\n",
      " [-3.03162332]\n",
      " [-2.98499292]\n",
      " [-2.95701467]\n",
      " [-2.93836251]\n",
      " [-2.92903643]\n",
      " [-2.90105819]\n",
      " [-2.89756091]\n",
      " [-2.89173211]\n",
      " [-2.89173211]]\n",
      "\n",
      "outer range (high) of the distribution:\n",
      "[[1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]\n",
      " [1.63141695]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10630/2526487313.py:2: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  saleprice_scaled = StandardScaler().fit_transform(df_train['Price'][:,np.newaxis]);\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "91fc3e063f9fc40b36aee72864be793171f62f32"
   },
   "cell_type": "markdown",
   "source": [
    "    -Values that are similar to each other stay close to 0\n",
    "    -Values that are a bit odd get high values such as the 7 values."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "a9ce9e94821aaa869a66174feff34e4fb7b0a96f"
   },
   "cell_type": "code",
   "source": [
    "if show_graphs:\n",
    "    #bivariate analysis saleprice/grlivarea\n",
    "    var = 'bedrooms'\n",
    "    data = pd.concat([df_train['Price'], df_train[var]], axis=1)\n",
    "    data.plot.scatter(x=var, y='Price', ylim=(0,800000));"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "565e61c2e6aabfacbebfea5b7de0b68d2d8d151d"
   },
   "cell_type": "markdown",
   "source": [
    "What has been revealed:\n",
    "\n",
    "* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n",
    "* The two observations in the top of the plot are those 7 something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them."
   ]
  },
  {
   "metadata": {
    "_uuid": "8028a479de52da9a0240437ad642f456234688d2"
   },
   "cell_type": "markdown",
   "source": [
    "# Prepare data\n",
    "Right now I think we have an idea of what kind of data we are interested in and what data we don't think are useful for us. Let's build a pipeline for removing the data."
   ]
  },
  {
   "metadata": {
    "_uuid": "a59af02c285e253ac8fb2b54e43f0598480babf2"
   },
   "cell_type": "markdown",
   "source": [
    "Let's reload the data so we can have a fresh start!"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "58901f4a6c2a09dec4ad37fd4324e53f9af86572"
   },
   "cell_type": "code",
   "source": [
    "#df_train = pd.read_csv('house_prices_train.csv', index_col=0)\n",
    "df_train = pd.read_csv('df_listings_v06.csv', index_col=0)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "1d03fb5b6cdd841a9fe0983595b2f379e24b2755"
   },
   "cell_type": "markdown",
   "source": [
    "Let's not log the data since a neural network is quite good at working with non-linear data. I also tested and verified that the model didn't perform better or worse if I logged the data before hand."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "6ce8bbf540debd24e8fd5a63c53c28a8a7e1b5d2",
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "cols = ['Price','bedrooms','bathrooms','nearestStation','location.latitude','location.longitude','latitude_deviation','longitude_deviation']\n",
    "features = ['bedrooms','bathrooms','nearestStation','location.latitude','location.longitude','latitude_deviation','longitude_deviation']\n",
    "df_train = df_train[cols]\n",
    "# Create dummy values\n",
    "df_train = pd.get_dummies(df_train)\n",
    "#filling NA's with the mean of the column:\n",
    "df_train = df_train.fillna(df_train.mean())\n",
    "# Always standard scale the data before using NN\n",
    "scale = StandardScaler()\n",
    "X_train = df_train[features]\n",
    "X_train = scale.fit_transform(X_train)\n",
    "# Y is just the 'SalePrice' column\n",
    "y = df_train['Price'].values\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.33, random_state=seed)\n",
    "X_test.shape, y_test.shape"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "((15468, 7), (15468,))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "30d990717c54ee2a96c9687bd4e6a82a21abd560",
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(optimizer ='adam', loss = 'mean_squared_error', \n",
    "              metrics =[metrics.mae])\n",
    "    return model"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "9a77c30ae22162e72bba30c1f8616fbb35a5e0dc",
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 14:57:33.716370: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                80        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                330       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                1240      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,691\n",
      "Trainable params: 1,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "scrolled": true,
    "_uuid": "30008da456af620fb89a59aeaac613f1c7d5e9bf",
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=32)"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "982/982 [==============================] - 11s 10ms/step - loss: 158671044608.0000 - mean_absolute_error: 379751.8125 - val_loss: 60680753152.0000 - val_mean_absolute_error: 223599.5000\n",
      "Epoch 2/150\n",
      "982/982 [==============================] - 7s 7ms/step - loss: 20363808768.0000 - mean_absolute_error: 112714.4844 - val_loss: 11430468608.0000 - val_mean_absolute_error: 85242.1406\n",
      "Epoch 3/150\n",
      "982/982 [==============================] - 7s 7ms/step - loss: 10641301504.0000 - mean_absolute_error: 82197.1484 - val_loss: 9497707520.0000 - val_mean_absolute_error: 78183.8438\n",
      "Epoch 4/150\n",
      "982/982 [==============================] - 7s 7ms/step - loss: 9112588288.0000 - mean_absolute_error: 76523.1797 - val_loss: 8483755520.0000 - val_mean_absolute_error: 74155.7656\n",
      "Epoch 5/150\n",
      "982/982 [==============================] - 8s 8ms/step - loss: 8302387200.0000 - mean_absolute_error: 73267.4141 - val_loss: 7942640128.0000 - val_mean_absolute_error: 71943.6484\n",
      "Epoch 6/150\n",
      "982/982 [==============================] - 8s 8ms/step - loss: 7844923392.0000 - mean_absolute_error: 71453.9062 - val_loss: 7607928320.0000 - val_mean_absolute_error: 70446.9062\n",
      "Epoch 7/150\n",
      "982/982 [==============================] - 13s 13ms/step - loss: 7558338048.0000 - mean_absolute_error: 70222.4141 - val_loss: 7388645888.0000 - val_mean_absolute_error: 69536.9219\n",
      "Epoch 8/150\n",
      "982/982 [==============================] - 14s 14ms/step - loss: 7361811968.0000 - mean_absolute_error: 69413.4375 - val_loss: 7228988928.0000 - val_mean_absolute_error: 68775.2266\n",
      "Epoch 9/150\n",
      "140/982 [===>..........................] - ETA: 6s - loss: 7521439744.0000 - mean_absolute_error: 70417.1250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, validation_data\u001B[38;5;241m=\u001B[39m(X_test,y_test), epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/engine/training.py:1570\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1568\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[1;32m   1569\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[0;32m-> 1570\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[1;32m   1572\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/callbacks.py:470\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[1;32m    464\u001B[0m \n\u001B[1;32m    465\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[1;32m    468\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[0;32m--> 470\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/callbacks.py:317\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[0;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    322\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/callbacks.py:340\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[0;34m(self, mode, batch, logs)\u001B[0m\n\u001B[1;32m    337\u001B[0m     batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[0;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[1;32m    343\u001B[0m     end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/callbacks.py:388\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[0;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m    387\u001B[0m     hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[0;32m--> 388\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/callbacks.py:1081\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m-> 1081\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/callbacks.py:1158\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1156\u001B[0m     \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[1;32m   1157\u001B[0m     logs \u001B[38;5;241m=\u001B[39m tf_utils\u001B[38;5;241m.\u001B[39msync_to_numpy_or_python_type(logs)\n\u001B[0;32m-> 1158\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogbar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/utils/generic_utils.py:1051\u001B[0m, in \u001B[0;36mProgbar.update\u001B[0;34m(self, current, values, finalize)\u001B[0m\n\u001B[1;32m   1048\u001B[0m         info \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1050\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m info\n\u001B[0;32m-> 1051\u001B[0m     \u001B[43mio_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_msg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline_break\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   1052\u001B[0m     message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/keras/utils/io_utils.py:80\u001B[0m, in \u001B[0;36mprint_msg\u001B[0;34m(message, line_break)\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     79\u001B[0m         sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mwrite(message)\n\u001B[0;32m---> 80\u001B[0m     \u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     82\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(message)\n",
      "File \u001B[0;32m~/PycharmProjects/capstone_streamlit/venv/lib/python3.8/site-packages/ipykernel/iostream.py:480\u001B[0m, in \u001B[0;36mOutStream.flush\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_thread\u001B[38;5;241m.\u001B[39mschedule(evt\u001B[38;5;241m.\u001B[39mset)\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;66;03m# and give a timeout to avoid\u001B[39;00m\n\u001B[0;32m--> 480\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mevt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_timeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    481\u001B[0m         \u001B[38;5;66;03m# write directly to __stderr__ instead of warning because\u001B[39;00m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001B[39;00m\n\u001B[1;32m    483\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIOStream.flush timed out\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n\u001B[1;32m    484\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.8/threading.py:558\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    556\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 558\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib/python3.8/threading.py:306\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 306\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    308\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "944e699c04d97aaa7264f0fab5fa8eb3612dcb89"
   },
   "cell_type": "markdown",
   "source": [
    "Let's investigate how well this model did!"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "d642df9bf8c35e3b3f3a8be8e73dc5f271aa4429",
    "scrolled": false,
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "b3c6af154d715d6ecef8a54a6e9485470e7c2385"
   },
   "cell_type": "markdown",
   "source": [
    "This result is not very good and gives us a mean absolute error just above 20000 dollars. I beleive this model performs bad due to the fact that we have a quite small data-set becuase a neural network performs the best when having a big dataset. "
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "656ac4cee8aba9da1488ad0d24e7dfdba16cf3bd"
   },
   "cell_type": "code",
   "source": [
    "!wget https://raw.githubusercontent.com/Data-Science-FMI/ml-from-scratch-2019/master/data/house_prices_test.csv\n",
    "\n",
    "df_test = pd.read_csv('house_prices_test.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "cols =\n",
    "id_col = df_test['Id'].values.tolist()\n",
    "df_test['GrLivArea'] = np.log1p(df_test['GrLivArea'])\n",
    "df_test = pd.get_dummies(df_test)\n",
    "df_test = df_test.fillna(df_test.mean())\n",
    "X_test = df_test[cols].values\n",
    "# Always standard scale the data before using NN\n",
    "scale = StandardScaler()\n",
    "X_test = scale.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "b0e3651e7b54c5bb1b0c2fee80b74ce0e3ba4b0c"
   },
   "cell_type": "code",
   "source": [
    "prediction = model.predict(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "46afd7b911c25323c947d27e4fa480e2c44f2c13",
    "scrolled": true,
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = id_col\n",
    "submission['SalePrice'] = prediction\n",
    "submission"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "c59f0688b4adb73a2af19f570fe48d7e622c2f96"
   },
   "cell_type": "code",
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "492d851b6a2070bbcf088dc68f9560b4d56b85e2"
   },
   "cell_type": "markdown",
   "source": [
    "**Sources of information**\n",
    "\n",
    "[Comprehensive data exploration with Python\n",
    "](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n",
    "\n",
    "*Can recommend this notebook it is a fun and informative read*"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "44cba4f83556ab5e88ad1a5e1588b7f924e995be"
   },
   "cell_type": "code",
   "source": [
    "model.evaluate(model.predict(X_test), y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
