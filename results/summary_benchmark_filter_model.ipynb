{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "models = ['xg boost', 'linear regression', 'knn', 'decision tree', 'random forest', 'catboost','light gradient boosting', 'neural network']\n",
    "model = None  #'xg boost'\n",
    "original_model = model\n",
    "h = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T00:00:52.632296Z",
     "iopub.status.busy": "2022-12-14T00:00:52.631912Z",
     "iopub.status.idle": "2022-12-14T00:00:53.255004Z",
     "shell.execute_reply": "2022-12-14T00:00:53.254157Z",
     "shell.execute_reply.started": "2022-12-14T00:00:52.632223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8: neural network\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   best score   best time  \\\nneural network m15 mega + dropout (v09)              0.622235     2077.84   \nneural network m11 mega (v06)                        0.612318     2569.45   \nneural network m14 mega (v09)                        0.610895     2179.86   \nneural network m13 mega (v09)                        0.607154      500.16   \nneural network m12 mega (v06)                        0.594032      813.27   \nneural network m11 mega (v09)                        0.587136      994.87   \nneural network m13 mega (v10)                        0.583716      142.89   \nneural network m05 rec deep (v06)                    0.580348       604.9   \nneural network m14 mega (v10)                        0.579095     1129.09   \nneural network m12 mega (v09)                        0.571972       848.7   \nneural network m15 mega + dropout (v11)                0.5707     1191.61   \nneural network m12 mega (v10)                        0.567453       240.1   \nneural network m05 rec deep (v09)                    0.558413     1471.09   \nneural network (v06)                                 0.556696  312.991321   \nneural network m03 2 layers+wider (v06)              0.549647      275.71   \nneural network m01 simple (v06)                      0.541221        36.2   \nneural network m02 two layers (v09)                  0.540824      178.62   \nneural network m04 3 layers+wider (v09)              0.540678      1367.3   \nneural network simplified (v06)                      0.540642         999   \nneural network - random search [i64,norm,d64^6,...   0.533579         NaN   \nneural network m04 3 layers+wider (v06)              0.520933      395.14   \nneural network m02 two layers (v06)                  0.516773      112.54   \nneural network m01 simple (v09)                      0.508847      188.63   \nneural network with autoencoding m15 mega + dro...   0.488747     1276.23   \nneural network m03 2 layers+wider (v09)                0.4523     1822.49   \nneural network with autoencoding m15 mega + dro...    0.42689     1641.39   \n\n                                                   Mean Absolute Error Accuracy  \\\nneural network m15 mega + dropout (v09)                     212534587876.077087   \nneural network m11 mega (v06)                                      56035.854723   \nneural network m14 mega (v09)                                      51080.541252   \nneural network m13 mega (v09)                                      50938.566359   \nneural network m12 mega (v06)                                      54967.975952   \nneural network m11 mega (v09)                                       53873.03808   \nneural network m13 mega (v10)                                       54668.92487   \nneural network m05 rec deep (v06)                                  59357.533817   \nneural network m14 mega (v10)                                      53123.957462   \nneural network m12 mega (v09)                                      54695.968258   \nneural network m15 mega + dropout (v11)                            53709.702226   \nneural network m12 mega (v10)                                      55444.019939   \nneural network m05 rec deep (v09)                                  56357.458454   \nneural network (v06)                                               66710.702966   \nneural network m03 2 layers+wider (v06)                            64376.517431   \nneural network m01 simple (v06)                                    69131.972772   \nneural network m02 two layers (v09)                                 59659.87364   \nneural network m04 3 layers+wider (v09)                            60774.606195   \nneural network simplified (v06)                                    59373.052887   \nneural network - random search [i64,norm,d64^6,...                 57201.738174   \nneural network m04 3 layers+wider (v06)                            64421.234531   \nneural network m02 two layers (v06)                                64363.094125   \nneural network m01 simple (v09)                                    71376.935786   \nneural network with autoencoding m15 mega + dro...                 58773.731907   \nneural network m03 2 layers+wider (v09)                            64123.564083   \nneural network with autoencoding m15 mega + dro...                 64379.512251   \n\n                                                   Mean Squared Error Accuracy  \\\nneural network m15 mega + dropout (v09)              47995052450841435832320.0   \nneural network m11 mega (v06)                                4929943923.096645   \nneural network m14 mega (v09)                                4395484312.865142   \nneural network m13 mega (v09)                                4437747297.256148   \nneural network m12 mega (v06)                                4807033830.676171   \nneural network m11 mega (v09)                                4663875436.065661   \nneural network m13 mega (v10)                                  4948088299.4757   \nneural network m05 rec deep (v06)                            5470834738.338646   \nneural network m14 mega (v10)                                4754712208.689863   \nneural network m12 mega (v09)                                4835177161.720935   \nneural network m15 mega + dropout (v11)                      4849542517.605685   \nneural network m12 mega (v10)                                   4886231.935939   \nneural network m05 rec deep (v09)                             5015665639.26466   \nneural network (v06)                                         6646858211.119819   \nneural network m03 2 layers+wider (v06)                      6268024253.295089   \nneural network m01 simple (v06)                              7155866825.337951   \nneural network m02 two layers (v09)                          5504782893.089812   \nneural network m04 3 layers+wider (v09)                       5662801657.51863   \nneural network simplified (v06)                              5531509335.791807   \nneural network - random search [i64,norm,d64^6,...           5268876368.550115   \nneural network m04 3 layers+wider (v06)                      6266407942.292152   \nneural network m02 two layers (v06)                          6262090339.685114   \nneural network m01 simple (v09)                               7550556480.57456   \nneural network with autoencoding m15 mega + dro...           5775319278.568146   \nneural network m03 2 layers+wider (v09)                      6242932923.632053   \nneural network with autoencoding m15 mega + dro...           6474086836.603735   \n\n                                                   R square Accuracy  \\\nneural network m15 mega + dropout (v09)                   -15.994805   \nneural network m11 mega (v06)                               0.563583   \nneural network m14 mega (v09)                               0.610895   \nneural network m13 mega (v09)                               0.607154   \nneural network m12 mega (v06)                               0.574463   \nneural network m11 mega (v09)                               0.587136   \nneural network m13 mega (v10)                               0.561977   \nneural network m05 rec deep (v06)                           0.515701   \nneural network m14 mega (v10)                               0.579095   \nneural network m12 mega (v09)                               0.571972   \nneural network m15 mega + dropout (v11)                       0.5707   \nneural network m12 mega (v10)                               0.567453   \nneural network m05 rec deep (v09)                           0.555995   \nneural network (v06)                                        0.411595   \nneural network m03 2 layers+wider (v06)                     0.445131   \nneural network m01 simple (v06)                             0.366536   \nneural network m02 two layers (v09)                         0.512696   \nneural network m04 3 layers+wider (v09)                     0.498708   \nneural network simplified (v06)                              0.51033   \nneural network - random search [i64,norm,d64^6,...          0.533579   \nneural network m04 3 layers+wider (v06)                     0.445274   \nneural network m02 two layers (v06)                         0.445656   \nneural network m01 simple (v09)                             0.331597   \nneural network with autoencoding m15 mega + dro...          0.488747   \nneural network m03 2 layers+wider (v09)                     0.447352   \nneural network with autoencoding m15 mega + dro...           0.42689   \n\n                                                   Root Mean Squared Error  \\\nneural network m15 mega + dropout (v09)                219077731526.600952   \nneural network m11 mega (v06)                                 70213.559396   \nneural network m14 mega (v09)                                 66298.448797   \nneural network m13 mega (v09)                                 66616.419127   \nneural network m12 mega (v06)                                  69332.77602   \nneural network m11 mega (v09)                                 68292.572335   \nneural network m13 mega (v10)                                 70342.649221   \nneural network m05 rec deep (v06)                             73965.091349   \nneural network m14 mega (v10)                                 68954.421241   \nneural network m12 mega (v09)                                 69535.438172   \nneural network m15 mega + dropout (v11)                       69638.656776   \nneural network m12 mega (v10)                                 69901.587507   \nneural network m05 rec deep (v09)                             70821.364286   \nneural network (v06)                                          81528.266332   \nneural network m03 2 layers+wider (v06)                       79170.854822   \nneural network m01 simple (v06)                               84592.356778   \nneural network m02 two layers (v09)                           74194.224122   \nneural network m04 3 layers+wider (v09)                        75251.58907   \nneural network simplified (v06)                               74374.117378   \nneural network - random search [i64,norm,d64^6,...            72587.026172   \nneural network m04 3 layers+wider (v06)                       79160.646424   \nneural network m02 two layers (v06)                           79133.370582   \nneural network m01 simple (v09)                               86893.938112   \nneural network with autoencoding m15 mega + dro...            75995.521438   \nneural network m03 2 layers+wider (v09)                       79012.232747   \nneural network with autoencoding m15 mega + dro...            80461.710376   \n\n                                                                 best run date  \\\nneural network m15 mega + dropout (v09)             2022-12-13 23:57:56.104125   \nneural network m11 mega (v06)                       2022-11-29 12:57:16.459719   \nneural network m14 mega (v09)                       2022-12-12 16:01:42.195065   \nneural network m13 mega (v09)                       2022-12-12 15:25:17.070719   \nneural network m12 mega (v06)                       2022-11-29 17:08:44.480482   \nneural network m11 mega (v09)                       2022-12-12 15:02:24.649254   \nneural network m13 mega (v10)                       2022-12-01 10:27:39.663081   \nneural network m05 rec deep (v06)                   2022-11-29 11:41:39.682217   \nneural network m14 mega (v10)                       2022-12-01 11:52:45.011704   \nneural network m12 mega (v09)                       2022-12-12 15:16:52.189760   \nneural network m15 mega + dropout (v11)             2022-12-21 02:55:33.483148   \nneural network m12 mega (v10)                       2022-12-01 09:57:17.586487   \nneural network m05 rec deep (v09)                   2022-12-12 13:21:58.501853   \nneural network (v06)                                2000-01-01 17:09:59.063570   \nneural network m03 2 layers+wider (v06)             2022-11-29 10:13:10.517896   \nneural network m01 simple (v06)                     2022-11-29 09:13:15.856770   \nneural network m02 two layers (v09)                 2022-11-30 13:34:57.703544   \nneural network m04 3 layers+wider (v09)             2022-12-12 11:55:30.653553   \nneural network simplified (v06)                     2022-11-20 20:03:40.645221   \nneural network - random search [i64,norm,d64^6,...                         NaN   \nneural network m04 3 layers+wider (v06)             2022-11-29 11:21:09.812732   \nneural network m02 two layers (v06)                 2022-11-29 09:31:18.853517   \nneural network m01 simple (v09)                     2022-11-30 13:08:10.248178   \nneural network with autoencoding m15 mega + dro...  2022-12-22 19:06:48.049482   \nneural network m03 2 layers+wider (v09)             2022-12-11 18:56:27.304803   \nneural network with autoencoding m15 mega + dro...  2022-12-21 01:26:33.005210   \n\n                                                                                          best method  \\\nneural network m15 mega + dropout (v09)             loss=4.38e+04 valloss=4.85e+04 +valsplit=0.1 +...   \nneural network m11 mega (v06)                              loss=2833.6 valloss=4034.41 stop=619/1000    \nneural network m14 mega (v09)                       loss=4.68e+04 valloss=5.01e+04 +valsplit=0.1 +...   \nneural network m13 mega (v09)                       loss=4.55e+04 valloss=5.03e+04 +valsplit=0.1 +...   \nneural network m12 mega (v06)                       loss=4386.51 valloss=4438.8 +valsplit=0.1 stop...   \nneural network m11 mega (v09)                       loss=4.26e+09 valloss=4.72e+09 +valsplit=0.1 +...   \nneural network m13 mega (v10)                       loss=3878948096.0 valloss=4822886400.0 +valspl...   \nneural network m05 rec deep (v06)                          loss=4908.71 valloss=4603.08 stop=214/500    \nneural network m14 mega (v10)                       loss=4.85e+04 valloss=5.34e+04 +valsplit=0.1 s...   \nneural network m12 mega (v09)                       loss=4.76e+09 valloss=4.77e+09 +valsplit=0.1 +...   \nneural network m15 mega + dropout (v11)             loss=4.43e+04 valloss=5.35e+04 +valsplit=0.1 +...   \nneural network m12 mega (v10)                       loss=4790.75 valloss=4998.79 +valsplit=0.1 sto...   \nneural network m05 rec deep (v09)                   loss=4.87e+09 valloss=4.84e+09 +valsplit=0.1 +...   \nneural network (v06)                                random search [input11, d^20-500-500-20-5, den...   \nneural network m03 2 layers+wider (v06)             mse +epochs=500 +learn=0.003 +loss=5229.047851...   \nneural network m01 simple (v06)                     recommended simple model/mse +norm +epochs=50 ...   \nneural network m02 two layers (v09)                 loss=5424.62 valloss=5263.41 +valsplit=0.1 sto...   \nneural network m04 3 layers+wider (v09)             loss=5.20e+09 valloss=5.09e+09 +valsplit=0.1 +...   \nneural network simplified (v06)                             recommended simple model + normalise, mse   \nneural network - random search [i64,norm,d64^6,...                                                NaN   \nneural network m04 3 layers+wider (v06)                     loss=5415.7 valloss=5095.94 stop=166/500    \nneural network m02 two layers (v06)                 mse +norm +epochs=50 +learn=0.003 +endloss=578...   \nneural network m01 simple (v09)                     loss=5724.92 valloss=5608.12 +valsplit=0.1 sto...   \nneural network with autoencoding m15 mega + dro...  loss=5.27e+04 valloss=5.91e+04 +valsplit=0.1 +...   \nneural network m03 2 layers+wider (v09)             loss=6.38e+09 valloss=6.41e+09 +valsplit=0.1 +...   \nneural network with autoencoding m15 mega + dro...  loss=6.51e+04 valloss=6.52e+04 +valsplit=0.1 +...   \n\n                                                   best is shared  suboptimal  \nneural network m15 mega + dropout (v09)                     False  suboptimal  \nneural network m11 mega (v06)                               False  suboptimal  \nneural network m14 mega (v09)                               False     pending  \nneural network m13 mega (v09)                               False     pending  \nneural network m12 mega (v06)                               False  suboptimal  \nneural network m11 mega (v09)                               False     pending  \nneural network m13 mega (v10)                               False  suboptimal  \nneural network m05 rec deep (v06)                           False  suboptimal  \nneural network m14 mega (v10)                               False     pending  \nneural network m12 mega (v09)                               False     pending  \nneural network m15 mega + dropout (v11)                     False     pending  \nneural network m12 mega (v10)                               False     pending  \nneural network m05 rec deep (v09)                           False  suboptimal  \nneural network (v06)                                          NaN  suboptimal  \nneural network m03 2 layers+wider (v06)                     False  suboptimal  \nneural network m01 simple (v06)                             False  suboptimal  \nneural network m02 two layers (v09)                         False  suboptimal  \nneural network m04 3 layers+wider (v09)                     False  suboptimal  \nneural network simplified (v06)                             False  suboptimal  \nneural network - random search [i64,norm,d64^6,...            NaN     pending  \nneural network m04 3 layers+wider (v06)                     False  suboptimal  \nneural network m02 two layers (v06)                         False  suboptimal  \nneural network m01 simple (v09)                             False  suboptimal  \nneural network with autoencoding m15 mega + dro...          False     pending  \nneural network m03 2 layers+wider (v09)                     False  suboptimal  \nneural network with autoencoding m15 mega + dro...          False     pending  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>best score</th>\n      <th>best time</th>\n      <th>Mean Absolute Error Accuracy</th>\n      <th>Mean Squared Error Accuracy</th>\n      <th>R square Accuracy</th>\n      <th>Root Mean Squared Error</th>\n      <th>best run date</th>\n      <th>best method</th>\n      <th>best is shared</th>\n      <th>suboptimal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>neural network m15 mega + dropout (v09)</th>\n      <td>0.622235</td>\n      <td>2077.84</td>\n      <td>212534587876.077087</td>\n      <td>47995052450841435832320.0</td>\n      <td>-15.994805</td>\n      <td>219077731526.600952</td>\n      <td>2022-12-13 23:57:56.104125</td>\n      <td>loss=4.38e+04 valloss=4.85e+04 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m11 mega (v06)</th>\n      <td>0.612318</td>\n      <td>2569.45</td>\n      <td>56035.854723</td>\n      <td>4929943923.096645</td>\n      <td>0.563583</td>\n      <td>70213.559396</td>\n      <td>2022-11-29 12:57:16.459719</td>\n      <td>loss=2833.6 valloss=4034.41 stop=619/1000</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m14 mega (v09)</th>\n      <td>0.610895</td>\n      <td>2179.86</td>\n      <td>51080.541252</td>\n      <td>4395484312.865142</td>\n      <td>0.610895</td>\n      <td>66298.448797</td>\n      <td>2022-12-12 16:01:42.195065</td>\n      <td>loss=4.68e+04 valloss=5.01e+04 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m13 mega (v09)</th>\n      <td>0.607154</td>\n      <td>500.16</td>\n      <td>50938.566359</td>\n      <td>4437747297.256148</td>\n      <td>0.607154</td>\n      <td>66616.419127</td>\n      <td>2022-12-12 15:25:17.070719</td>\n      <td>loss=4.55e+04 valloss=5.03e+04 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m12 mega (v06)</th>\n      <td>0.594032</td>\n      <td>813.27</td>\n      <td>54967.975952</td>\n      <td>4807033830.676171</td>\n      <td>0.574463</td>\n      <td>69332.77602</td>\n      <td>2022-11-29 17:08:44.480482</td>\n      <td>loss=4386.51 valloss=4438.8 +valsplit=0.1 stop...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m11 mega (v09)</th>\n      <td>0.587136</td>\n      <td>994.87</td>\n      <td>53873.03808</td>\n      <td>4663875436.065661</td>\n      <td>0.587136</td>\n      <td>68292.572335</td>\n      <td>2022-12-12 15:02:24.649254</td>\n      <td>loss=4.26e+09 valloss=4.72e+09 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m13 mega (v10)</th>\n      <td>0.583716</td>\n      <td>142.89</td>\n      <td>54668.92487</td>\n      <td>4948088299.4757</td>\n      <td>0.561977</td>\n      <td>70342.649221</td>\n      <td>2022-12-01 10:27:39.663081</td>\n      <td>loss=3878948096.0 valloss=4822886400.0 +valspl...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m05 rec deep (v06)</th>\n      <td>0.580348</td>\n      <td>604.9</td>\n      <td>59357.533817</td>\n      <td>5470834738.338646</td>\n      <td>0.515701</td>\n      <td>73965.091349</td>\n      <td>2022-11-29 11:41:39.682217</td>\n      <td>loss=4908.71 valloss=4603.08 stop=214/500</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m14 mega (v10)</th>\n      <td>0.579095</td>\n      <td>1129.09</td>\n      <td>53123.957462</td>\n      <td>4754712208.689863</td>\n      <td>0.579095</td>\n      <td>68954.421241</td>\n      <td>2022-12-01 11:52:45.011704</td>\n      <td>loss=4.85e+04 valloss=5.34e+04 +valsplit=0.1 s...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m12 mega (v09)</th>\n      <td>0.571972</td>\n      <td>848.7</td>\n      <td>54695.968258</td>\n      <td>4835177161.720935</td>\n      <td>0.571972</td>\n      <td>69535.438172</td>\n      <td>2022-12-12 15:16:52.189760</td>\n      <td>loss=4.76e+09 valloss=4.77e+09 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m15 mega + dropout (v11)</th>\n      <td>0.5707</td>\n      <td>1191.61</td>\n      <td>53709.702226</td>\n      <td>4849542517.605685</td>\n      <td>0.5707</td>\n      <td>69638.656776</td>\n      <td>2022-12-21 02:55:33.483148</td>\n      <td>loss=4.43e+04 valloss=5.35e+04 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m12 mega (v10)</th>\n      <td>0.567453</td>\n      <td>240.1</td>\n      <td>55444.019939</td>\n      <td>4886231.935939</td>\n      <td>0.567453</td>\n      <td>69901.587507</td>\n      <td>2022-12-01 09:57:17.586487</td>\n      <td>loss=4790.75 valloss=4998.79 +valsplit=0.1 sto...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m05 rec deep (v09)</th>\n      <td>0.558413</td>\n      <td>1471.09</td>\n      <td>56357.458454</td>\n      <td>5015665639.26466</td>\n      <td>0.555995</td>\n      <td>70821.364286</td>\n      <td>2022-12-12 13:21:58.501853</td>\n      <td>loss=4.87e+09 valloss=4.84e+09 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network (v06)</th>\n      <td>0.556696</td>\n      <td>312.991321</td>\n      <td>66710.702966</td>\n      <td>6646858211.119819</td>\n      <td>0.411595</td>\n      <td>81528.266332</td>\n      <td>2000-01-01 17:09:59.063570</td>\n      <td>random search [input11, d^20-500-500-20-5, den...</td>\n      <td>NaN</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m03 2 layers+wider (v06)</th>\n      <td>0.549647</td>\n      <td>275.71</td>\n      <td>64376.517431</td>\n      <td>6268024253.295089</td>\n      <td>0.445131</td>\n      <td>79170.854822</td>\n      <td>2022-11-29 10:13:10.517896</td>\n      <td>mse +epochs=500 +learn=0.003 +loss=5229.047851...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m01 simple (v06)</th>\n      <td>0.541221</td>\n      <td>36.2</td>\n      <td>69131.972772</td>\n      <td>7155866825.337951</td>\n      <td>0.366536</td>\n      <td>84592.356778</td>\n      <td>2022-11-29 09:13:15.856770</td>\n      <td>recommended simple model/mse +norm +epochs=50 ...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m02 two layers (v09)</th>\n      <td>0.540824</td>\n      <td>178.62</td>\n      <td>59659.87364</td>\n      <td>5504782893.089812</td>\n      <td>0.512696</td>\n      <td>74194.224122</td>\n      <td>2022-11-30 13:34:57.703544</td>\n      <td>loss=5424.62 valloss=5263.41 +valsplit=0.1 sto...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m04 3 layers+wider (v09)</th>\n      <td>0.540678</td>\n      <td>1367.3</td>\n      <td>60774.606195</td>\n      <td>5662801657.51863</td>\n      <td>0.498708</td>\n      <td>75251.58907</td>\n      <td>2022-12-12 11:55:30.653553</td>\n      <td>loss=5.20e+09 valloss=5.09e+09 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network simplified (v06)</th>\n      <td>0.540642</td>\n      <td>999</td>\n      <td>59373.052887</td>\n      <td>5531509335.791807</td>\n      <td>0.51033</td>\n      <td>74374.117378</td>\n      <td>2022-11-20 20:03:40.645221</td>\n      <td>recommended simple model + normalise, mse</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network - random search [i64,norm,d64^6,d1] (v11)</th>\n      <td>0.533579</td>\n      <td>NaN</td>\n      <td>57201.738174</td>\n      <td>5268876368.550115</td>\n      <td>0.533579</td>\n      <td>72587.026172</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m04 3 layers+wider (v06)</th>\n      <td>0.520933</td>\n      <td>395.14</td>\n      <td>64421.234531</td>\n      <td>6266407942.292152</td>\n      <td>0.445274</td>\n      <td>79160.646424</td>\n      <td>2022-11-29 11:21:09.812732</td>\n      <td>loss=5415.7 valloss=5095.94 stop=166/500</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m02 two layers (v06)</th>\n      <td>0.516773</td>\n      <td>112.54</td>\n      <td>64363.094125</td>\n      <td>6262090339.685114</td>\n      <td>0.445656</td>\n      <td>79133.370582</td>\n      <td>2022-11-29 09:31:18.853517</td>\n      <td>mse +norm +epochs=50 +learn=0.003 +endloss=578...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network m01 simple (v09)</th>\n      <td>0.508847</td>\n      <td>188.63</td>\n      <td>71376.935786</td>\n      <td>7550556480.57456</td>\n      <td>0.331597</td>\n      <td>86893.938112</td>\n      <td>2022-11-30 13:08:10.248178</td>\n      <td>loss=5724.92 valloss=5608.12 +valsplit=0.1 sto...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network with autoencoding m15 mega + dropout (v11)</th>\n      <td>0.488747</td>\n      <td>1276.23</td>\n      <td>58773.731907</td>\n      <td>5775319278.568146</td>\n      <td>0.488747</td>\n      <td>75995.521438</td>\n      <td>2022-12-22 19:06:48.049482</td>\n      <td>loss=5.27e+04 valloss=5.91e+04 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>neural network m03 2 layers+wider (v09)</th>\n      <td>0.4523</td>\n      <td>1822.49</td>\n      <td>64123.564083</td>\n      <td>6242932923.632053</td>\n      <td>0.447352</td>\n      <td>79012.232747</td>\n      <td>2022-12-11 18:56:27.304803</td>\n      <td>loss=6.38e+09 valloss=6.41e+09 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>neural network with autoencoding m15 mega + dropout (v09)</th>\n      <td>0.42689</td>\n      <td>1641.39</td>\n      <td>64379.512251</td>\n      <td>6474086836.603735</td>\n      <td>0.42689</td>\n      <td>80461.710376</td>\n      <td>2022-12-21 01:26:33.005210</td>\n      <td>loss=6.51e+04 valloss=6.52e+04 +valsplit=0.1 +...</td>\n      <td>False</td>\n      <td>pending</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not original_model:\n",
    "    h = (h + 1) % len(models)\n",
    "    model = models[h]\n",
    "print(f'{h + 1}/{len(models)}: {model}')\n",
    "\n",
    "dff = pd.read_json('results.json')\n",
    "\n",
    "#version = 'v11'\n",
    "#version = 'v09'\n",
    "version = 'all'\n",
    "\n",
    "vNN_columns = dff.columns if version == 'all' else [c for c in dff.columns if version in c]\n",
    "if model != 'all': vNN_columns = [c for c in dff.columns if model in c]\n",
    "\n",
    "dataset_versions_df = dff[vNN_columns].T.sort_values(\"best score\", ascending=False)\n",
    "dataset_versions_df_summary = dataset_versions_df[\n",
    "    ['best score', 'best time', 'Mean Absolute Error Accuracy', 'Mean Squared Error Accuracy', 'R square Accuracy',\n",
    "     'Root Mean Squared Error', 'best run date', 'best method', 'best is shared', \"suboptimal\"]]\n",
    "dataset_versions_df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T17:58:27.432682Z",
     "iopub.status.busy": "2022-12-11T17:58:27.432324Z",
     "iopub.status.idle": "2022-12-11T17:58:27.457488Z",
     "shell.execute_reply": "2022-12-11T17:58:27.455698Z",
     "shell.execute_reply.started": "2022-12-11T17:58:27.432654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        Mean Absolute Error Accuracy  \\\nxg boost (tree) (v06)                   41210.351288   \nxg boost (tree) (v09)                   42675.746856   \nxg boost (v11) rs                       42603.011667   \nxg boost (v05) rs                       42229.001752   \nxg boost (v09)                          51922.608639   \nxg boost (v06)                          45988.280765   \nxg boost (v10)                          46626.687574   \nxg boost (tree) (v11)                   52330.434568   \nxg boost (v04) rs                       50419.209714   \nxg boost (v03) rs                       51147.260967   \nxg boost (v11)                          61556.686774   \nxg boost (linear) (v11)                 62224.321741   \nxg boost (v02) rs                       45160.157667   \nxg boost - basic (v02)                  48536.519056   \n\n                        Mean Squared Error Accuracy R square Accuracy  \\\nxg boost (tree) (v06)             3082428096.006462          0.727132   \nxg boost (tree) (v09)              3147715326.58041          0.721352   \nxg boost (v11) rs                 3151478137.618505          0.721019   \nxg boost (v05) rs                 3219625252.591313          0.710594   \nxg boost (v09)                    4355546020.138196          0.614431   \nxg boost (v06)                    3528869693.681411          0.687611   \nxg boost (v10)                    3594689512.156493          0.681785   \nxg boost (tree) (v11)             4477737747.935365          0.603614   \nxg boost (v04) rs                 4504941322.675693          0.603522   \nxg boost (v03) rs                 4733296553.824069          0.574533   \nxg boost (v11)                    5825092993.841407          0.484341   \nxg boost (linear) (v11)           5901034500.694016          0.477618   \nxg boost (v02) rs                 3627875443.187743          0.682955   \nxg boost - basic (v02)            3979587053.666782          0.652219   \n\n                        Root Mean Squared Error        _method  \\\nxg boost (tree) (v06)              55519.619019  random search   \nxg boost (tree) (v09)              56104.503621  random search   \nxg boost (v11) rs                  56138.027554            NaN   \nxg boost (v05) rs                  56741.741713            NaN   \nxg boost (v09)                     65996.560669  random search   \nxg boost (v06)                     59404.290196  random search   \nxg boost (v10)                     59955.729602  random search   \nxg boost (tree) (v11)              66915.900561  random search   \nxg boost (v04) rs                  67118.859665            NaN   \nxg boost (v03) rs                  68798.957505            NaN   \nxg boost (v11)                     76322.296833  random search   \nxg boost (linear) (v11)            76818.191209  random search   \nxg boost (v02) rs                  60231.847416            NaN   \nxg boost - basic (v02)             63083.968278            NaN   \n\n                                                                   _params  \\\nxg boost (tree) (v06)    {'model__booster': 'dart', 'model__colsample_b...   \nxg boost (tree) (v09)    {'model__booster': 'dart', 'model__colsample_b...   \nxg boost (v11) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v05) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v09)           {'model__booster': 'dart', 'model__early_stopp...   \nxg boost (v06)           {'model__booster': 'dart', 'model__early_stopp...   \nxg boost (v10)           {'model__booster': 'gbtree', 'model__early_sto...   \nxg boost (tree) (v11)    {'model__booster': 'gbtree', 'model__early_sto...   \nxg boost (v04) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v03) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v11)           {'model__booster': 'gblinear', 'model__early_s...   \nxg boost (linear) (v11)  {'model__booster': 'gblinear', 'model__early_s...   \nxg boost (v02) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost - basic (v02)                                                 NaN   \n\n                           _score _train time best is shared    best method  \\\nxg boost (tree) (v06)    0.727132  134.174901          False  random search   \nxg boost (tree) (v09)    0.721352  179.820912          False  random search   \nxg boost (v11) rs        0.721019    2.719016            NaN            NaN   \nxg boost (v05) rs        0.710594    2.337607            NaN            NaN   \nxg boost (v09)           0.614431   21.169572          False  random search   \nxg boost (v06)           0.687611   11.474795            NaN  random search   \nxg boost (v10)           0.681785    9.309589          False  random search   \nxg boost (tree) (v11)    0.603614   14.210391          False  random search   \nxg boost (v04) rs        0.603522    2.653133            NaN            NaN   \nxg boost (v03) rs        0.574533    3.303759            NaN            NaN   \nxg boost (v11)           0.484341    1.663234          False  random search   \nxg boost (linear) (v11)  0.477618   74.795648          False  random search   \nxg boost (v02) rs        0.682955    5.500423            NaN            NaN   \nxg boost - basic (v02)   0.652219    3.163121            NaN            NaN   \n\n                         ...                        date  \\\nxg boost (tree) (v06)    ...  2022-12-07 09:43:37.103009   \nxg boost (tree) (v09)    ...  2022-12-14 00:46:51.090690   \nxg boost (v11) rs        ...  2022-10-30 13:45:08.761048   \nxg boost (v05) rs        ...  2022-10-19 12:47:50.767724   \nxg boost (v09)           ...  2022-11-30 14:16:09.155790   \nxg boost (v06)           ...  2022-11-14 09:42:14.670928   \nxg boost (v10)           ...  2022-11-30 14:45:52.207314   \nxg boost (tree) (v11)    ...  2022-11-30 20:18:59.876471   \nxg boost (v04) rs        ...  2022-10-18 22:02:51.281873   \nxg boost (v03) rs        ...  2022-10-18 19:31:55.115003   \nxg boost (v11)           ...  2022-11-30 16:55:55.436173   \nxg boost (linear) (v11)  ...  2022-11-30 21:29:04.771525   \nxg boost (v02) rs        ...  2022-10-13 14:18:26.765461   \nxg boost - basic (v02)   ...  2022-10-13 12:58:02.988578   \n\n                                          first run random_state   run_env  \\\nxg boost (tree) (v06)    2022-12-03 00:21:25.848148          101  gradient   \nxg boost (tree) (v09)    2022-12-14 00:46:51.092108          101  gradient   \nxg boost (v11) rs        2022-10-30 13:45:08.764931          101       NaN   \nxg boost (v05) rs        2022-10-19 00:23:46.326282          101       NaN   \nxg boost (v09)           2022-11-30 09:33:31.526343          101  gradient   \nxg boost (v06)           2022-11-06 22:13:02.393884          101  gradient   \nxg boost (v10)           2022-11-30 14:45:52.209401          101  gradient   \nxg boost (tree) (v11)    2022-11-30 20:18:59.880879          101  gradient   \nxg boost (v04) rs        2022-10-18 22:02:51.284270          101       NaN   \nxg boost (v03) rs        2022-10-18 19:16:54.647950          101       NaN   \nxg boost (v11)           2022-11-30 16:55:55.438093          101  gradient   \nxg boost (linear) (v11)  2022-11-30 18:37:05.398836          101  gradient   \nxg boost (v02) rs        2022-10-13 13:42:33.640185          101       NaN   \nxg boost - basic (v02)   2022-10-13 12:47:43.630000          101       NaN   \n\n                         silver method  \\\nxg boost (tree) (v06)    random search   \nxg boost (tree) (v09)              NaN   \nxg boost (v11) rs                  NaN   \nxg boost (v05) rs                  NaN   \nxg boost (v09)           random search   \nxg boost (v06)                     NaN   \nxg boost (v10)                     NaN   \nxg boost (tree) (v11)              NaN   \nxg boost (v04) rs                  NaN   \nxg boost (v03) rs                  NaN   \nxg boost (v11)                     NaN   \nxg boost (linear) (v11)  random search   \nxg boost (v02) rs                  NaN   \nxg boost - basic (v02)             NaN   \n\n                                                             silver params  \\\nxg boost (tree) (v06)    {'model__booster': 'dart', 'model__colsample_b...   \nxg boost (tree) (v09)                                                  NaN   \nxg boost (v11) rs                                                      NaN   \nxg boost (v05) rs                                                      NaN   \nxg boost (v09)           {'model__booster': 'dart', 'model__early_stopp...   \nxg boost (v06)                                                         NaN   \nxg boost (v10)                                                         NaN   \nxg boost (tree) (v11)                                                  NaN   \nxg boost (v04) rs                                                      NaN   \nxg boost (v03) rs                                                      NaN   \nxg boost (v11)                                                         NaN   \nxg boost (linear) (v11)  {'model__booster': 'gblinear', 'model__early_s...   \nxg boost (v02) rs                                                      NaN   \nxg boost - basic (v02)                                                 NaN   \n\n                                    silver run date silver score silver time  \\\nxg boost (tree) (v06)    2022-12-03 00:21:25.790717     0.725989  217.285686   \nxg boost (tree) (v09)                           NaN          NaN         NaN   \nxg boost (v11) rs                               NaN          NaN         NaN   \nxg boost (v05) rs                               NaN          NaN         NaN   \nxg boost (v09)           2022-11-30 14:16:09.155790     0.614431   21.169572   \nxg boost (v06)                                  NaN          NaN         NaN   \nxg boost (v10)                                  NaN          NaN         NaN   \nxg boost (tree) (v11)                           NaN          NaN         NaN   \nxg boost (v04) rs                               NaN          NaN         NaN   \nxg boost (v03) rs                               NaN          NaN         NaN   \nxg boost (v11)                                  NaN          NaN         NaN   \nxg boost (linear) (v11)  2022-11-30 21:29:04.771525     0.477618   74.795648   \nxg boost (v02) rs                               NaN          NaN         NaN   \nxg boost - basic (v02)                          NaN          NaN         NaN   \n\n                         suboptimal  \nxg boost (tree) (v06)       pending  \nxg boost (tree) (v09)       pending  \nxg boost (v11) rs           pending  \nxg boost (v05) rs           pending  \nxg boost (v09)           suboptimal  \nxg boost (v06)              pending  \nxg boost (v10)              pending  \nxg boost (tree) (v11)       pending  \nxg boost (v04) rs           pending  \nxg boost (v03) rs           pending  \nxg boost (v11)              pending  \nxg boost (linear) (v11)  suboptimal  \nxg boost (v02) rs               NaN  \nxg boost - basic (v02)          NaN  \n\n[14 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean Absolute Error Accuracy</th>\n      <th>Mean Squared Error Accuracy</th>\n      <th>R square Accuracy</th>\n      <th>Root Mean Squared Error</th>\n      <th>_method</th>\n      <th>_params</th>\n      <th>_score</th>\n      <th>_train time</th>\n      <th>best is shared</th>\n      <th>best method</th>\n      <th>...</th>\n      <th>date</th>\n      <th>first run</th>\n      <th>random_state</th>\n      <th>run_env</th>\n      <th>silver method</th>\n      <th>silver params</th>\n      <th>silver run date</th>\n      <th>silver score</th>\n      <th>silver time</th>\n      <th>suboptimal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>xg boost (tree) (v06)</th>\n      <td>41210.351288</td>\n      <td>3082428096.006462</td>\n      <td>0.727132</td>\n      <td>55519.619019</td>\n      <td>random search</td>\n      <td>{'model__booster': 'dart', 'model__colsample_b...</td>\n      <td>0.727132</td>\n      <td>134.174901</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-12-07 09:43:37.103009</td>\n      <td>2022-12-03 00:21:25.848148</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>random search</td>\n      <td>{'model__booster': 'dart', 'model__colsample_b...</td>\n      <td>2022-12-03 00:21:25.790717</td>\n      <td>0.725989</td>\n      <td>217.285686</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (tree) (v09)</th>\n      <td>42675.746856</td>\n      <td>3147715326.58041</td>\n      <td>0.721352</td>\n      <td>56104.503621</td>\n      <td>random search</td>\n      <td>{'model__booster': 'dart', 'model__colsample_b...</td>\n      <td>0.721352</td>\n      <td>179.820912</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-12-14 00:46:51.090690</td>\n      <td>2022-12-14 00:46:51.092108</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v11) rs</th>\n      <td>42603.011667</td>\n      <td>3151478137.618505</td>\n      <td>0.721019</td>\n      <td>56138.027554</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>0.721019</td>\n      <td>2.719016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2022-10-30 13:45:08.761048</td>\n      <td>2022-10-30 13:45:08.764931</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v05) rs</th>\n      <td>42229.001752</td>\n      <td>3219625252.591313</td>\n      <td>0.710594</td>\n      <td>56741.741713</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>0.710594</td>\n      <td>2.337607</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2022-10-19 12:47:50.767724</td>\n      <td>2022-10-19 00:23:46.326282</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v09)</th>\n      <td>51922.608639</td>\n      <td>4355546020.138196</td>\n      <td>0.614431</td>\n      <td>65996.560669</td>\n      <td>random search</td>\n      <td>{'model__booster': 'dart', 'model__early_stopp...</td>\n      <td>0.614431</td>\n      <td>21.169572</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-11-30 14:16:09.155790</td>\n      <td>2022-11-30 09:33:31.526343</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>random search</td>\n      <td>{'model__booster': 'dart', 'model__early_stopp...</td>\n      <td>2022-11-30 14:16:09.155790</td>\n      <td>0.614431</td>\n      <td>21.169572</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>xg boost (v06)</th>\n      <td>45988.280765</td>\n      <td>3528869693.681411</td>\n      <td>0.687611</td>\n      <td>59404.290196</td>\n      <td>random search</td>\n      <td>{'model__booster': 'dart', 'model__early_stopp...</td>\n      <td>0.687611</td>\n      <td>11.474795</td>\n      <td>NaN</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-11-14 09:42:14.670928</td>\n      <td>2022-11-06 22:13:02.393884</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v10)</th>\n      <td>46626.687574</td>\n      <td>3594689512.156493</td>\n      <td>0.681785</td>\n      <td>59955.729602</td>\n      <td>random search</td>\n      <td>{'model__booster': 'gbtree', 'model__early_sto...</td>\n      <td>0.681785</td>\n      <td>9.309589</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-11-30 14:45:52.207314</td>\n      <td>2022-11-30 14:45:52.209401</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (tree) (v11)</th>\n      <td>52330.434568</td>\n      <td>4477737747.935365</td>\n      <td>0.603614</td>\n      <td>66915.900561</td>\n      <td>random search</td>\n      <td>{'model__booster': 'gbtree', 'model__early_sto...</td>\n      <td>0.603614</td>\n      <td>14.210391</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-11-30 20:18:59.876471</td>\n      <td>2022-11-30 20:18:59.880879</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v04) rs</th>\n      <td>50419.209714</td>\n      <td>4504941322.675693</td>\n      <td>0.603522</td>\n      <td>67118.859665</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>0.603522</td>\n      <td>2.653133</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2022-10-18 22:02:51.281873</td>\n      <td>2022-10-18 22:02:51.284270</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v03) rs</th>\n      <td>51147.260967</td>\n      <td>4733296553.824069</td>\n      <td>0.574533</td>\n      <td>68798.957505</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>0.574533</td>\n      <td>3.303759</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2022-10-18 19:31:55.115003</td>\n      <td>2022-10-18 19:16:54.647950</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (v11)</th>\n      <td>61556.686774</td>\n      <td>5825092993.841407</td>\n      <td>0.484341</td>\n      <td>76322.296833</td>\n      <td>random search</td>\n      <td>{'model__booster': 'gblinear', 'model__early_s...</td>\n      <td>0.484341</td>\n      <td>1.663234</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-11-30 16:55:55.436173</td>\n      <td>2022-11-30 16:55:55.438093</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>pending</td>\n    </tr>\n    <tr>\n      <th>xg boost (linear) (v11)</th>\n      <td>62224.321741</td>\n      <td>5901034500.694016</td>\n      <td>0.477618</td>\n      <td>76818.191209</td>\n      <td>random search</td>\n      <td>{'model__booster': 'gblinear', 'model__early_s...</td>\n      <td>0.477618</td>\n      <td>74.795648</td>\n      <td>False</td>\n      <td>random search</td>\n      <td>...</td>\n      <td>2022-11-30 21:29:04.771525</td>\n      <td>2022-11-30 18:37:05.398836</td>\n      <td>101</td>\n      <td>gradient</td>\n      <td>random search</td>\n      <td>{'model__booster': 'gblinear', 'model__early_s...</td>\n      <td>2022-11-30 21:29:04.771525</td>\n      <td>0.477618</td>\n      <td>74.795648</td>\n      <td>suboptimal</td>\n    </tr>\n    <tr>\n      <th>xg boost (v02) rs</th>\n      <td>45160.157667</td>\n      <td>3627875443.187743</td>\n      <td>0.682955</td>\n      <td>60231.847416</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>0.682955</td>\n      <td>5.500423</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2022-10-13 14:18:26.765461</td>\n      <td>2022-10-13 13:42:33.640185</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost - basic (v02)</th>\n      <td>48536.519056</td>\n      <td>3979587053.666782</td>\n      <td>0.652219</td>\n      <td>63083.968278</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.652219</td>\n      <td>3.163121</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2022-10-13 12:58:02.988578</td>\n      <td>2022-10-13 12:47:43.630000</td>\n      <td>101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>14 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_versions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-11T17:58:27.459481Z",
     "iopub.status.busy": "2022-12-11T17:58:27.459230Z",
     "iopub.status.idle": "2022-12-11T17:58:27.472176Z",
     "shell.execute_reply": "2022-12-11T17:58:27.471352Z",
     "shell.execute_reply.started": "2022-12-11T17:58:27.459459Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        best score   best time silver score silver time  \\\nxg boost (tree) (v06)     0.727132  134.174901     0.725989  217.285686   \nxg boost (tree) (v09)     0.721352  179.820912          NaN         NaN   \nxg boost (v11) rs         0.721019         NaN          NaN         NaN   \nxg boost (v05) rs         0.710594         NaN          NaN         NaN   \nxg boost (v09)            0.701117   119.28102     0.614431   21.169572   \nxg boost (v06)            0.687611   11.474795          NaN         NaN   \nxg boost (v10)            0.681785    9.309589          NaN         NaN   \nxg boost (tree) (v11)     0.603614   14.210391          NaN         NaN   \nxg boost (v04) rs         0.603522         NaN          NaN         NaN   \nxg boost (v03) rs         0.582071         NaN          NaN         NaN   \nxg boost (v11)            0.484341    1.663234          NaN         NaN   \nxg boost (linear) (v11)   0.484341   12.681655     0.477618   74.795648   \nxg boost (v02) rs              NaN         NaN          NaN         NaN   \nxg boost - basic (v02)         NaN         NaN          NaN         NaN   \n\n                           best method  silver method best is shared  \nxg boost (tree) (v06)    random search  random search          False  \nxg boost (tree) (v09)    random search            NaN          False  \nxg boost (v11) rs                  NaN            NaN            NaN  \nxg boost (v05) rs                  NaN            NaN            NaN  \nxg boost (v09)           random search  random search          False  \nxg boost (v06)           random search            NaN            NaN  \nxg boost (v10)           random search            NaN          False  \nxg boost (tree) (v11)    random search            NaN          False  \nxg boost (v04) rs                  NaN            NaN            NaN  \nxg boost (v03) rs                  NaN            NaN            NaN  \nxg boost (v11)           random search            NaN          False  \nxg boost (linear) (v11)  random search  random search          False  \nxg boost (v02) rs                  NaN            NaN            NaN  \nxg boost - basic (v02)             NaN            NaN            NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>best score</th>\n      <th>best time</th>\n      <th>silver score</th>\n      <th>silver time</th>\n      <th>best method</th>\n      <th>silver method</th>\n      <th>best is shared</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>xg boost (tree) (v06)</th>\n      <td>0.727132</td>\n      <td>134.174901</td>\n      <td>0.725989</td>\n      <td>217.285686</td>\n      <td>random search</td>\n      <td>random search</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (tree) (v09)</th>\n      <td>0.721352</td>\n      <td>179.820912</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>random search</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v11) rs</th>\n      <td>0.721019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v05) rs</th>\n      <td>0.710594</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v09)</th>\n      <td>0.701117</td>\n      <td>119.28102</td>\n      <td>0.614431</td>\n      <td>21.169572</td>\n      <td>random search</td>\n      <td>random search</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v06)</th>\n      <td>0.687611</td>\n      <td>11.474795</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>random search</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v10)</th>\n      <td>0.681785</td>\n      <td>9.309589</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>random search</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (tree) (v11)</th>\n      <td>0.603614</td>\n      <td>14.210391</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>random search</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v04) rs</th>\n      <td>0.603522</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v03) rs</th>\n      <td>0.582071</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v11)</th>\n      <td>0.484341</td>\n      <td>1.663234</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>random search</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (linear) (v11)</th>\n      <td>0.484341</td>\n      <td>12.681655</td>\n      <td>0.477618</td>\n      <td>74.795648</td>\n      <td>random search</td>\n      <td>random search</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v02) rs</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost - basic (v02)</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary_1_vs_2 = dataset_versions_df[\n",
    "    ['best score', 'best time', 'silver score', 'silver time', 'best method', 'silver method', 'best is shared']]\n",
    "df_summary_1_vs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-11T17:58:27.473783Z",
     "iopub.status.busy": "2022-12-11T17:58:27.473530Z",
     "iopub.status.idle": "2022-12-11T17:58:27.490023Z",
     "shell.execute_reply": "2022-12-11T17:58:27.489187Z",
     "shell.execute_reply.started": "2022-12-11T17:58:27.473760Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        best score   best time silver score silver time  \\\nxg boost (tree) (v06)     0.727132  134.174901     0.725989  217.285686   \nxg boost (tree) (v09)     0.721352  179.820912          NaN         NaN   \nxg boost (v11) rs         0.721019         NaN          NaN         NaN   \nxg boost (v05) rs         0.710594         NaN          NaN         NaN   \nxg boost (v09)            0.701117   119.28102     0.614431   21.169572   \nxg boost (v06)            0.687611   11.474795          NaN         NaN   \nxg boost (v10)            0.681785    9.309589          NaN         NaN   \nxg boost (tree) (v11)     0.603614   14.210391          NaN         NaN   \nxg boost (v04) rs         0.603522         NaN          NaN         NaN   \nxg boost (v03) rs         0.582071         NaN          NaN         NaN   \nxg boost (v11)            0.484341    1.663234          NaN         NaN   \nxg boost (linear) (v11)   0.484341   12.681655     0.477618   74.795648   \nxg boost (v02) rs              NaN         NaN          NaN         NaN   \nxg boost - basic (v02)         NaN         NaN          NaN         NaN   \n\n                                                               best params  \\\nxg boost (tree) (v06)    {'model__booster': 'dart', 'model__colsample_b...   \nxg boost (tree) (v09)    {'model__booster': 'dart', 'model__colsample_b...   \nxg boost (v11) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v05) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v09)           {'model__booster': 'dart', 'model__early_stopp...   \nxg boost (v06)           {'model__booster': 'dart', 'model__early_stopp...   \nxg boost (v10)           {'model__booster': 'gbtree', 'model__early_sto...   \nxg boost (tree) (v11)    {'model__booster': 'gbtree', 'model__early_sto...   \nxg boost (v04) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v03) rs        {'model__max_depth': 10, 'model__n_estimators'...   \nxg boost (v11)           {'model__booster': 'gblinear', 'model__early_s...   \nxg boost (linear) (v11)  {'model__booster': 'gblinear', 'model__early_s...   \nxg boost (v02) rs                                   MULTIPLE PARAM OPTIONS   \nxg boost - basic (v02)                                                 NaN   \n\n                                                             silver params  \\\nxg boost (tree) (v06)    {'model__booster': 'dart', 'model__colsample_b...   \nxg boost (tree) (v09)                                                  NaN   \nxg boost (v11) rs                                                      NaN   \nxg boost (v05) rs                                                      NaN   \nxg boost (v09)           {'model__booster': 'dart', 'model__early_stopp...   \nxg boost (v06)                                                         NaN   \nxg boost (v10)                                                         NaN   \nxg boost (tree) (v11)                                                  NaN   \nxg boost (v04) rs                                                      NaN   \nxg boost (v03) rs                                                      NaN   \nxg boost (v11)                                                         NaN   \nxg boost (linear) (v11)  {'model__booster': 'gblinear', 'model__early_s...   \nxg boost (v02) rs                                                      NaN   \nxg boost - basic (v02)                                                 NaN   \n\n                        best is shared  \nxg boost (tree) (v06)            False  \nxg boost (tree) (v09)            False  \nxg boost (v11) rs                  NaN  \nxg boost (v05) rs                  NaN  \nxg boost (v09)                   False  \nxg boost (v06)                     NaN  \nxg boost (v10)                   False  \nxg boost (tree) (v11)            False  \nxg boost (v04) rs                  NaN  \nxg boost (v03) rs                  NaN  \nxg boost (v11)                   False  \nxg boost (linear) (v11)          False  \nxg boost (v02) rs                  NaN  \nxg boost - basic (v02)             NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>best score</th>\n      <th>best time</th>\n      <th>silver score</th>\n      <th>silver time</th>\n      <th>best params</th>\n      <th>silver params</th>\n      <th>best is shared</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>xg boost (tree) (v06)</th>\n      <td>0.727132</td>\n      <td>134.174901</td>\n      <td>0.725989</td>\n      <td>217.285686</td>\n      <td>{'model__booster': 'dart', 'model__colsample_b...</td>\n      <td>{'model__booster': 'dart', 'model__colsample_b...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (tree) (v09)</th>\n      <td>0.721352</td>\n      <td>179.820912</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__booster': 'dart', 'model__colsample_b...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v11) rs</th>\n      <td>0.721019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v05) rs</th>\n      <td>0.710594</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v09)</th>\n      <td>0.701117</td>\n      <td>119.28102</td>\n      <td>0.614431</td>\n      <td>21.169572</td>\n      <td>{'model__booster': 'dart', 'model__early_stopp...</td>\n      <td>{'model__booster': 'dart', 'model__early_stopp...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v06)</th>\n      <td>0.687611</td>\n      <td>11.474795</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__booster': 'dart', 'model__early_stopp...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v10)</th>\n      <td>0.681785</td>\n      <td>9.309589</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__booster': 'gbtree', 'model__early_sto...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (tree) (v11)</th>\n      <td>0.603614</td>\n      <td>14.210391</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__booster': 'gbtree', 'model__early_sto...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v04) rs</th>\n      <td>0.603522</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v03) rs</th>\n      <td>0.582071</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__max_depth': 10, 'model__n_estimators'...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost (v11)</th>\n      <td>0.484341</td>\n      <td>1.663234</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'model__booster': 'gblinear', 'model__early_s...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (linear) (v11)</th>\n      <td>0.484341</td>\n      <td>12.681655</td>\n      <td>0.477618</td>\n      <td>74.795648</td>\n      <td>{'model__booster': 'gblinear', 'model__early_s...</td>\n      <td>{'model__booster': 'gblinear', 'model__early_s...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>xg boost (v02) rs</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MULTIPLE PARAM OPTIONS</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>xg boost - basic (v02)</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_vs_2b = dataset_versions_df[\n",
    "    ['best score', 'best time', 'silver score', 'silver time', 'best params', 'silver params', 'best is shared']]\n",
    "df_1_vs_2b\n",
    "#{'model__booster': 'gbtree', 'model__early_stopping_rounds': None, 'model__gamma': 100, 'model__learning_rate': None, 'model__max_delta_step': 0, 'model__max_depth': 6, 'model__min_child_weight': ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
